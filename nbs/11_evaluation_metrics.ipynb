{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8G40rX1m3D5C",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp evaluation.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W-Wd2-fL3D5C",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "> Functions to calculate cross-modal retrieval (R@k, MR) and zero-shot classification accuracy for Indic-CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PzO1cWj-3D5D",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8XJ4n_cI3D5D",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Mount Google Drive (Optional, but recommended for persistent storage)\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "    # Define PROJECT_ROOT for Colab\n",
    "    PROJECT_ROOT = Path('/content/drive/MyDrive/Indic-Clip') # Adjust path if needed\n",
    "    if not PROJECT_ROOT.exists():\n",
    "        print(f\"Warning: Project directory not found at {PROJECT_ROOT}. Please ensure it exists.\")\n",
    "    else:\n",
    "        # Add project root to sys.path\n",
    "        if str(PROJECT_ROOT) not in sys.path:\n",
    "            sys.path.insert(0, str(PROJECT_ROOT))\n",
    "            print(f\"Added {PROJECT_ROOT} to sys.path\")\n",
    "        # Change current working directory\n",
    "        os.chdir(PROJECT_ROOT)\n",
    "        print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, skipping Drive mount.\")\n",
    "    # Define PROJECT_ROOT for local execution (adjust if needed)\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    if PROJECT_ROOT.name == 'nbs': PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    print(f\"Running locally. Project root assumed: {PROJECT_ROOT}\")\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "         sys.path.insert(0, str(PROJECT_ROOT))\n",
    "         print(f\"Added {PROJECT_ROOT} to sys.path\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Colab setup: {e}\")\n",
    "    PROJECT_ROOT = Path('.').resolve()\n",
    "    print(f\"Defaulting project root to current dir: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tOa56U4t3D5D",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Install requirements if needed (especially in Colab)\n",
    "# !pip install -qr requirements.txt\n",
    "# !pip install scikit-learn # Ensure sklearn is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yX43sYdE3D5E",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J-2uN22m3D5E",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "from typing import List, Dict, Callable\n",
    "from fastcore.all import *\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- Project Imports ---\n",
    "try:\n",
    "    from indic_clip.core import get_logger, setup_logging\n",
    "    from indic_clip.model.clip import IndicCLIP # Needed for type hinting and model methods\n",
    "    from indic_clip.data.tokenization import IndicBERTTokenizer # Needed for type hinting\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Could not import project modules in 11_evaluation_metrics.ipynb. Ensure path is correct.\")\n",
    "    # Define dummy logger if core not found\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "    def setup_logging(): pass\n",
    "    # Define dummy classes for type hints if needed\n",
    "    class IndicCLIP(torch.nn.Module): pass\n",
    "    class IndicBERTTokenizer: pass\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q73QGqYk3D5F",
   "metadata": {},
   "source": [
    "## Cross-Modal Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V7jI2J8r3D5F",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_retrieval_metrics(\n",
    "    image_features: torch.Tensor, \n",
    "    text_features: torch.Tensor, \n",
    "    logit_scale: torch.Tensor, \n",
    "    k_values: List[int] = [1, 5, 10]\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculates Recall@k (R@k) and Mean Recall (MR) for image-to-text and text-to-image retrieval.\n",
    "\n",
    "    Assumes image_features and text_features are L2 normalized and correspond to paired data\n",
    "    along the batch dimension (i.e., image_i matches text_i).\n",
    "\n",
    "    Args:\n",
    "        image_features (torch.Tensor): Tensor of normalized image features (B, EmbeddingDim).\n",
    "        text_features (torch.Tensor): Tensor of normalized text features (B, EmbeddingDim).\n",
    "        logit_scale (torch.Tensor): The temperature scaling factor (scalar tensor, already exponentiated).\n",
    "        k_values (List[int]): List of k values for which to calculate Recall@k.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary containing calculated metrics like 'i2t_r@1', 't2i_r@5', 'mean_recall'.\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    device = image_features.device\n",
    "\n",
    "    # Calculate similarity matrix\n",
    "    similarity = logit_scale * image_features @ text_features.t()\n",
    "    num_samples = similarity.shape[0]\n",
    "    \n",
    "    # --- Image-to-Text Retrieval ---\n",
    "    # Find the top k text indices for each image query\n",
    "    i2t_indices = similarity.topk(max(k_values), dim=1)[1]\n",
    "    # Create ground truth labels (image i should match text i)\n",
    "    gt_labels = torch.arange(num_samples, device=device).view(-1, 1)\n",
    "    \n",
    "    # Check if the ground truth label is within the top k predictions\n",
    "    i2t_correct = i2t_indices == gt_labels\n",
    "\n",
    "    # Calculate R@k for image-to-text\n",
    "    all_recalls = []\n",
    "    for k in k_values:\n",
    "        recall_k = i2t_correct[:, :k].any(dim=1).float().mean().item()\n",
    "        metrics[f'i2t_r@{k}'] = recall_k\n",
    "        all_recalls.append(recall_k)\n",
    "        logger.debug(f\"i2t R@{k}: {recall_k:.4f}\")\n",
    "\n",
    "    # --- Text-to-Image Retrieval ---\n",
    "    # Find the top k image indices for each text query (using transpose)\n",
    "    t2i_indices = similarity.t().topk(max(k_values), dim=1)[1]\n",
    "    # Ground truth labels are the same (text i should match image i)\n",
    "    \n",
    "    # Check if the ground truth label is within the top k predictions\n",
    "    t2i_correct = t2i_indices == gt_labels\n",
    "    \n",
    "    # Calculate R@k for text-to-image\n",
    "    for k in k_values:\n",
    "        recall_k = t2i_correct[:, :k].any(dim=1).float().mean().item()\n",
    "        metrics[f't2i_r@{k}'] = recall_k\n",
    "        all_recalls.append(recall_k)\n",
    "        logger.debug(f\"t2i R@{k}: {recall_k:.4f}\")\n",
    "\n",
    "    # --- Mean Recall (MR) ---\n",
    "    metrics['mean_recall'] = np.mean(all_recalls) if all_recalls else 0.0\n",
    "    logger.debug(f\"Mean Recall (MR): {metrics['mean_recall']:.4f}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9v6b_Vl3D5G",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i3x_45P53D5H",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_zeroshot_accuracy(\n",
    "    image_features: torch.Tensor,\n",
    "    image_labels: List[int] | np.ndarray | torch.Tensor,\n",
    "    class_names: List[str],\n",
    "    templates: List[str] | Callable[[str], str],\n",
    "    model: IndicCLIP,\n",
    "    tokenizer: IndicBERTTokenizer,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the Top-1 Zero-Shot Classification accuracy.\n",
    "\n",
    "    Args:\n",
    "        image_features (torch.Tensor): Tensor of normalized image features (B, EmbeddingDim).\n",
    "        image_labels (List[int] | np.ndarray | torch.Tensor): Ground truth labels for the images (indices corresponding to class_names).\n",
    "        class_names (List[str]): List of class names.\n",
    "        templates (List[str] | Callable[[str], str]): List of prompt templates (e.g., \"a photo of {}\") \n",
    "                                                       or a function that takes a class name and returns a prompt.\n",
    "        model (IndicCLIP): The trained IndicCLIP model instance.\n",
    "        tokenizer (IndicBERTTokenizer): The tokenizer instance used by the model.\n",
    "\n",
    "    Returns:\n",
    "        float: The Top-1 zero-shot classification accuracy.\n",
    "    \"\"\"\n",
    "    device = image_features.device\n",
    "    num_classes = len(class_names)\n",
    "    model.eval() # Ensure model is in evaluation mode\n",
    "\n",
    "    logger.info(f\"Starting zero-shot classification for {num_classes} classes.\")\n",
    "\n",
    "    # 1. Generate and encode text prompts for all classes\n",
    "    all_prompts = []\n",
    "    if callable(templates):\n",
    "        for classname in class_names:\n",
    "            all_prompts.append(templates(classname))\n",
    "    else: # templates is a list of strings\n",
    "        for template in templates:\n",
    "            for classname in class_names:\n",
    "                all_prompts.append(template.format(classname))\n",
    "    \n",
    "    logger.debug(f\"Generated {len(all_prompts)} prompts.\")\n",
    "\n",
    "    with torch.no_grad(): # No need for gradients during text encoding\n",
    "        # Tokenize prompts - assumes tokenizer handles batching\n",
    "        tokenized_prompts = tokenizer.tokenize(all_prompts)\n",
    "        # Move tensors within the tokenized output to the model's device\n",
    "        tokenized_prompts = {k: v.to(device) for k, v in tokenized_prompts.items() if isinstance(v, torch.Tensor)}\n",
    "        \n",
    "        # Encode text prompts\n",
    "        # Note: model.encode_text expects a dictionary like {'input_ids': ..., 'attention_mask': ...}\n",
    "        # Ensure tokenizer output format matches this expectation.\n",
    "        text_embeddings = model.encode_text(tokenized_prompts)\n",
    "        \n",
    "        # Normalize text embeddings\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "\n",
    "        # Average embeddings if multiple templates were used\n",
    "        if isinstance(templates, list) and len(templates) > 1:\n",
    "            num_templates = len(templates)\n",
    "            text_embeddings = text_embeddings.view(num_templates, num_classes, -1).mean(dim=0)\n",
    "            text_embeddings = F.normalize(text_embeddings, p=2, dim=-1) # Re-normalize after averaging\n",
    "        \n",
    "        logger.debug(f\"Encoded text embeddings shape: {text_embeddings.shape}\")\n",
    "            \n",
    "        # 2. Calculate Similarity and Predict\n",
    "        # Image features should already be normalized\n",
    "        # Calculate cosine similarity (dot product of normalized features)\n",
    "        similarity = image_features @ text_embeddings.t() # Shape: [B, NumClasses]\n",
    "        logger.debug(f\"Calculated similarity shape: {similarity.shape}\")\n",
    "\n",
    "        # Get predictions (index of the class with highest similarity)\n",
    "        predictions = similarity.argmax(dim=1).cpu().numpy()\n",
    "        logger.debug(f\"Predictions shape: {predictions.shape}\")\n",
    "\n",
    "    # 3. Calculate Accuracy\n",
    "    # Ensure labels are in numpy format\n",
    "    if isinstance(image_labels, torch.Tensor):\n",
    "        image_labels = image_labels.cpu().numpy()\n",
    "    elif isinstance(image_labels, list):\n",
    "        image_labels = np.array(image_labels)\n",
    "        \n",
    "    logger.debug(f\"Ground truth labels shape: {image_labels.shape}\")\n",
    "\n",
    "    accuracy = accuracy_score(image_labels, predictions)\n",
    "    logger.info(f\"Zero-shot Top-1 Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o5s8eL8X3D5I",
   "metadata": {},
   "source": [
    "## Example Usage & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0T22vRz13D5I",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# Example usage for calculate_retrieval_metrics\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Testing Retrieval Metrics ---\")\n",
    "    B = 4 # Batch size\n",
    "    D = 512 # Embedding dimension\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create dummy normalized features\n",
    "    img_feat = F.normalize(torch.randn(B, D, device=device), dim=-1)\n",
    "    txt_feat = F.normalize(torch.randn(B, D, device=device), dim=-1)\n",
    "    logit_scale = torch.ones([], device=device) * 100 # Use a high scale for testing\n",
    "    k_vals = [1, 2]\n",
    "\n",
    "    # Make diagonal elements have higher similarity for testing\n",
    "    for i in range(B):\n",
    "        txt_feat[i] = img_feat[i] * 0.8 + F.normalize(torch.randn(D, device=device), dim=-1) * 0.2\n",
    "    txt_feat = F.normalize(txt_feat, dim=-1)\n",
    "\n",
    "    metrics = calculate_retrieval_metrics(img_feat, txt_feat, logit_scale, k_values=k_vals)\n",
    "    print(f\"Calculated Retrieval Metrics: {metrics}\")\n",
    "    assert 'i2t_r@1' in metrics\n",
    "    assert 't2i_r@1' in metrics\n",
    "    assert 'mean_recall' in metrics\n",
    "\n",
    "    print(\"\\n--- Testing Zero-Shot Classification (Requires Mock Model/Tokenizer) ---\")\n",
    "\n",
    "    # Define Mock classes for testing if modules weren't imported\n",
    "    class MockTokenizer:\n",
    "        def tokenize(self, texts):\n",
    "            # Simple tokenization: just create dummy tensors\n",
    "            max_len = 10\n",
    "            ids = torch.randint(0, 1000, (len(texts), max_len))\n",
    "            mask = torch.ones_like(ids)\n",
    "            return {'input_ids': ids, 'attention_mask': mask}\n",
    "\n",
    "    class MockIndicCLIP(torch.nn.Module):\n",
    "        def __init__(self, embed_dim):\n",
    "            super().__init__()\n",
    "            self.embed_dim = embed_dim\n",
    "            # Dummy parameters to allow moving to device\n",
    "            self.dummy_param = torch.nn.Parameter(torch.empty(1))\n",
    "\n",
    "        def encode_text(self, tokenized_input):\n",
    "            bs = tokenized_input['input_ids'].shape[0]\n",
    "            # Return random normalized embeddings\n",
    "            return F.normalize(torch.randn(bs, self.embed_dim, device=self.dummy_param.device), dim=-1)\n",
    "        \n",
    "        def eval(self):\n",
    "            pass # Mock eval mode\n",
    "\n",
    "    # Setup dummy data for zero-shot\n",
    "    B_zs = 4 # Batch size for zero-shot test\n",
    "    img_feat_zs = F.normalize(torch.randn(B_zs, D, device=device), dim=-1)\n",
    "    img_labels_zs = [0, 1, 0, 2] # Example labels (indices)\n",
    "    class_names_zs = ['cat', 'dog', 'bird']\n",
    "    templates_zs = [\"a photo of a {}\"]\n",
    "    mock_model = MockIndicCLIP(D).to(device)\n",
    "    mock_tokenizer = MockTokenizer()\n",
    "\n",
    "    accuracy = calculate_zeroshot_accuracy(\n",
    "        img_feat_zs,\n",
    "        img_labels_zs,\n",
    "        class_names_zs,\n",
    "        templates_zs,\n",
    "        mock_model,\n",
    "        mock_tokenizer\n",
    "    )\n",
    "    print(f\"Calculated Zero-Shot Accuracy (dummy): {accuracy:.4f}\")\n",
    "    assert isinstance(accuracy, float) and 0.0 <= accuracy <= 1.0\n",
    "\n",
    "    print(\"\\nTests completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "z0H5x9K63D5J",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # Run this in terminal to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F6xYx_0hI79u",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
