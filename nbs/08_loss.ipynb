{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alxkDbkRdW0K",
   "metadata": {},
   "source": [
    "# Contrastive Loss (InfoNCE)\n",
    "\n",
    "> Implements the InfoNCE loss function for CLIP training, handling distributed data parallel (DDP) correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wbWmktO6dW0L",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eWwHAwDSdW0L",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yjTnC5L1dW0L",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Google Drive mounted successfully.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Mount Google Drive (Optional, but recommended for persistent storage)\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, skipping Drive mount.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wXxfDUNFdW0M",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indic_clip.core not found initially.\n",
      "Added /content/drive/MyDrive/Indic-Clip to sys.path\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive detected, setting PROJECT_ROOT to /content/drive/MyDrive/Indic-Clip\n",
      "Ensure your project files are located there.\n",
      "Imported indic_clip.core after path adjustment.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    print(\"Reloaded indic_clip.core\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"indic_clip.core not found initially.\")\n",
    "    # Attempt to set sys.path if running in Colab and project cloned\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        project_parent = '/content' # Assuming cloned into /content/indic-clip\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "             project_parent = '/content/drive/MyDrive/Indic-Clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive\")\n",
    "            # raise # Stop execution if core components missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g6BOho0KdW0M",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Indic-Clip\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%cd /content/drive/MyDrive/Indic-Clip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hOd-lvcIdW0M",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.8/297.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m104.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "!pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jNofslRXdW0M",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bW3V00oIdW0M",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "from fastai.vision.all import *\n",
    "\n",
    "try:\n",
    "    from indic_clip.core import get_logger, setup_logging\n",
    "except ModuleNotFoundError:\n",
    "    # Fallback if core not found\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "    def setup_logging(): pass\n",
    "\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MiErC5csdW0N",
   "metadata": {},
   "source": [
    "## AllGather Helper for DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wPFxU7ATdW0N",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AllGather(torch.autograd.Function):\n",
    "    \"\"\"Custom autograd function to gather tensors from all processes, supporting gradients.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Performs the all_gather operation and prepares context for backward pass.\"\"\"\n",
    "        # Check if distributed environment is initialized\n",
    "        if not dist.is_available() or not dist.is_initialized():\n",
    "            # If not distributed, just return the input tensor\n",
    "            return tensor\n",
    "\n",
    "        # Ensure tensor is contiguous before gathering\n",
    "        tensor = tensor.contiguous()\n",
    "        world_size = dist.get_world_size()\n",
    "        # Create a list to hold tensors from all ranks\n",
    "        output = [torch.empty_like(tensor) for _ in range(world_size)]\n",
    "        # Perform the all_gather operation\n",
    "        dist.all_gather(output, tensor)\n",
    "        # Concatenate the gathered tensors along the batch dimension (dim=0)\n",
    "        gathered_tensor = torch.cat(output, dim=0)\n",
    "\n",
    "        # Save world_size for backward pass (optional, could re-fetch)\n",
    "        # ctx.world_size = world_size\n",
    "        return gathered_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Performs the reduce_scatter operation for the backward pass.\"\"\"\n",
    "        # Check if distributed environment is initialized\n",
    "        if not dist.is_available() or not dist.is_initialized():\n",
    "            # If not distributed, just return the gradient\n",
    "            return grad_output\n",
    "\n",
    "        # Ensure grad_output is contiguous\n",
    "        grad_output = grad_output.contiguous()\n",
    "        world_size = dist.get_world_size()\n",
    "\n",
    "        # Check if the gradient tensor size is divisible by world_size\n",
    "        if grad_output.shape[0] % world_size != 0:\n",
    "            raise RuntimeError(\"Gradient output size must be divisible by world size for all_gather backward pass.\")\n",
    "\n",
    "        # Calculate the chunk size for each process\n",
    "        chunk_size = grad_output.shape[0] // world_size\n",
    "\n",
    "        # Prepare the input tensor for reduce_scatter (this will hold the gradient for the current rank)\n",
    "        grad_input = torch.empty(chunk_size, *grad_output.shape[1:], dtype=grad_output.dtype, device=grad_output.device)\n",
    "\n",
    "        # Perform reduce_scatter: sums gradients corresponding to each rank's input\n",
    "        # The list comprehension splits the gathered gradient tensor back into chunks\n",
    "        dist.reduce_scatter(grad_input, list(grad_output.chunk(world_size, dim=0)), op=dist.ReduceOp.SUM)\n",
    "\n",
    "        # grad_input now contains the correct gradient sum for the input tensor on this rank\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6OPs-olJdW0N",
   "metadata": {},
   "source": [
    "## Contrastive Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hwzlOxdddW0N",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ContrastiveLoss(Module):\n",
    "    \"\"\"Calculates the contrastive loss (InfoNCE) between image and text features.\n",
    "\n",
    "    Handles distributed training by gathering features across GPUs before calculating loss.\n",
    "    Assumes input features (image_features, text_features) are already L2 normalized.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, axis:int = -1, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            args: Arguments passed to the parent BaseLoss.\n",
    "            axis (int): The axis to perform the reduction over (passed to BaseLoss).\n",
    "            kwargs: Keyword arguments passed to the parent BaseLoss.\n",
    "        \"\"\"\n",
    "        self.all_gather = AllGather.apply # Use the custom autograd function\n",
    "\n",
    "    def forward(self, preds: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"\n",
    "        Calculates the contrastive loss.\n",
    "\n",
    "        Args:\n",
    "            preds (tuple): A tuple containing:\n",
    "                - image_features (torch.Tensor): Normalized image features (B, D).\n",
    "                - text_features (torch.Tensor): Normalized text features (B, D).\n",
    "                - logit_scale (torch.Tensor): The learnable logit scaling factor (scalar tensor).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The calculated contrastive loss (scalar tensor).\n",
    "        \"\"\"\n",
    "        # logger.info(\">>> ContrastiveLoss.forward ENTERED\")\n",
    "\n",
    "        image_features, text_features, logit_scale = preds\n",
    "\n",
    "        if torch.isnan(image_features).any() or torch.isinf(image_features).any():\n",
    "            logger.error(\"!!! NaN/Inf DETECTED IN INPUT image_features !!!\")\n",
    "        if torch.isnan(text_features).any() or torch.isinf(text_features).any():\n",
    "            logger.error(\"!!! NaN/Inf DETECTED IN INPUT text_features !!!\")\n",
    "        if torch.isnan(logit_scale).any() or torch.isinf(logit_scale).any():\n",
    "            logger.error(f\"!!! NaN/Inf DETECTED IN INPUT logit_scale: {logit_scale.item()} !!!\")\n",
    "\n",
    "        # logger.info(f\"Input shapes: Img={image_features.shape}, Txt={text_features.shape}, Scale={logit_scale.shape}\")\n",
    "        # logger.info(f\"Input norms (mean): Img={image_features.norm(dim=-1).mean().item():.4f}, Txt={text_features.norm(dim=-1).mean().item():.4f}\")\n",
    "        # logger.info(f\"Logit Scale value: {logit_scale.item():.4f}\")\n",
    "\n",
    "        # --- Gather Features in Distributed Setting ---\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            gathered_image_features = self.all_gather(image_features)\n",
    "            gathered_text_features = self.all_gather(text_features)\n",
    "            world_size = dist.get_world_size()\n",
    "        else:\n",
    "            gathered_image_features = image_features\n",
    "            gathered_text_features = text_features\n",
    "            world_size = 1\n",
    "\n",
    "        # --- Calculate Similarity Scores ---\n",
    "        # Note: logit_scale is applied *before* softmax in cross_entropy\n",
    "        # We use the raw logit_scale parameter and apply exp() inside the loss calculation if needed,\n",
    "        # or directly multiply as CLIP does.\n",
    "        # The forward pass of IndicCLIP already returns exp(logit_scale).\n",
    "        # Here, we assume logit_scale passed in is already exponentiated.\n",
    "\n",
    "        # Cosine similarity as dot product of normalized features\n",
    "        # logits_per_image: How well each image matches each text [Global B, Global B]\n",
    "        logits_per_image = logit_scale * gathered_image_features @ gathered_text_features.t()\n",
    "        # logits_per_text: How well each text matches each image [Global B, Global B]\n",
    "        logits_per_text = logits_per_image.t() # More efficient than recalculating\n",
    "\n",
    "        # --- Calculate Loss ---\n",
    "        # Create ground truth labels. The diagonal elements (i,i) correspond to matching pairs.\n",
    "        local_batch_size = image_features.size(0)\n",
    "        global_batch_size = gathered_image_features.size(0)\n",
    "\n",
    "        # Ensure calculation happens on the correct device\n",
    "        device = image_features.device\n",
    "        labels = torch.arange(global_batch_size, device=device, dtype=torch.long)\n",
    "\n",
    "        if torch.isnan(logits_per_image).any() or torch.isinf(logits_per_image).any():\n",
    "          logger.warning(\"NaN/Inf detected in logits_per_image!\")\n",
    "          # Optionally print min/max/mean\n",
    "          logger.warning(f\"Logit Scale (exp): {logit_scale.item()}\")\n",
    "          logger.warning(f\"Image Features Norm: {gathered_image_features.norm(dim=-1).mean().item()}\")\n",
    "          logger.warning(f\"Text Features Norm: {gathered_text_features.norm(dim=-1).mean().item()}\")\n",
    "\n",
    "\n",
    "        # Calculate cross-entropy loss for both directions\n",
    "        loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "\n",
    "        # Average the two losses\n",
    "        total_loss = (loss_img + loss_txt) / 2\n",
    "\n",
    "        # logger.info(f\"loss_img: {loss_img}, loss_txt: {loss_txt}, total loss: {total_loss}\")\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GCEO7CQ_dW0N",
   "metadata": {},
   "source": [
    "## Example Usage and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xLP5hiQ8dW0N",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Testing ContrastiveLoss (Non-Distributed) ---\n",
      "Input Shapes:\n",
      "  Image Features: torch.Size([4, 512])\n",
      "  Text Features:  torch.Size([4, 512])\n",
      "  Logit Scale:    torch.Size([])\n",
      "Output Loss: 1.4547 (Type: <class 'torch.Tensor'>, Device: cpu)\n",
      "\n",
      "--- Testing ContrastiveLoss (Simulated Distributed, World Size=2) ---\n",
      "Distributed environment not available/initialized. Skipping DDP test.\n",
      "Note: To run the distributed test, initialize a process group first.\n",
      "Example (requires torchrun or similar):\n",
      "  import torch.distributed as dist\n",
      "  dist.init_process_group(backend='nccl') # Or 'gloo' for CPU\n",
      "  # ... run the test code ...\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Testing ContrastiveLoss (Non-Distributed) ---\")\n",
    "    # Setup dummy data\n",
    "    batch_size = 4\n",
    "    embed_dim = 512\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Simulate normalized features\n",
    "    img_feat = F.normalize(torch.randn(batch_size, embed_dim, device=device), dim=-1)\n",
    "    txt_feat = F.normalize(torch.randn(batch_size, embed_dim, device=device), dim=-1)\n",
    "    # Simulate logit scale from model (already exponentiated)\n",
    "    logit_scale = torch.exp(torch.ones([], device=device) * torch.log(torch.tensor(1/0.07)))\n",
    "\n",
    "    print(\"Input Shapes:\")\n",
    "    print(f\"  Image Features: {img_feat.shape}\")\n",
    "    print(f\"  Text Features:  {txt_feat.shape}\")\n",
    "    print(f\"  Logit Scale:    {logit_scale.shape}\")\n",
    "\n",
    "    # Instantiate loss\n",
    "    loss_fn = ContrastiveLoss()\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_val = loss_fn((img_feat, txt_feat, logit_scale))\n",
    "\n",
    "    print(f\"Output Loss: {loss_val.item():.4f} (Type: {type(loss_val)}, Device: {loss_val.device})\")\n",
    "    assert isinstance(loss_val, torch.Tensor) and loss_val.ndim == 0\n",
    "\n",
    "    # --- Test Distributed Scenario (Simulated) ---\n",
    "    print(\"\\n--- Testing ContrastiveLoss (Simulated Distributed, World Size=2) ---\")\n",
    "\n",
    "    if dist.is_available() and not dist.is_initialized():\n",
    "        # This part requires initializing a process group, usually done via torchrun/launch\n",
    "        # We can't fully simulate it here without that setup.\n",
    "        print(\"Distributed environment not available/initialized. Skipping DDP test.\")\n",
    "        print(\"Note: To run the distributed test, initialize a process group first.\")\n",
    "        print(\"Example (requires torchrun or similar):\")\n",
    "        print(\"  import torch.distributed as dist\")\n",
    "        print(\"  dist.init_process_group(backend='nccl') # Or 'gloo' for CPU\")\n",
    "        print(\"  # ... run the test code ...\")\n",
    "\n",
    "    elif dist.is_available() and dist.is_initialized():\n",
    "        # This block would run if a process group *is* initialized\n",
    "        world_size = dist.get_world_size()\n",
    "        rank = dist.get_rank()\n",
    "        print(f\"Running DDP test on Rank {rank}/{world_size}\")\n",
    "\n",
    "        # Assume img_feat, txt_feat, logit_scale are the local tensors for this rank\n",
    "        loss_val_ddp = loss_fn((img_feat, txt_feat, logit_scale))\n",
    "\n",
    "        print(f\"DDP Output Loss (Rank {rank}): {loss_val_ddp.item():.4f}\")\n",
    "        assert isinstance(loss_val_ddp, torch.Tensor) and loss_val_ddp.ndim == 0\n",
    "        # Note: Loss value might differ across ranks if inputs are different,\n",
    "        # but the calculation mechanism (gathering) is tested.\n",
    "        # For identical inputs across ranks (less realistic), losses should match.\n",
    "\n",
    "    else:\n",
    "         print(\"Torch distributed is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sK-pm2I-dW0N",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # Run this in terminal to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6idgPXn8gq1o",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
