{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alxkDbkRdW0K",
   "metadata": {},
   "source": [
    "# Contrastive Loss (InfoNCE)\n",
    "\n",
    "> Implements the InfoNCE loss function for CLIP training, handling distributed data parallel (DDP) correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wbWmktO6dW0L",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eWwHAwDSdW0L",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yjTnC5L1dW0L",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Mount Google Drive (Optional, but recommended for persistent storage)\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, skipping Drive mount.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wXxfDUNFdW0M",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    print(\"Reloaded indic_clip.core\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"indic_clip.core not found initially.\")\n",
    "    # Attempt to set sys.path if running in Colab and project cloned\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        project_parent = '/content' # Assuming cloned into /content/indic-clip\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "             project_parent = '/content/drive/MyDrive/Indic-Clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive\")\n",
    "            # raise # Stop execution if core components missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g6BOho0KdW0M",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%cd /content/drive/MyDrive/Indic-Clip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hOd-lvcIdW0M",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "!pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jNofslRXdW0M",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bW3V00oIdW0M",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributed as dist\n",
    "import logging\n",
    "from typing import Optional\n",
    "\n",
    "from fastai.vision.all import *\n",
    "\n",
    "try:\n",
    "    from indic_clip.core import get_logger, setup_logging\n",
    "except ModuleNotFoundError:\n",
    "    # Fallback if core not found\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "    def setup_logging(): pass\n",
    "\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MiErC5csdW0N",
   "metadata": {},
   "source": [
    "## AllGather Helper for DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wPFxU7ATdW0N",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AllGather(torch.autograd.Function):\n",
    "    \"\"\"Custom autograd function to gather tensors from all processes, supporting gradients.\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Performs the all_gather operation and prepares context for backward pass.\"\"\"\n",
    "        # Check if distributed environment is initialized\n",
    "        if not dist.is_available() or not dist.is_initialized():\n",
    "            # If not distributed, just return the input tensor\n",
    "            return tensor\n",
    "\n",
    "        # Ensure tensor is contiguous before gathering\n",
    "        tensor = tensor.contiguous()\n",
    "        world_size = dist.get_world_size()\n",
    "        # Create a list to hold tensors from all ranks\n",
    "        output = [torch.empty_like(tensor) for _ in range(world_size)]\n",
    "        # Perform the all_gather operation\n",
    "        dist.all_gather(output, tensor)\n",
    "        # Concatenate the gathered tensors along the batch dimension (dim=0)\n",
    "        gathered_tensor = torch.cat(output, dim=0)\n",
    "\n",
    "        # Save world_size for backward pass (optional, could re-fetch)\n",
    "        # ctx.world_size = world_size\n",
    "        return gathered_tensor\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Performs the reduce_scatter operation for the backward pass.\"\"\"\n",
    "        # Check if distributed environment is initialized\n",
    "        if not dist.is_available() or not dist.is_initialized():\n",
    "            # If not distributed, just return the gradient\n",
    "            return grad_output\n",
    "\n",
    "        logger.warning(\"!!! AllGather DEBUG: Returning SLICE of grad_output in backward pass !!!\")\n",
    "        # Ensure grad_output is contiguous\n",
    "        grad_output = grad_output.contiguous()\n",
    "        world_size = dist.get_world_size()\n",
    "\n",
    "        # Check if the gradient tensor size is divisible by world_size\n",
    "        if grad_output.shape[0] % world_size != 0:\n",
    "            raise RuntimeError(\"Gradient output size must be divisible by world size for all_gather backward pass.\")\n",
    "\n",
    "        # Calculate the chunk size for each process\n",
    "        chunk_size = grad_output.shape[0] // world_size\n",
    "\n",
    "        # Prepare the input tensor for reduce_scatter (this will hold the gradient for the current rank)\n",
    "        grad_input = torch.empty(chunk_size, *grad_output.shape[1:], dtype=grad_output.dtype, device=grad_output.device)\n",
    "\n",
    "        # Perform reduce_scatter: sums gradients corresponding to each rank's input\n",
    "        # The list comprehension splits the gathered gradient tensor back into chunks\n",
    "        dist.reduce_scatter(grad_input, list(grad_output.chunk(world_size, dim=0)), op=dist.ReduceOp.SUM)\n",
    "\n",
    "        # grad_input now contains the correct gradient sum for the input tensor on this rank\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6OPs-olJdW0N",
   "metadata": {},
   "source": [
    "## Contrastive Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hwzlOxdddW0N",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| export\n",
    "class ContrastiveLoss(Module): # Inherit from fastai's Module or nn.Module\n",
    "    \"\"\"Calculates the contrastive loss (InfoNCE) between image and text features.\n",
    "    Handles distributed training by gathering features across GPUs before calculating loss.\n",
    "    Designed to be used directly as Learner's loss_func.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.all_gather = AllGather.apply\n",
    "\n",
    "    # *** IMPORTANT: Match fastai loss signature ***\n",
    "    def forward(self, preds: tuple[torch.Tensor, torch.Tensor, torch.Tensor], *target) -> torch.Tensor:\n",
    "        # preds is the output from the model: (image_features, text_features, logit_scale)\n",
    "        # target is the dummy category batch, which we ignore here.\n",
    "        # logger.critical(\"LOSS FORWARD CALLED\") # Use critical for visibility\n",
    "\n",
    "        image_features, text_features, logit_scale = preds\n",
    "        device = image_features.device # Get the device from one of the inputs\n",
    "\n",
    "        # --- Check features JUST BEFORE gathering/similarity calculation ---\n",
    "        if torch.isnan(image_features).any():\n",
    "            logger.error(\"!!! NaN DETECTED IN image_features JUST BEFORE ContrastiveLoss similarity calc !!!\")\n",
    "            return torch.tensor(torch.nan, device=device, requires_grad=True)\n",
    "        if torch.isnan(text_features).any():\n",
    "            logger.error(\"!!! NaN DETECTED IN text_features JUST BEFORE ContrastiveLoss similarity calc !!!\")\n",
    "            return torch.tensor(torch.nan, device=device, requires_grad=True)\n",
    "        if torch.isnan(logit_scale):\n",
    "            logger.error(f\"!!! NaN DETECTED IN logit_scale JUST BEFORE ContrastiveLoss similarity calc: {logit_scale.item()} !!!\")\n",
    "            return torch.tensor(torch.nan, device=device, requires_grad=True)\n",
    "\n",
    "        # --- Check norms AGAIN here (should be ~1.0) ---\n",
    "        img_norms = image_features.norm(p=2, dim=-1)\n",
    "        txt_norms = text_features.norm(p=2, dim=-1)\n",
    "        if img_norms.min() < 0.9 or img_norms.max() > 1.1:\n",
    "            logger.warning(f\"Image feature norms deviate significantly from 1.0: min={img_norms.min().item()}, max={img_norms.max().item()}\")\n",
    "        if txt_norms.min() < 0.9 or txt_norms.max() > 1.1:\n",
    "            logger.warning(f\"Text feature norms deviate significantly from 1.0: min={txt_norms.min().item()}, max={txt_norms.max().item()}\")\n",
    "\n",
    "        # --- Gather Features Across GPUs (if applicable) ---\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            logger.debug(\"Gathering features across GPUs.\")\n",
    "            gathered_image_features = self.all_gather(image_features)\n",
    "            gathered_text_features = self.all_gather(text_features)\n",
    "            # Logit scale is usually not gathered, assuming it's the same across devices\n",
    "            # or handled differently (e.g., only rank 0 updates it).\n",
    "        else:\n",
    "            logger.debug(\"Not a distributed setup. Using local features.\")\n",
    "            gathered_image_features = image_features\n",
    "            gathered_text_features = text_features\n",
    "        # ---------------------------------------------------\n",
    "\n",
    "        # --- Calculate Similarity ---\n",
    "        # Note: logit_scale is used directly (already exponentiated by the model)\n",
    "        logits_per_image = logit_scale * gathered_image_features @ gathered_text_features.t()\n",
    "        # --- Check Logits Immediately ---\n",
    "        if torch.isnan(logits_per_image).any() or torch.isinf(logits_per_image).any():\n",
    "            logger.error(\"!!! NaN/Inf detected in logits_per_image AFTER similarity calculation !!!\")\n",
    "            logger.error(f\"Logit Scale was: {logit_scale.item():.4f}\")\n",
    "            logger.error(f\"Gathered Img Feat Stats: mean={gathered_image_features.mean().item()}, std={gathered_image_features.std().item()}, min={gathered_image_features.min().item()}, max={gathered_image_features.max().item()}\")\n",
    "            logger.error(f\"Gathered Txt Feat Stats: mean={gathered_text_features.mean().item()}, std={gathered_text_features.std().item()}, min={gathered_text_features.min().item()}, max={gathered_text_features.max().item()}\")\n",
    "            return torch.tensor(torch.nan, device=device, requires_grad=True) # Return NaN\n",
    "\n",
    "        logits_per_text = logits_per_image.t() # Efficient transpose\n",
    "\n",
    "        # --- Create Labels ---\n",
    "        # Ground truth is the identity matrix diagonal (image i matches text i)\n",
    "        # The size should match the batch size *after* gathering\n",
    "        gathered_batch_size = gathered_image_features.shape[0]\n",
    "        labels = torch.arange(gathered_batch_size, device=device, dtype=torch.long)\n",
    "        # ---------------------\n",
    "\n",
    "        # --- Calculate Loss ---\n",
    "        loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "        loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "\n",
    "        # --- Check component losses ---\n",
    "        if torch.isnan(loss_img): logger.error(f\"!!! loss_img is NaN !!! Input logits max: {logits_per_image.max().item()}\")\n",
    "        if torch.isnan(loss_txt): logger.error(f\"!!! loss_txt is NaN !!! Input logits max: {logits_per_text.max().item()}\")\n",
    "\n",
    "        total_loss = (loss_img + loss_txt) / 2\n",
    "\n",
    "        # --- Final NaN check ---\n",
    "        if torch.isnan(total_loss):\n",
    "            logger.error(f\"!!! total_loss is NaN !!! loss_img={loss_img.item()}, loss_txt={loss_txt.item()}\")\n",
    "            return torch.tensor(torch.nan, device=device, requires_grad=True) # Return NaN if components were NaN\n",
    "\n",
    "        # logger.info(f\"Returning total_loss: value={total_loss.item()}, shape={total_loss.shape}, dtype={total_loss.dtype}, device={total_loss.device}, requires_grad={total_loss.requires_grad}\")\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GCEO7CQ_dW0N",
   "metadata": {},
   "source": [
    "## Example Usage and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xLP5hiQ8dW0N",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Testing ContrastiveLoss (Non-Distributed) ---\")\n",
    "    # Setup dummy data\n",
    "    batch_size = 4\n",
    "    embed_dim = 512\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Simulate normalized features\n",
    "    img_feat = F.normalize(torch.randn(batch_size, embed_dim, device=device), dim=-1)\n",
    "    txt_feat = F.normalize(torch.randn(batch_size, embed_dim, device=device), dim=-1)\n",
    "    # Simulate logit scale from model (already exponentiated)\n",
    "    logit_scale = torch.exp(torch.ones([], device=device) * torch.log(torch.tensor(1/0.07)))\n",
    "\n",
    "    print(\"Input Shapes:\")\n",
    "    print(f\"  Image Features: {img_feat.shape}\")\n",
    "    print(f\"  Text Features:  {txt_feat.shape}\")\n",
    "    print(f\"  Logit Scale:    {logit_scale.shape}\")\n",
    "\n",
    "    # Instantiate loss\n",
    "    loss_fn = ContrastiveLoss()\n",
    "\n",
    "    # Calculate loss\n",
    "    loss_val = loss_fn((img_feat, txt_feat, logit_scale))\n",
    "\n",
    "    print(f\"Output Loss: {loss_val.item():.4f} (Type: {type(loss_val)}, Device: {loss_val.device})\")\n",
    "    assert isinstance(loss_val, torch.Tensor) and loss_val.ndim == 0\n",
    "\n",
    "    # --- Test Distributed Scenario (Simulated) ---\n",
    "    print(\"\\n--- Testing ContrastiveLoss (Simulated Distributed, World Size=2) ---\")\n",
    "\n",
    "    if dist.is_available() and not dist.is_initialized():\n",
    "        # This part requires initializing a process group, usually done via torchrun/launch\n",
    "        # We can't fully simulate it here without that setup.\n",
    "        print(\"Distributed environment not available/initialized. Skipping DDP test.\")\n",
    "        print(\"Note: To run the distributed test, initialize a process group first.\")\n",
    "        print(\"Example (requires torchrun or similar):\")\n",
    "        print(\"  import torch.distributed as dist\")\n",
    "        print(\"  dist.init_process_group(backend='nccl') # Or 'gloo' for CPU\")\n",
    "        print(\"  # ... run the test code ...\")\n",
    "\n",
    "    elif dist.is_available() and dist.is_initialized():\n",
    "        # This block would run if a process group *is* initialized\n",
    "        world_size = dist.get_world_size()\n",
    "        rank = dist.get_rank()\n",
    "        print(f\"Running DDP test on Rank {rank}/{world_size}\")\n",
    "\n",
    "        # Assume img_feat, txt_feat, logit_scale are the local tensors for this rank\n",
    "        loss_val_ddp = loss_fn((img_feat, txt_feat, logit_scale))\n",
    "\n",
    "        print(f\"DDP Output Loss (Rank {rank}): {loss_val_ddp.item():.4f}\")\n",
    "        assert isinstance(loss_val_ddp, torch.Tensor) and loss_val_ddp.ndim == 0\n",
    "        # Note: Loss value might differ across ranks if inputs are different,\n",
    "        # but the calculation mechanism (gathering) is tested.\n",
    "        # For identical inputs across ranks (less realistic), losses should match.\n",
    "\n",
    "    else:\n",
    "         print(\"Torch distributed is not available on this system.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sK-pm2I-dW0N",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # Run this in terminal to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6idgPXn8gq1o",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
