{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "> Functions for cleaning and filtering raw image-text pair data. Includes image dimension/aspect ratio filtering, text length filtering, and duplicate removal using image hashing and text matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indic_clip.core not found initially.\n",
      "Added /workspace/indic-clip to sys.path\n",
      "Imported indic_clip.core after path adjustment.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    print(\"Reloaded indic_clip.core\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"indic_clip.core not found initially.\")\n",
    "    # Attempt to set sys.path if running in Colab and project cloned\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        project_parent = '/content' # Assuming cloned into /content/indic-clip\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "             project_parent = '/content/drive/MyDrive/Indic-Clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive\")\n",
    "            # raise # Stop execution if core components missing\n",
    "    else:\n",
    "        project_parent = '/workspace'\n",
    "        if Path('/workspace/indic-clip').exists():\n",
    "             project_parent = '/workspace/indic-clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /workspace/indic-clip/indic-clip/core.py or similar in Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import imagehash\n",
    "import pandas as pd # Optional, but useful for handling dataframes\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "try:\n",
    "    from indic_clip.core import (\n",
    "        HINDI_RAW_PATH,\n",
    "        SANSKRIT_RAW_PATH,\n",
    "        SYNTHETIC_RAW_PATH,\n",
    "        PROCESSED_DATA_PATH,\n",
    "        get_logger,\n",
    "        setup_logging,\n",
    "        ensure_dir\n",
    "    )\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Error importing from indic_clip.core. Using Fallbacks.\")\n",
    "    # Fallbacks if core isn't importable (e.g., interactive testing)\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    if 'google.colab' in sys.modules:\n",
    "        PROJECT_ROOT=Path('/content/Indic-Clip')\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "           PROJECT_ROOT=Path('/content/drive/MyDrive/Indic-Clip')\n",
    "    else:\n",
    "        PROJECT_ROOT=Path('.').resolve()\n",
    "        if PROJECT_ROOT.name == 'nbs': PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    DATA_PATH = PROJECT_ROOT / 'data'\n",
    "    RAW_DATA_PATH = DATA_PATH / 'raw'\n",
    "    HINDI_RAW_PATH = RAW_DATA_PATH / 'hindi'\n",
    "    SANSKRIT_RAW_PATH = RAW_DATA_PATH / 'sanskrit'\n",
    "    SYNTHETIC_RAW_PATH = RAW_DATA_PATH / 'synthetic'\n",
    "    PROCESSED_DATA_PATH = DATA_PATH / 'processed'\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "    def setup_logging(): pass\n",
    "    def ensure_dir(path: Path): path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# --- Filtering Thresholds ---\n",
    "MIN_IMAGE_RESOLUTION = 224 # Minimum width and height\n",
    "MAX_IMAGE_RESOLUTION = 4096 # Optional: Maximum width/height\n",
    "MIN_ASPECT_RATIO = 1/3   # Allow images like 1:3\n",
    "MAX_ASPECT_RATIO = 3     # Allow images like 3:1\n",
    "MIN_TEXT_LENGTH = 5      # Minimum number of characters in caption\n",
    "MAX_TEXT_LENGTH = 256    # Maximum number of characters in caption\n",
    "\n",
    "# --- Output ---\n",
    "FILTERED_DATA_FILENAME = \"filtered_data.jsonl\"\n",
    "FILTERED_OUTPUT_PATH = PROCESSED_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_raw_data(jsonl_path: Path) -> list:\n",
    "    \"\"\"Loads raw data from a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        jsonl_path: Path to the input JSONL file (e.g., flickr8k_hindi_raw.jsonl).\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries, where each dictionary represents a row.\n",
    "        Returns an empty list if the file is not found or is empty.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    if not jsonl_path.exists():\n",
    "        logger.error(f\"Raw data file not found: {jsonl_path}\")\n",
    "        return data\n",
    "\n",
    "    try:\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data.append(json.loads(line.strip()))\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warning(f\"Skipping invalid JSON line in {jsonl_path}: {line.strip()}\")\n",
    "        logger.info(f\"Loaded {len(data)} items from {jsonl_path}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading raw data from {jsonl_path}: {e}\")\n",
    "        return [] # Return empty list on error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_image_metadata(image_path: Path) -> dict | None:\n",
    "    \"\"\"Gets metadata (width, height, aspect ratio) for an image.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary {'width': int, 'height': int, 'aspect_ratio': float} or None if error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "            if width == 0 or height == 0:\n",
    "                logger.warning(f\"Image has zero dimension: {image_path} (Size: {width}x{height})\")\n",
    "                return None\n",
    "            aspect_ratio = width / height\n",
    "            return {\"width\": width, \"height\": height, \"aspect_ratio\": aspect_ratio}\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"Image file not found: {image_path}\")\n",
    "        return None\n",
    "    except UnidentifiedImageError:\n",
    "        logger.warning(f\"Could not identify image file (possibly corrupt): {image_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def calculate_image_hash(image_path: Path) -> str | None:\n",
    "    \"\"\"Calculates the perceptual hash (phash) of an image.\n",
    "\n",
    "    Args:\n",
    "        image_path: Path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        The phash string or None if hashing fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            img_hash = imagehash.phash(img)\n",
    "            return str(img_hash)\n",
    "    except FileNotFoundError:\n",
    "        # Already logged in get_image_metadata typically, but can log again if needed\n",
    "        # logger.warning(f\"Image file not found for hashing: {image_path}\")\n",
    "        return None\n",
    "    except UnidentifiedImageError:\n",
    "        # logger.warning(f\"Could not identify image file for hashing: {image_path}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating hash for image {image_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def filter_data(raw_data: list, base_image_path: Path) -> list:\n",
    "    \"\"\"Applies filtering rules to the raw data.\n",
    "\n",
    "    Filters include:\n",
    "    - Image resolution (min/max)\n",
    "    - Image aspect ratio (min/max)\n",
    "    - Text length (min/max)\n",
    "    - Duplicate removal (based on image phash OR exact text match)\n",
    "\n",
    "    Args:\n",
    "        raw_data: List of dictionaries loaded from raw JSONL.\n",
    "                  Expected keys: 'image_path_relative', 'caption'.\n",
    "        base_image_path: The base directory where images corresponding to\n",
    "                         'image_path_relative' are stored (e.g., HINDI_RAW_PATH).\n",
    "\n",
    "    Returns:\n",
    "        A list of filtered dictionaries.\n",
    "    \"\"\"\n",
    "    filtered_list = []\n",
    "    seen_image_hashes = set()\n",
    "    seen_captions = set()\n",
    "    skipped_counts = {\n",
    "        \"invalid_entry\": 0,\n",
    "        \"image_error\": 0,\n",
    "        \"resolution\": 0,\n",
    "        \"aspect_ratio\": 0,\n",
    "        \"text_length\": 0,\n",
    "        \"duplicate_image\": 0,\n",
    "        \"duplicate_caption\": 0,\n",
    "    }\n",
    "\n",
    "    logger.info(f\"Starting filtering process for {len(raw_data)} raw items...\")\n",
    "\n",
    "    for item in tqdm(raw_data, desc=\"Filtering Data\"):\n",
    "        if not isinstance(item, dict) or 'image_path_relative' not in item or 'caption' not in item:\n",
    "            skipped_counts[\"invalid_entry\"] += 1\n",
    "            # logger.warning(f\"Skipping invalid entry: {item}\")\n",
    "            continue\n",
    "\n",
    "        relative_img_path = item['image_path_relative']\n",
    "        caption = item['caption']\n",
    "        full_image_path = base_image_path / relative_img_path\n",
    "\n",
    "        # 1. Filter by Image Metadata\n",
    "        metadata = get_image_metadata(full_image_path)\n",
    "        if metadata is None:\n",
    "            skipped_counts[\"image_error\"] += 1\n",
    "            continue # Skip if image can't be opened or has errors\n",
    "\n",
    "        if not (MIN_IMAGE_RESOLUTION <= metadata['width'] <= MAX_IMAGE_RESOLUTION and\n",
    "                MIN_IMAGE_RESOLUTION <= metadata['height'] <= MAX_IMAGE_RESOLUTION):\n",
    "            skipped_counts[\"resolution\"] += 1\n",
    "            # logger.debug(f\"Skipping {relative_img_path}: Resolution ({metadata['width']}x{metadata['height']}) out of bounds.\")\n",
    "            continue\n",
    "\n",
    "        if not (MIN_ASPECT_RATIO <= metadata['aspect_ratio'] <= MAX_ASPECT_RATIO):\n",
    "            skipped_counts[\"aspect_ratio\"] += 1\n",
    "            # logger.debug(f\"Skipping {relative_img_path}: Aspect ratio ({metadata['aspect_ratio']:.2f}) out of bounds.\")\n",
    "            continue\n",
    "\n",
    "        # 2. Filter by Text Length\n",
    "        # Ensure caption is a string before checking length\n",
    "        if not isinstance(caption, str) or not (MIN_TEXT_LENGTH <= len(caption) <= MAX_TEXT_LENGTH):\n",
    "            skipped_counts[\"text_length\"] += 1\n",
    "            # logger.debug(f\"Skipping {relative_img_path}: Caption length ({len(caption) if isinstance(caption, str) else 'N/A'}) out of bounds.\")\n",
    "            continue\n",
    "\n",
    "        # 3. Filter by Duplicates\n",
    "        img_hash = calculate_image_hash(full_image_path)\n",
    "\n",
    "        # Use normalized caption for duplicate checking if needed later\n",
    "        # For now, use exact match on the raw caption\n",
    "        normalized_caption = caption.strip() # Basic normalization\n",
    "\n",
    "        is_duplicate = False\n",
    "        if img_hash is not None and img_hash in seen_image_hashes:\n",
    "            skipped_counts[\"duplicate_image\"] += 1\n",
    "            is_duplicate = True\n",
    "            # logger.debug(f\"Skipping {relative_img_path}: Duplicate image hash ({img_hash}).\")\n",
    "\n",
    "        if normalized_caption in seen_captions:\n",
    "             # Only count as caption duplicate if not already counted as image duplicate\n",
    "            if not is_duplicate:\n",
    "                 skipped_counts[\"duplicate_caption\"] += 1\n",
    "                 is_duplicate = True\n",
    "            # logger.debug(f\"Skipping {relative_img_path}: Duplicate caption.\")\n",
    "\n",
    "        if is_duplicate:\n",
    "            continue\n",
    "\n",
    "        # If all filters passed, add to list and update seen sets\n",
    "        filtered_list.append(item)\n",
    "        if img_hash is not None:\n",
    "            seen_image_hashes.add(img_hash)\n",
    "        seen_captions.add(normalized_caption)\n",
    "\n",
    "    logger.info(f\"Finished filtering. Kept {len(filtered_list)} items.\")\n",
    "    logger.info(f\"Skipped counts: {skipped_counts}\")\n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 10:49:40 - __main__ - INFO - --- Running Data Preprocessing Script (Basic Filtering) ---\n",
      "2025-04-22 10:49:40 - __main__ - INFO - Loading Hindi raw data from /workspace/indic-clip/data/raw/hindi/flickr8k_hindi_raw.jsonl...\n",
      "2025-04-22 10:49:40 - __main__ - INFO - Loaded 8090 items from /workspace/indic-clip/data/raw/hindi/flickr8k_hindi_raw.jsonl\n",
      "2025-04-22 10:49:40 - __main__ - INFO - Filtering Hindi data...\n",
      "2025-04-22 10:49:40 - __main__ - INFO - Starting filtering process for 8090 raw items...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a431823c723486d99bf085e62b5b131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filtering Data:   0%|          | 0/8090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 10:50:06 - __main__ - INFO - Finished filtering. Kept 8006 items.\n",
      "2025-04-22 10:50:06 - __main__ - INFO - Skipped counts: {'invalid_entry': 0, 'image_error': 0, 'resolution': 48, 'aspect_ratio': 0, 'text_length': 3, 'duplicate_image': 1, 'duplicate_caption': 32}\n",
      "2025-04-22 10:50:06 - __main__ - INFO - Saving 8006 filtered items to /workspace/indic-clip/data/processed/filtered_data.jsonl...\n",
      "2025-04-22 10:50:06 - __main__ - INFO - Successfully saved filtered data.\n",
      "2025-04-22 10:50:06 - __main__ - INFO - --- Data Preprocessing Script (Basic Filtering) Finished ---\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "if __name__ == '__main__':\n",
    "    logger.info(\"--- Running Data Preprocessing Script (Basic Filtering) ---\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    # Define input raw data file(s) - Process Hindi data first\n",
    "    # We'll add logic later to process Sanskrit/Synthetic if needed\n",
    "    hindi_raw_jsonl = HINDI_RAW_PATH / 'flickr8k_hindi_raw.jsonl'\n",
    "    # sanskrit_raw_jsonl = SANSKRIT_RAW_PATH / 'sanskrit_raw.jsonl'\n",
    "    # synthetic_raw_jsonl = SYNTHETIC_RAW_PATH / 'synthetic_raw.jsonl'\n",
    "\n",
    "    # Base paths for images corresponding to each source\n",
    "    hindi_image_base = HINDI_RAW_PATH\n",
    "    # sanskrit_image_base = SANSKRIT_RAW_PATH # Adjust if images are in subdirs\n",
    "    # synthetic_image_base = SYNTHETIC_RAW_PATH # Adjust if images are in subdirs\n",
    "\n",
    "    # --- Load Raw Data ---\n",
    "    logger.info(f\"Loading Hindi raw data from {hindi_raw_jsonl}...\")\n",
    "    raw_hindi_data = load_raw_data(hindi_raw_jsonl)\n",
    "\n",
    "    # --- Apply Filtering ---\n",
    "    all_filtered_data = []\n",
    "    if raw_hindi_data:\n",
    "        logger.info(\"Filtering Hindi data...\")\n",
    "        filtered_hindi = filter_data(raw_hindi_data, hindi_image_base)\n",
    "        all_filtered_data.extend(filtered_hindi)\n",
    "    else:\n",
    "        logger.warning(\"No Hindi raw data loaded, skipping filtering.\")\n",
    "\n",
    "    # TODO: Add similar loading and filtering steps for Sanskrit and Synthetic data\n",
    "    # when those sources become available. Ensure to use the correct base_image_path.\n",
    "    # Example:\n",
    "    # raw_sanskrit_data = load_raw_data(sanskrit_raw_jsonl)\n",
    "    # if raw_sanskrit_data:\n",
    "    #     logger.info(\"Filtering Sanskrit data...\")\n",
    "    #     filtered_sanskrit = filter_data(raw_sanskrit_data, sanskrit_image_base)\n",
    "    #     all_filtered_data.extend(filtered_sanskrit) # Append keeping track of source if needed\n",
    "\n",
    "    # --- Save Filtered Data ---\n",
    "    output_filepath = FILTERED_OUTPUT_PATH / FILTERED_DATA_FILENAME\n",
    "    logger.info(f\"Saving {len(all_filtered_data)} filtered items to {output_filepath}...\")\n",
    "    ensure_dir(FILTERED_OUTPUT_PATH)\n",
    "    try:\n",
    "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
    "            for item in all_filtered_data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        logger.info(f\"Successfully saved filtered data.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving filtered data to {output_filepath}: {e}\")\n",
    "\n",
    "    logger.info(\"--- Data Preprocessing Script (Basic Filtering) Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "# nbdev.nbdev_export() # Run this in terminal to export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
