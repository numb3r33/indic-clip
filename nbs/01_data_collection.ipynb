{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "> Functions and tools for acquiring image-text pair data, primarily using existing datasets from Kaggle. Includes Colab setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup (Run these cells if using Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries\n",
    "# Note: nbdev install might be needed if running nbdev commands\n",
    "# !pip install -qr /workspace/indic-clip/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running in Colab, assuming local Kaggle setup.\n"
     ]
    }
   ],
   "source": [
    "# Kaggle API Setup: Upload your kaggle.json file\n",
    "try:\n",
    "    from google.colab import files\n",
    "    import os\n",
    "\n",
    "    # Create .kaggle directory if it doesn't exist\n",
    "    kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "    if not os.path.exists(kaggle_dir):\n",
    "        os.makedirs(kaggle_dir)\n",
    "        print(f\"Created directory: {kaggle_dir}\")\n",
    "\n",
    "    # Check if kaggle.json already exists\n",
    "    kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "    if not os.path.exists(kaggle_json_path):\n",
    "        print(\"Please upload your kaggle.json file:\")\n",
    "        uploaded = files.upload() # This prompts the user to upload\n",
    "\n",
    "        for fn in uploaded.keys():\n",
    "            if fn == 'kaggle.json':\n",
    "                print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
    "                # Move the uploaded file to the correct location\n",
    "                !mkdir -p ~/.kaggle/\n",
    "                !cp kaggle.json ~/.kaggle/\n",
    "                !chmod 600 ~/.kaggle/kaggle.json # Set correct permissions\n",
    "                print(\"kaggle.json copied and permissions set.\")\n",
    "            else:\n",
    "                print(f\"Ignoring uploaded file: {fn}. Please upload 'kaggle.json'.\")\n",
    "    else:\n",
    "        print(\"kaggle.json already exists.\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, assuming local Kaggle setup.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Kaggle setup: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indic_clip.core not found initially.\n",
      "Added /workspace/indic-clip to sys.path\n",
      "Imported indic_clip.core after path adjustment.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Reload core module in case Drive mount changed PROJECT_ROOT\n",
    "# This is fragile, better to define paths relative to notebook or pass explicitly\n",
    "# Or ensure core is imported *after* potential drive mount\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    importlib.reload(indic_clip.core)\n",
    "    print(\"Reloaded indic_clip.core\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"indic_clip.core not found initially.\")\n",
    "    # Attempt to set sys.path if running in Colab and project cloned\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        project_parent = '/content' # Assuming cloned into /content/indic-clip\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "             project_parent = '/content/drive/MyDrive/Indic-Clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive\")\n",
    "            # raise # Stop execution if core components missing\n",
    "    else:\n",
    "        project_parent = '/workspace'\n",
    "        if Path('/workspace/indic-clip').exists():\n",
    "             project_parent = '/workspace/indic-clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /workspace/indic-clip/indic-clip/core.py or similar in Drive\")\n",
    "            # raise # Stop execution if core components missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle library imported.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "# Try importing core components\n",
    "try:\n",
    "    from indic_clip.core import (PROJECT_ROOT, HINDI_RAW_PATH, SANSKRIT_RAW_PATH,\n",
    "                               SYNTHETIC_RAW_PATH, get_logger, setup_logging, ensure_dir)\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error importing from indic_clip.core: {e}\")\n",
    "    print(\"Please ensure the indic_clip library is installed (pip install -e .) or the path is correct.\")\n",
    "    # Define fallbacks if running interactively without full setup\n",
    "    if 'google.colab' in sys.modules:\n",
    "        PROJECT_ROOT=Path('/content/Indic-Clip')\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "           PROJECT_ROOT=Path('/content/drive/MyDrive/Indic-Clip')\n",
    "    else:\n",
    "        PROJECT_ROOT=Path('.').resolve()\n",
    "    print(f\"Using fallback PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "    DATA_PATH = PROJECT_ROOT / 'data'\n",
    "    RAW_DATA_PATH = DATA_PATH / 'raw'\n",
    "    HINDI_RAW_PATH = RAW_DATA_PATH / 'hindi'\n",
    "    SANSKRIT_RAW_PATH = RAW_DATA_PATH / 'sanskrit'\n",
    "    SYNTHETIC_RAW_PATH = RAW_DATA_PATH / 'synthetic'\n",
    "    # Define simple logging if setup fails\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def ensure_dir(path: Path): path.mkdir(parents=True, exist_ok=True)\n",
    "    def setup_logging(): pass # No-op\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "\n",
    "try:\n",
    "    import kaggle\n",
    "    print(\"Kaggle library imported.\")\n",
    "except OSError as e:\n",
    "    print(\"Kaggle API Error: Ensure kaggle.json is uploaded/configured correctly in Colab or locally.\")\n",
    "    # raise e # Don't raise here, let download attempt fail later\n",
    "except ImportError:\n",
    "     print(\"ERROR: Kaggle library not installed. Run !pip install kaggle\")\n",
    "\n",
    "# Setup logging for this module\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_formatted_data(data: list, output_path: Path, filename: str):\n",
    "    \"\"\"Saves a list of data (dicts) to a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        data: A list of dictionaries, where each dict represents an image-text pair\n",
    "              (e.g., {'image_filename': 'name.jpg', 'caption': 'text', 'source': 'datasource'}).\n",
    "        output_path: The directory Path object where the file should be saved.\n",
    "        filename: The name of the output file (e.g., 'flickr8k_hindi_raw.jsonl').\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        logger.warning(f\"No data provided to save for {filename}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    ensure_dir(output_path)\n",
    "    filepath = output_path / filename\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f: # Overwrite mode for consistency on rerun\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        logger.info(f\"Successfully wrote {len(data)} items to {filepath}\")\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Error saving data to {filepath}: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred while saving data to {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_kaggle_dataset(dataset_slug: str, download_path: Path):\n",
    "    \"\"\"Downloads a dataset from Kaggle using the official API.\n",
    "\n",
    "    Args:\n",
    "        dataset_slug: The Kaggle dataset slug (e.g., 'user/dataset-name').\n",
    "        download_path: The Path object representing the directory to download files into.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Attempting to download dataset '{dataset_slug}' to '{download_path}'...\")\n",
    "    ensure_dir(download_path)\n",
    "    try:\n",
    "        kaggle.api.authenticate() # Reads credentials from ~/.kaggle/kaggle.json or env vars\n",
    "        kaggle.api.dataset_download_files(dataset_slug, path=download_path, unzip=False, quiet=False)\n",
    "        logger.info(f\"Dataset '{dataset_slug}' downloaded successfully to '{download_path}'.\")\n",
    "        return True\n",
    "    except NameError:\n",
    "         logger.error(\"Kaggle library not imported correctly. Cannot download.\")\n",
    "         return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download dataset '{dataset_slug}': {e}\")\n",
    "        logger.error(\"Please ensure the Kaggle API is configured correctly (kaggle.json or env vars) and you accepted the dataset's terms on the Kaggle website if required.\")\n",
    "        # Consider raising the exception if download is critical\n",
    "        # raise e\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def unzip_file(zip_path: Path, extract_to: Path):\n",
    "    \"\"\"Unzips a file to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        zip_path: The Path object of the zip file.\n",
    "        extract_to: The Path object of the directory to extract files into.\n",
    "    \"\"\"\n",
    "    if not zip_path.exists():\n",
    "        logger.error(f\"Zip file not found at {zip_path}. Cannot unzip.\")\n",
    "        return False\n",
    "\n",
    "    logger.info(f\"Unzipping '{zip_path.name}' to '{extract_to}'...\")\n",
    "    ensure_dir(extract_to)\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for member in tqdm(zip_ref.infolist(), desc=f'Extracting {zip_path.name}'):\n",
    "                try:\n",
    "                    # Ensure extraction path is safe (within extract_to)\n",
    "                    target_path = os.path.join(extract_to, member.filename)\n",
    "                    if not os.path.abspath(target_path).startswith(os.path.abspath(extract_to)):\n",
    "                         logger.warning(f\"Skipping potentially unsafe path in zip: {member.filename}\")\n",
    "                         continue\n",
    "                    zip_ref.extract(member, extract_to)\n",
    "                except zipfile.error as e:\n",
    "                    logger.error(f\"Error extracting {member.filename} from {zip_path.name}: {e}\")\n",
    "                except Exception as e:\n",
    "                     logger.error(f\"Unexpected error extracting {member.filename}: {e}\")\n",
    "        logger.info(f\"Successfully unzipped '{zip_path.name}'.\")\n",
    "        # Optional: Remove the zip file after successful extraction\n",
    "        # os.remove(zip_path)\n",
    "        # logger.info(f\"Removed zip file: '{zip_path.name}'\")\n",
    "        return True\n",
    "    except zipfile.BadZipFile:\n",
    "        logger.error(f\"Error: '{zip_path.name}' is not a valid zip file or is corrupted.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during unzipping '{zip_path.name}': {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_hindi_captions(csv_path: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads Hindi captions from the specified CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path object to the captions CSV file.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the captions, or None if loading fails.\n",
    "    \"\"\"\n",
    "    if not csv_path.exists():\n",
    "        logger.error(f\"Caption file not found: {csv_path}\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Loading captions from {csv_path}...\")\n",
    "    try:\n",
    "        # The provided CSV seems to have a header based on sample\n",
    "        df = pd.read_csv(csv_path, header=0)\n",
    "\n",
    "        # Basic validation\n",
    "        required_columns = ['image', 'caption']\n",
    "        # Clean column names (strip whitespace etc.)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            logger.error(f\"CSV file {csv_path} missing required columns. Expected: {required_columns}, Found: {df.columns.tolist()}\")\n",
    "            return None\n",
    "\n",
    "        logger.info(f\"Successfully loaded {len(df)} captions from {csv_path}.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.error(f\"Error: Caption file {csv_path} is empty.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading captions from {csv_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanskrit Data Collection Interface (Placeholder)\n",
    "\n",
    "Define placeholder functions or an interface for acquiring Sanskrit image-text pairs (e.g., from digitized manuscripts). This acknowledges the difficulty and allows integration later. Comments highlight the manual/collaborative nature of this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_sanskrit_data_placeholder() -> list:\n",
    "    \"\"\"Placeholder function representing the Sanskrit data acquisition process.\n",
    "\n",
    "    In a real scenario, this function would interact with APIs, databases,\n",
    "    or parsed files from digitized manuscripts or other sources.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries (or an empty list), each containing\n",
    "        'image_filename', 'caption' (Sanskrit text), and 'source'.\n",
    "    \"\"\"\n",
    "    logger.warning(\"Using placeholder function for Sanskrit data. No actual Sanskrit data loaded.\")\n",
    "    # TODO: Replace this with actual logic to load Sanskrit data\n",
    "    # This might involve:\n",
    "    # - Reading pre-processed files created manually or via collaboration\n",
    "    # - Connecting to specific digital library APIs\n",
    "    # - Processing OCR results linked to manuscript images\n",
    "    sanskrit_data = [\n",
    "        # {\n",
    "        #     'image_filename': 'manuscript_page_1_illustration_1.jpg',\n",
    "        #     'caption': 'ॐ असतो मा सद्गमय । तमसो मा ज्योतिर्गमय । मृत्योर्मा अमृतं गमय ॥',\n",
    "        #     'source': 'example_manuscript_archive'\n",
    "        # },\n",
    "    ]\n",
    "    if sanskrit_data:\n",
    "       logger.info(f\"Loaded {len(sanskrit_data)} placeholder Sanskrit items.\")\n",
    "    return sanskrit_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Integration Point (Placeholder)\n",
    "\n",
    "Define a function or placeholder to integrate synthetic data from the IndicTTI project. Specify the expected input format (e.g., path to a file/directory containing image paths/data and corresponding Hindi/Sanskrit captions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_synthetic_data_placeholder(data_path: Path) -> list:\n",
    "    \"\"\"Placeholder function representing the synthetic data integration process.\n",
    "\n",
    "    In a real scenario, this would read data generated by the IndicTTI project,\n",
    "    assuming a specific format (e.g., a directory of images and a metadata file).\n",
    "\n",
    "    Args:\n",
    "        data_path: Path to the directory or file containing synthetic data.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries (or an empty list), each containing\n",
    "        'image_filename', 'caption' (could be Hindi or Sanskrit), and 'source'.\n",
    "    \"\"\"\n",
    "    logger.warning(\"Using placeholder function for Synthetic data. No actual data loaded.\")\n",
    "    # TODO: Replace with actual logic to load synthetic data from IndicTTI\n",
    "    # Example: Assume a metadata JSONL file exists at data_path\n",
    "    metadata_file = data_path / 'metadata.jsonl'\n",
    "    synthetic_data = []\n",
    "    if metadata_file.exists():\n",
    "        try:\n",
    "            with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line)\n",
    "                    # Assume item has 'image_filename' and 'caption' keys\n",
    "                    if 'image_filename' in item and 'caption' in item:\n",
    "                         item['source'] = 'indic_tti_synthetic'\n",
    "                         synthetic_data.append(item)\n",
    "                    else:\n",
    "                         logger.warning(f\"Skipping synthetic item due to missing keys: {item}\")\n",
    "            logger.info(f\"Loaded {len(synthetic_data)} items from synthetic source: {metadata_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading synthetic data from {metadata_file}: {e}\")\n",
    "    else:\n",
    "        logger.warning(f\"Synthetic data metadata file not found at {metadata_file}\")\n",
    "\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 10:46:09 - __main__ - INFO - --- Running Data Acquisition Script ---\n",
      "2025-04-22 10:46:09 - __main__ - INFO - Step 1: Downloading datasets from Kaggle...\n",
      "2025-04-22 10:46:09 - __main__ - INFO - Extracted images directory '/workspace/indic-clip/data/raw/hindi/Images' not found.\n",
      "2025-04-22 10:46:09 - __main__ - INFO - Image zip file '/workspace/indic-clip/data/raw/hindi/flickr8k.zip' not found. Will attempt download.\n",
      "2025-04-22 10:46:09 - __main__ - INFO - Extracted captions CSV '/workspace/indic-clip/data/raw/hindi/Clean-1Sentences_withComma.txt' not found.\n",
      "2025-04-22 10:46:09 - __main__ - INFO - Captions zip file '/workspace/indic-clip/data/raw/hindi/flickr8k-hindi-captions.zip' not found. Will attempt download.\n",
      "2025-04-22 10:46:09 - __main__ - INFO - Attempting to download dataset 'adityajn105/flickr8k' to '/workspace/indic-clip/data/raw/hindi'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/adityajn105/flickr8k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 10:46:23 - __main__ - INFO - Dataset 'adityajn105/flickr8k' downloaded successfully to '/workspace/indic-clip/data/raw/hindi'.\n",
      "2025-04-22 10:46:23 - __main__ - INFO - Attempting to download dataset 'dsmeena/flickr8k-hindi-captions' to '/workspace/indic-clip/data/raw/hindi'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/dsmeena/flickr8k-hindi-captions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 10:46:24 - __main__ - INFO - Dataset 'dsmeena/flickr8k-hindi-captions' downloaded successfully to '/workspace/indic-clip/data/raw/hindi'.\n",
      "2025-04-22 10:46:24 - __main__ - INFO - Step 2: Unzipping downloaded files (if necessary)...\n",
      "2025-04-22 10:46:24 - __main__ - INFO - Unzipping 'flickr8k.zip' to '/workspace/indic-clip/data/raw/hindi'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359c09e745dc4366becbb0aab64c3b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting flickr8k.zip:   0%|          | 0/8092 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 10:46:28 - __main__ - INFO - Successfully unzipped 'flickr8k.zip'.\n",
      "2025-04-22 10:46:28 - __main__ - INFO - Unzipping 'flickr8k-hindi-captions.zip' to '/workspace/indic-clip/data/raw/hindi'...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76fda4730e74c81b39f4de207c5fc19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting flickr8k-hindi-captions.zip:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 10:46:28 - __main__ - INFO - Successfully unzipped 'flickr8k-hindi-captions.zip'.\n",
      "2025-04-22 10:46:28 - __main__ - INFO - Step 3: Loading and formatting Hindi captions...\n",
      "2025-04-22 10:46:28 - __main__ - INFO - Loading captions from /workspace/indic-clip/data/raw/hindi/Clean-1Sentences_withComma.txt...\n",
      "2025-04-22 10:46:28 - __main__ - INFO - Successfully loaded 8090 captions from /workspace/indic-clip/data/raw/hindi/Clean-1Sentences_withComma.txt.\n",
      "2025-04-22 10:46:28 - __main__ - INFO - Formatting 8090 loaded captions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0766dcac1f4970944d23054f3d2dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting Hindi Captions:   0%|          | 0/8090 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 10:46:29 - __main__ - INFO - Step 4: Loading Sanskrit data (placeholder)...\n",
      "2025-04-22 10:46:29 - __main__ - WARNING - Using placeholder function for Sanskrit data. No actual Sanskrit data loaded.\n",
      "2025-04-22 10:46:29 - __main__ - INFO - Step 5: Loading synthetic data (placeholder)...\n",
      "2025-04-22 10:46:29 - __main__ - WARNING - Using placeholder function for Synthetic data. No actual data loaded.\n",
      "2025-04-22 10:46:29 - __main__ - WARNING - Synthetic data metadata file not found at /workspace/indic-clip/data/raw/synthetic/metadata.jsonl\n",
      "2025-04-22 10:46:29 - __main__ - INFO - Step 6: Saving formatted data...\n",
      "2025-04-22 10:46:29 - __main__ - INFO - Successfully wrote 8090 items to /workspace/indic-clip/data/raw/hindi/flickr8k_hindi_raw.jsonl\n",
      "2025-04-22 10:46:29 - __main__ - INFO - No Sanskrit data to save.\n",
      "2025-04-22 10:46:29 - __main__ - INFO - No synthetic data to save.\n",
      "2025-04-22 10:46:29 - __main__ - INFO - --- Data Acquisition Script Finished ---\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "# This block orchestrates the data acquisition process.\n",
    "# Ensure you have run the Colab Setup cells above if applicable.\n",
    "\n",
    "# Ensure core components are loaded after potential Colab setup / Drive mount\n",
    "# It might be safer to put this entire block in a function called from outside\n",
    "# or explicitly re-import core here if running interactively.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger.info(\"--- Running Data Acquisition Script ---\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    # Kaggle dataset slugs\n",
    "    FLICKR8K_IMAGES_SLUG = \"adityajn105/flickr8k\"\n",
    "    FLICKR8K_HINDI_CAPTIONS_SLUG = \"dsmeena/flickr8k-hindi-captions\"\n",
    "\n",
    "    # Define paths using variables from indic_clip.core\n",
    "    # Assumes PROJECT_ROOT is correctly set for Colab or local\n",
    "    IMAGES_DOWNLOAD_PATH = HINDI_RAW_PATH\n",
    "    CAPTIONS_DOWNLOAD_PATH = HINDI_RAW_PATH\n",
    "    IMAGES_EXTRACT_PATH = HINDI_RAW_PATH\n",
    "    CAPTIONS_EXTRACT_PATH = HINDI_RAW_PATH\n",
    "\n",
    "    # Expected filenames after download/extraction (adjust if needed based on Kaggle dataset structure)\n",
    "    IMAGES_ZIP_FILENAME = 'flickr8k.zip' # Default name from kaggle API might vary\n",
    "    CAPTIONS_ZIP_FILENAME = 'flickr8k-hindi-captions.zip' # Default name\n",
    "    # <<< Check the actual filename in the captions zip file >>>\n",
    "    CAPTIONS_CSV_FILENAME = 'Clean-1Sentences_withComma.txt' # This needs verification after download!\n",
    "    # Common variations: captions.csv, Hindi_Captions.csv, etc.\n",
    "    # It's crucial this matches the actual extracted file name.\n",
    "\n",
    "    # --- Download Datasets ---\n",
    "    logger.info(\"Step 1: Downloading datasets from Kaggle...\")\n",
    "    # Define full paths to zip files\n",
    "    images_zip_path = IMAGES_DOWNLOAD_PATH / IMAGES_ZIP_FILENAME\n",
    "    captions_zip_path = CAPTIONS_DOWNLOAD_PATH / CAPTIONS_ZIP_FILENAME\n",
    "\n",
    "    # Define expected output locations after extraction\n",
    "    extracted_images_dir = IMAGES_EXTRACT_PATH / 'Images' # Flickr8k images are in 'Images' subdir\n",
    "    extracted_captions_csv = CAPTIONS_EXTRACT_PATH / CAPTIONS_CSV_FILENAME\n",
    "\n",
    "    download_images_flag = False\n",
    "    unzip_images_flag = False\n",
    "    download_captions_flag = False\n",
    "    unzip_captions_flag = False\n",
    "\n",
    "    # Decide whether to download/unzip images\n",
    "    if not extracted_images_dir.exists():\n",
    "        logger.info(f\"Extracted images directory '{extracted_images_dir}' not found.\")\n",
    "        if not images_zip_path.exists():\n",
    "            logger.info(f\"Image zip file '{images_zip_path}' not found. Will attempt download.\")\n",
    "            download_images_flag = True\n",
    "        else:\n",
    "            logger.info(f\"Image zip file already exists at {images_zip_path}. Skipping download.\")\n",
    "        unzip_images_flag = True # Need to unzip if extracted dir doesn't exist\n",
    "    else:\n",
    "        logger.info(f\"Image directory '{extracted_images_dir}' already exists. Skipping image download and unzip.\")\n",
    "\n",
    "    # Decide whether to download/unzip captions\n",
    "    if not extracted_captions_csv.exists():\n",
    "        logger.info(f\"Extracted captions CSV '{extracted_captions_csv}' not found.\")\n",
    "        if not captions_zip_path.exists():\n",
    "             logger.info(f\"Captions zip file '{captions_zip_path}' not found. Will attempt download.\")\n",
    "             download_captions_flag = True\n",
    "        else:\n",
    "            logger.info(f\"Captions zip file already exists at {captions_zip_path}. Skipping download.\")\n",
    "        unzip_captions_flag = True # Need to unzip if extracted csv doesn't exist\n",
    "    else:\n",
    "        logger.info(f\"Captions CSV file '{extracted_captions_csv}' already exists. Skipping captions download and unzip.\")\n",
    "\n",
    "    # Perform downloads\n",
    "    if download_images_flag:\n",
    "        if not download_kaggle_dataset(FLICKR8K_IMAGES_SLUG, IMAGES_DOWNLOAD_PATH):\n",
    "             unzip_images_flag = False # Don't attempt unzip if download failed\n",
    "    if download_captions_flag:\n",
    "        if not download_kaggle_dataset(FLICKR8K_HINDI_CAPTIONS_SLUG, CAPTIONS_DOWNLOAD_PATH):\n",
    "             unzip_captions_flag = False # Don't attempt unzip if download failed\n",
    "\n",
    "    # --- Unzip Files ---\n",
    "    logger.info(\"Step 2: Unzipping downloaded files (if necessary)...\")\n",
    "    if unzip_images_flag and images_zip_path.exists():\n",
    "        unzip_file(images_zip_path, IMAGES_EXTRACT_PATH)\n",
    "\n",
    "    if unzip_captions_flag and captions_zip_path.exists():\n",
    "        unzip_file(captions_zip_path, CAPTIONS_EXTRACT_PATH)\n",
    "        # IMPORTANT: Verify CAPTIONS_CSV_FILENAME matches the extracted file now!\n",
    "        if not extracted_captions_csv.exists():\n",
    "             logger.error(f\"Caption file '{CAPTIONS_CSV_FILENAME}' not found in {CAPTIONS_EXTRACT_PATH} after unzipping. Check the zip contents and update CAPTIONS_CSV_FILENAME.\")\n",
    "\n",
    "    # --- Load and Format Hindi Captions ---\n",
    "    logger.info(\"Step 3: Loading and formatting Hindi captions...\")\n",
    "    hindi_captions_df = None\n",
    "    if extracted_captions_csv.exists():\n",
    "        hindi_captions_df = load_hindi_captions(extracted_captions_csv)\n",
    "    else:\n",
    "        logger.error(f\"Cannot load captions, file not found: {extracted_captions_csv}\")\n",
    "\n",
    "    formatted_hindi_data = []\n",
    "    if hindi_captions_df is not None:\n",
    "        logger.info(f\"Formatting {len(hindi_captions_df)} loaded captions...\")\n",
    "        # Construct relative path for images within the raw directory structure\n",
    "        # Assumes images are extracted to HINDI_RAW_PATH / 'Images'\n",
    "        image_subfolder = 'Images'\n",
    "\n",
    "        for index, row in tqdm(hindi_captions_df.iterrows(), total=len(hindi_captions_df), desc=\"Formatting Hindi Captions\"):\n",
    "            image_id_base = row['image'] # Base ID like '1000268201_693b08cb0e'\n",
    "            caption = row['caption']\n",
    "\n",
    "            # Construct filename (assuming .jpg extension, common for Flickr8k)\n",
    "            image_filename_only = f\"{image_id_base}.jpg\"\n",
    "            # Store relative path within the source directory for later use\n",
    "            image_relative_path = f\"{image_subfolder}/{image_filename_only}\"\n",
    "\n",
    "            # Basic check: ensure image file actually exists after extraction\n",
    "            image_full_path = IMAGES_EXTRACT_PATH / image_subfolder / image_filename_only\n",
    "            if not image_full_path.exists():\n",
    "                logger.warning(f\"Image file not found: {image_full_path}. Skipping caption for {image_id_base}.\")\n",
    "                continue\n",
    "\n",
    "            if isinstance(caption, str) and caption.strip(): # Basic validation\n",
    "                formatted_hindi_data.append({\n",
    "                    # Store relative path from the source's root (HINDI_RAW_PATH)\n",
    "                    'image_path_relative': image_relative_path,\n",
    "                    'caption': caption.strip(),\n",
    "                    'source': 'flickr8k_hindi'\n",
    "                })\n",
    "            else:\n",
    "                 logger.warning(f\"Skipping row {index} for image {image_id_base} due to invalid caption: {caption}\")\n",
    "    else:\n",
    "        logger.error(\"Failed to load Hindi captions DataFrame. Cannot format data.\")\n",
    "\n",
    "    # --- Load Sanskrit Data (Placeholder) ---\n",
    "    logger.info(\"Step 4: Loading Sanskrit data (placeholder)...\")\n",
    "    formatted_sanskrit_data = get_sanskrit_data_placeholder()\n",
    "    # Adjust 'image_path_relative' if real data is used\n",
    "    # for item in formatted_sanskrit_data:\n",
    "    #     item['image_path_relative'] = f\"sanskrit_images/{item['image_filename']}\" # Example\n",
    "\n",
    "    # --- Load Synthetic Data (Placeholder) ---\n",
    "    logger.info(\"Step 5: Loading synthetic data (placeholder)...\")\n",
    "    formatted_synthetic_data = get_synthetic_data_placeholder(SYNTHETIC_RAW_PATH)\n",
    "    # Adjust 'image_path_relative' if real data is used\n",
    "    # for item in formatted_synthetic_data:\n",
    "    #     item['image_path_relative'] = f\"synthetic_images/{item['image_filename']}\" # Example\n",
    "\n",
    "    # --- Combine and Save Data ---\n",
    "    logger.info(\"Step 6: Saving formatted data...\")\n",
    "    # Save Hindi data\n",
    "    save_formatted_data(formatted_hindi_data, HINDI_RAW_PATH, 'flickr8k_hindi_raw.jsonl')\n",
    "\n",
    "    # Save Sanskrit data (if any)\n",
    "    if formatted_sanskrit_data:\n",
    "        save_formatted_data(formatted_sanskrit_data, SANSKRIT_RAW_PATH, 'sanskrit_raw.jsonl')\n",
    "    else:\n",
    "        logger.info(\"No Sanskrit data to save.\")\n",
    "\n",
    "    # Save Synthetic data (if any)\n",
    "    if formatted_synthetic_data:\n",
    "        save_formatted_data(formatted_synthetic_data, SYNTHETIC_RAW_PATH, 'synthetic_raw.jsonl')\n",
    "    else:\n",
    "        logger.info(\"No synthetic data to save.\")\n",
    "\n",
    "    logger.info(\"--- Data Acquisition Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # Run this in terminal to export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
