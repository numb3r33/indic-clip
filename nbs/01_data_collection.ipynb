{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "> Functions and tools for acquiring image-text pair data, primarily using existing datasets from Kaggle. Includes Colab setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup (Run these cells if using Google Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "# Note: nbdev install might be needed if running nbdev commands\n",
    "!pip install -q kaggle nbdev fastai==2.7.19 transformers timm imagehash tqdm zipfile36 wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive mounted successfully.\n"
     ]
    }
   ],
   "source": [
    "# Mount Google Drive (Optional, but recommended for persistent storage)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, skipping Drive mount.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please upload your kaggle.json file:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-6a35a0d7-2636-471a-a787-a0400cc568b5\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-6a35a0d7-2636-471a-a787-a0400cc568b5\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving kaggle.json to kaggle.json\n",
      "User uploaded file \"kaggle.json\" with length 67 bytes\n",
      "kaggle.json copied and permissions set.\n"
     ]
    }
   ],
   "source": [
    "# Kaggle API Setup: Upload your kaggle.json file\n",
    "try:\n",
    "    from google.colab import files\n",
    "    import os\n",
    "\n",
    "    # Create .kaggle directory if it doesn't exist\n",
    "    kaggle_dir = os.path.expanduser('~/.kaggle')\n",
    "    if not os.path.exists(kaggle_dir):\n",
    "        os.makedirs(kaggle_dir)\n",
    "        print(f\"Created directory: {kaggle_dir}\")\n",
    "\n",
    "    # Check if kaggle.json already exists\n",
    "    kaggle_json_path = os.path.join(kaggle_dir, 'kaggle.json')\n",
    "    if not os.path.exists(kaggle_json_path):\n",
    "        print(\"Please upload your kaggle.json file:\")\n",
    "        uploaded = files.upload() # This prompts the user to upload\n",
    "\n",
    "        for fn in uploaded.keys():\n",
    "            if fn == 'kaggle.json':\n",
    "                print(f'User uploaded file \"{fn}\" with length {len(uploaded[fn])} bytes')\n",
    "                # Move the uploaded file to the correct location\n",
    "                !mkdir -p ~/.kaggle/\n",
    "                !cp kaggle.json ~/.kaggle/\n",
    "                !chmod 600 ~/.kaggle/kaggle.json # Set correct permissions\n",
    "                print(\"kaggle.json copied and permissions set.\")\n",
    "            else:\n",
    "                print(f\"Ignoring uploaded file: {fn}. Please upload 'kaggle.json'.\")\n",
    "    else:\n",
    "        print(\"kaggle.json already exists.\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, assuming local Kaggle setup.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Kaggle setup: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indic_clip.core not found initially.\n",
      "Added /content/drive/MyDrive/Indic-Clip to sys.path\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive detected, setting PROJECT_ROOT to /content/drive/MyDrive/Indic-Clip\n",
      "Ensure your project files are located there.\n",
      "Imported indic_clip.core after path adjustment.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Reload core module in case Drive mount changed PROJECT_ROOT\n",
    "# This is fragile, better to define paths relative to notebook or pass explicitly\n",
    "# Or ensure core is imported *after* potential drive mount\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    importlib.reload(indic_clip.core)\n",
    "    print(\"Reloaded indic_clip.core\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"indic_clip.core not found initially.\")\n",
    "    # Attempt to set sys.path if running in Colab and project cloned\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        project_parent = '/content' # Assuming cloned into /content/indic-clip\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "             project_parent = '/content/drive/MyDrive/Indic-Clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive\")\n",
    "            # raise # Stop execution if core components missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle library imported.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "# Try importing core components\n",
    "try:\n",
    "    from indic_clip.core import (PROJECT_ROOT, HINDI_RAW_PATH, SANSKRIT_RAW_PATH,\n",
    "                               SYNTHETIC_RAW_PATH, get_logger, setup_logging, ensure_dir)\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error importing from indic_clip.core: {e}\")\n",
    "    print(\"Please ensure the indic_clip library is installed (pip install -e .) or the path is correct.\")\n",
    "    # Define fallbacks if running interactively without full setup\n",
    "    if 'google.colab' in sys.modules:\n",
    "        PROJECT_ROOT=Path('/content/Indic-Clip')\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "           PROJECT_ROOT=Path('/content/drive/MyDrive/Indic-Clip')\n",
    "    else:\n",
    "        PROJECT_ROOT=Path('.').resolve()\n",
    "    print(f\"Using fallback PROJECT_ROOT: {PROJECT_ROOT}\")\n",
    "    DATA_PATH = PROJECT_ROOT / 'data'\n",
    "    RAW_DATA_PATH = DATA_PATH / 'raw'\n",
    "    HINDI_RAW_PATH = RAW_DATA_PATH / 'hindi'\n",
    "    SANSKRIT_RAW_PATH = RAW_DATA_PATH / 'sanskrit'\n",
    "    SYNTHETIC_RAW_PATH = RAW_DATA_PATH / 'synthetic'\n",
    "    # Define simple logging if setup fails\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def ensure_dir(path: Path): path.mkdir(parents=True, exist_ok=True)\n",
    "    def setup_logging(): pass # No-op\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "\n",
    "try:\n",
    "    import kaggle\n",
    "    print(\"Kaggle library imported.\")\n",
    "except OSError as e:\n",
    "    print(\"Kaggle API Error: Ensure kaggle.json is uploaded/configured correctly in Colab or locally.\")\n",
    "    # raise e # Don't raise here, let download attempt fail later\n",
    "except ImportError:\n",
    "     print(\"ERROR: Kaggle library not installed. Run !pip install kaggle\")\n",
    "\n",
    "# Setup logging for this module\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def save_formatted_data(data: list, output_path: Path, filename: str):\n",
    "    \"\"\"Saves a list of data (dicts) to a JSONL file.\n",
    "\n",
    "    Args:\n",
    "        data: A list of dictionaries, where each dict represents an image-text pair\n",
    "              (e.g., {'image_filename': 'name.jpg', 'caption': 'text', 'source': 'datasource'}).\n",
    "        output_path: The directory Path object where the file should be saved.\n",
    "        filename: The name of the output file (e.g., 'flickr8k_hindi_raw.jsonl').\n",
    "    \"\"\"\n",
    "    if not data:\n",
    "        logger.warning(f\"No data provided to save for {filename}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    ensure_dir(output_path)\n",
    "    filepath = output_path / filename\n",
    "\n",
    "    try:\n",
    "        with open(filepath, 'w', encoding='utf-8') as f: # Overwrite mode for consistency on rerun\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        logger.info(f\"Successfully wrote {len(data)} items to {filepath}\")\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Error saving data to {filepath}: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred while saving data to {filepath}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def download_kaggle_dataset(dataset_slug: str, download_path: Path):\n",
    "    \"\"\"Downloads a dataset from Kaggle using the official API.\n",
    "\n",
    "    Args:\n",
    "        dataset_slug: The Kaggle dataset slug (e.g., 'user/dataset-name').\n",
    "        download_path: The Path object representing the directory to download files into.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Attempting to download dataset '{dataset_slug}' to '{download_path}'...\")\n",
    "    ensure_dir(download_path)\n",
    "    try:\n",
    "        kaggle.api.authenticate() # Reads credentials from ~/.kaggle/kaggle.json or env vars\n",
    "        kaggle.api.dataset_download_files(dataset_slug, path=download_path, unzip=False, quiet=False)\n",
    "        logger.info(f\"Dataset '{dataset_slug}' downloaded successfully to '{download_path}'.\")\n",
    "        return True\n",
    "    except NameError:\n",
    "         logger.error(\"Kaggle library not imported correctly. Cannot download.\")\n",
    "         return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to download dataset '{dataset_slug}': {e}\")\n",
    "        logger.error(\"Please ensure the Kaggle API is configured correctly (kaggle.json or env vars) and you accepted the dataset's terms on the Kaggle website if required.\")\n",
    "        # Consider raising the exception if download is critical\n",
    "        # raise e\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def unzip_file(zip_path: Path, extract_to: Path):\n",
    "    \"\"\"Unzips a file to a specified directory.\n",
    "\n",
    "    Args:\n",
    "        zip_path: The Path object of the zip file.\n",
    "        extract_to: The Path object of the directory to extract files into.\n",
    "    \"\"\"\n",
    "    if not zip_path.exists():\n",
    "        logger.error(f\"Zip file not found at {zip_path}. Cannot unzip.\")\n",
    "        return False\n",
    "\n",
    "    logger.info(f\"Unzipping '{zip_path.name}' to '{extract_to}'...\")\n",
    "    ensure_dir(extract_to)\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for member in tqdm(zip_ref.infolist(), desc=f'Extracting {zip_path.name}'):\n",
    "                try:\n",
    "                    # Ensure extraction path is safe (within extract_to)\n",
    "                    target_path = os.path.join(extract_to, member.filename)\n",
    "                    if not os.path.abspath(target_path).startswith(os.path.abspath(extract_to)):\n",
    "                         logger.warning(f\"Skipping potentially unsafe path in zip: {member.filename}\")\n",
    "                         continue\n",
    "                    zip_ref.extract(member, extract_to)\n",
    "                except zipfile.error as e:\n",
    "                    logger.error(f\"Error extracting {member.filename} from {zip_path.name}: {e}\")\n",
    "                except Exception as e:\n",
    "                     logger.error(f\"Unexpected error extracting {member.filename}: {e}\")\n",
    "        logger.info(f\"Successfully unzipped '{zip_path.name}'.\")\n",
    "        # Optional: Remove the zip file after successful extraction\n",
    "        # os.remove(zip_path)\n",
    "        # logger.info(f\"Removed zip file: '{zip_path.name}'\")\n",
    "        return True\n",
    "    except zipfile.BadZipFile:\n",
    "        logger.error(f\"Error: '{zip_path.name}' is not a valid zip file or is corrupted.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred during unzipping '{zip_path.name}': {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_hindi_captions(csv_path: Path) -> pd.DataFrame | None:\n",
    "    \"\"\"Loads Hindi captions from the specified CSV file.\n",
    "\n",
    "    Args:\n",
    "        csv_path: Path object to the captions CSV file.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame containing the captions, or None if loading fails.\n",
    "    \"\"\"\n",
    "    if not csv_path.exists():\n",
    "        logger.error(f\"Caption file not found: {csv_path}\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Loading captions from {csv_path}...\")\n",
    "    try:\n",
    "        # The provided CSV seems to have a header based on sample\n",
    "        df = pd.read_csv(csv_path, header=0)\n",
    "\n",
    "        # Basic validation\n",
    "        required_columns = ['image', 'caption']\n",
    "        # Clean column names (strip whitespace etc.)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        if not all(col in df.columns for col in required_columns):\n",
    "            logger.error(f\"CSV file {csv_path} missing required columns. Expected: {required_columns}, Found: {df.columns.tolist()}\")\n",
    "            return None\n",
    "\n",
    "        logger.info(f\"Successfully loaded {len(df)} captions from {csv_path}.\")\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError:\n",
    "        logger.error(f\"Error: Caption file {csv_path} is empty.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading captions from {csv_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanskrit Data Collection Interface (Placeholder)\n",
    "\n",
    "Define placeholder functions or an interface for acquiring Sanskrit image-text pairs (e.g., from digitized manuscripts). This acknowledges the difficulty and allows integration later. Comments highlight the manual/collaborative nature of this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_sanskrit_data_placeholder() -> list:\n",
    "    \"\"\"Placeholder function representing the Sanskrit data acquisition process.\n",
    "\n",
    "    In a real scenario, this function would interact with APIs, databases,\n",
    "    or parsed files from digitized manuscripts or other sources.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries (or an empty list), each containing\n",
    "        'image_filename', 'caption' (Sanskrit text), and 'source'.\n",
    "    \"\"\"\n",
    "    logger.warning(\"Using placeholder function for Sanskrit data. No actual Sanskrit data loaded.\")\n",
    "    # TODO: Replace this with actual logic to load Sanskrit data\n",
    "    # This might involve:\n",
    "    # - Reading pre-processed files created manually or via collaboration\n",
    "    # - Connecting to specific digital library APIs\n",
    "    # - Processing OCR results linked to manuscript images\n",
    "    sanskrit_data = [\n",
    "        # {\n",
    "        #     'image_filename': 'manuscript_page_1_illustration_1.jpg',\n",
    "        #     'caption': 'ॐ असतो मा सद्गमय । तमसो मा ज्योतिर्गमय । मृत्योर्मा अमृतं गमय ॥',\n",
    "        #     'source': 'example_manuscript_archive'\n",
    "        # },\n",
    "    ]\n",
    "    if sanskrit_data:\n",
    "       logger.info(f\"Loaded {len(sanskrit_data)} placeholder Sanskrit items.\")\n",
    "    return sanskrit_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Integration Point (Placeholder)\n",
    "\n",
    "Define a function or placeholder to integrate synthetic data from the IndicTTI project. Specify the expected input format (e.g., path to a file/directory containing image paths/data and corresponding Hindi/Sanskrit captions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_synthetic_data_placeholder(data_path: Path) -> list:\n",
    "    \"\"\"Placeholder function representing the synthetic data integration process.\n",
    "\n",
    "    In a real scenario, this would read data generated by the IndicTTI project,\n",
    "    assuming a specific format (e.g., a directory of images and a metadata file).\n",
    "\n",
    "    Args:\n",
    "        data_path: Path to the directory or file containing synthetic data.\n",
    "\n",
    "    Returns:\n",
    "        A list of dictionaries (or an empty list), each containing\n",
    "        'image_filename', 'caption' (could be Hindi or Sanskrit), and 'source'.\n",
    "    \"\"\"\n",
    "    logger.warning(\"Using placeholder function for Synthetic data. No actual data loaded.\")\n",
    "    # TODO: Replace with actual logic to load synthetic data from IndicTTI\n",
    "    # Example: Assume a metadata JSONL file exists at data_path\n",
    "    metadata_file = data_path / 'metadata.jsonl'\n",
    "    synthetic_data = []\n",
    "    if metadata_file.exists():\n",
    "        try:\n",
    "            with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    item = json.loads(line)\n",
    "                    # Assume item has 'image_filename' and 'caption' keys\n",
    "                    if 'image_filename' in item and 'caption' in item:\n",
    "                         item['source'] = 'indic_tti_synthetic'\n",
    "                         synthetic_data.append(item)\n",
    "                    else:\n",
    "                         logger.warning(f\"Skipping synthetic item due to missing keys: {item}\")\n",
    "            logger.info(f\"Loaded {len(synthetic_data)} items from synthetic source: {metadata_file}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading synthetic data from {metadata_file}: {e}\")\n",
    "    else:\n",
    "        logger.warning(f\"Synthetic data metadata file not found at {metadata_file}\")\n",
    "\n",
    "    return synthetic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution: Data Acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/content/drive/MyDrive/Indic-Clip/data/raw/hindi')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HINDI_RAW_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "# This block orchestrates the data acquisition process.\n",
    "# Ensure you have run the Colab Setup cells above if applicable.\n",
    "\n",
    "# Ensure core components are loaded after potential Colab setup / Drive mount\n",
    "# It might be safer to put this entire block in a function called from outside\n",
    "# or explicitly re-import core here if running interactively.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger.info(\"--- Running Data Acquisition Script ---\")\n",
    "\n",
    "    # --- Configuration ---\n",
    "    # Kaggle dataset slugs\n",
    "    FLICKR8K_IMAGES_SLUG = \"adityajn105/flickr8k\"\n",
    "    FLICKR8K_HINDI_CAPTIONS_SLUG = \"dsmeena/flickr8k-hindi-captions\"\n",
    "\n",
    "    # Define paths using variables from indic_clip.core\n",
    "    # Assumes PROJECT_ROOT is correctly set for Colab or local\n",
    "    IMAGES_DOWNLOAD_PATH = HINDI_RAW_PATH\n",
    "    CAPTIONS_DOWNLOAD_PATH = HINDI_RAW_PATH\n",
    "    IMAGES_EXTRACT_PATH = HINDI_RAW_PATH\n",
    "    CAPTIONS_EXTRACT_PATH = HINDI_RAW_PATH\n",
    "\n",
    "    # Expected filenames after download/extraction (adjust if needed based on Kaggle dataset structure)\n",
    "    IMAGES_ZIP_FILENAME = 'flickr8k.zip' # Default name from kaggle API might vary\n",
    "    CAPTIONS_ZIP_FILENAME = 'flickr8k-hindi-captions.zip' # Default name\n",
    "    # <<< Check the actual filename in the captions zip file >>>\n",
    "    CAPTIONS_CSV_FILENAME = 'Clean-1Sentences_withComma.txt' # This needs verification after download!\n",
    "    # Common variations: captions.csv, Hindi_Captions.csv, etc.\n",
    "    # It's crucial this matches the actual extracted file name.\n",
    "\n",
    "    # --- Download Datasets ---\n",
    "    logger.info(\"Step 1: Downloading datasets from Kaggle...\")\n",
    "    # Define full paths to zip files\n",
    "    images_zip_path = IMAGES_DOWNLOAD_PATH / IMAGES_ZIP_FILENAME\n",
    "    captions_zip_path = CAPTIONS_DOWNLOAD_PATH / CAPTIONS_ZIP_FILENAME\n",
    "\n",
    "    # Define expected output locations after extraction\n",
    "    extracted_images_dir = IMAGES_EXTRACT_PATH / 'Images' # Flickr8k images are in 'Images' subdir\n",
    "    extracted_captions_csv = CAPTIONS_EXTRACT_PATH / CAPTIONS_CSV_FILENAME\n",
    "\n",
    "    download_images_flag = False\n",
    "    unzip_images_flag = False\n",
    "    download_captions_flag = False\n",
    "    unzip_captions_flag = False\n",
    "\n",
    "    # Decide whether to download/unzip images\n",
    "    if not extracted_images_dir.exists():\n",
    "        logger.info(f\"Extracted images directory '{extracted_images_dir}' not found.\")\n",
    "        if not images_zip_path.exists():\n",
    "            logger.info(f\"Image zip file '{images_zip_path}' not found. Will attempt download.\")\n",
    "            download_images_flag = True\n",
    "        else:\n",
    "            logger.info(f\"Image zip file already exists at {images_zip_path}. Skipping download.\")\n",
    "        unzip_images_flag = True # Need to unzip if extracted dir doesn't exist\n",
    "    else:\n",
    "        logger.info(f\"Image directory '{extracted_images_dir}' already exists. Skipping image download and unzip.\")\n",
    "\n",
    "    # Decide whether to download/unzip captions\n",
    "    if not extracted_captions_csv.exists():\n",
    "        logger.info(f\"Extracted captions CSV '{extracted_captions_csv}' not found.\")\n",
    "        if not captions_zip_path.exists():\n",
    "             logger.info(f\"Captions zip file '{captions_zip_path}' not found. Will attempt download.\")\n",
    "             download_captions_flag = True\n",
    "        else:\n",
    "            logger.info(f\"Captions zip file already exists at {captions_zip_path}. Skipping download.\")\n",
    "        unzip_captions_flag = True # Need to unzip if extracted csv doesn't exist\n",
    "    else:\n",
    "        logger.info(f\"Captions CSV file '{extracted_captions_csv}' already exists. Skipping captions download and unzip.\")\n",
    "\n",
    "    # Perform downloads\n",
    "    if download_images_flag:\n",
    "        if not download_kaggle_dataset(FLICKR8K_IMAGES_SLUG, IMAGES_DOWNLOAD_PATH):\n",
    "             unzip_images_flag = False # Don't attempt unzip if download failed\n",
    "    if download_captions_flag:\n",
    "        if not download_kaggle_dataset(FLICKR8K_HINDI_CAPTIONS_SLUG, CAPTIONS_DOWNLOAD_PATH):\n",
    "             unzip_captions_flag = False # Don't attempt unzip if download failed\n",
    "\n",
    "    # --- Unzip Files ---\n",
    "    logger.info(\"Step 2: Unzipping downloaded files (if necessary)...\")\n",
    "    if unzip_images_flag and images_zip_path.exists():\n",
    "        unzip_file(images_zip_path, IMAGES_EXTRACT_PATH)\n",
    "\n",
    "    if unzip_captions_flag and captions_zip_path.exists():\n",
    "        unzip_file(captions_zip_path, CAPTIONS_EXTRACT_PATH)\n",
    "        # IMPORTANT: Verify CAPTIONS_CSV_FILENAME matches the extracted file now!\n",
    "        if not extracted_captions_csv.exists():\n",
    "             logger.error(f\"Caption file '{CAPTIONS_CSV_FILENAME}' not found in {CAPTIONS_EXTRACT_PATH} after unzipping. Check the zip contents and update CAPTIONS_CSV_FILENAME.\")\n",
    "\n",
    "    # --- Load and Format Hindi Captions ---\n",
    "    logger.info(\"Step 3: Loading and formatting Hindi captions...\")\n",
    "    hindi_captions_df = None\n",
    "    if extracted_captions_csv.exists():\n",
    "        hindi_captions_df = load_hindi_captions(extracted_captions_csv)\n",
    "    else:\n",
    "        logger.error(f\"Cannot load captions, file not found: {extracted_captions_csv}\")\n",
    "\n",
    "    formatted_hindi_data = []\n",
    "    if hindi_captions_df is not None:\n",
    "        logger.info(f\"Formatting {len(hindi_captions_df)} loaded captions...\")\n",
    "        # Construct relative path for images within the raw directory structure\n",
    "        # Assumes images are extracted to HINDI_RAW_PATH / 'Images'\n",
    "        image_subfolder = 'Images'\n",
    "\n",
    "        for index, row in tqdm(hindi_captions_df.iterrows(), total=len(hindi_captions_df), desc=\"Formatting Hindi Captions\"):\n",
    "            image_id_base = row['image'] # Base ID like '1000268201_693b08cb0e'\n",
    "            caption = row['caption']\n",
    "\n",
    "            # Construct filename (assuming .jpg extension, common for Flickr8k)\n",
    "            image_filename_only = f\"{image_id_base}.jpg\"\n",
    "            # Store relative path within the source directory for later use\n",
    "            image_relative_path = f\"{image_subfolder}/{image_filename_only}\"\n",
    "\n",
    "            # Basic check: ensure image file actually exists after extraction\n",
    "            image_full_path = IMAGES_EXTRACT_PATH / image_subfolder / image_filename_only\n",
    "            if not image_full_path.exists():\n",
    "                logger.warning(f\"Image file not found: {image_full_path}. Skipping caption for {image_id_base}.\")\n",
    "                continue\n",
    "\n",
    "            if isinstance(caption, str) and caption.strip(): # Basic validation\n",
    "                formatted_hindi_data.append({\n",
    "                    # Store relative path from the source's root (HINDI_RAW_PATH)\n",
    "                    'image_path_relative': image_relative_path,\n",
    "                    'caption': caption.strip(),\n",
    "                    'source': 'flickr8k_hindi'\n",
    "                })\n",
    "            else:\n",
    "                 logger.warning(f\"Skipping row {index} for image {image_id_base} due to invalid caption: {caption}\")\n",
    "    else:\n",
    "        logger.error(\"Failed to load Hindi captions DataFrame. Cannot format data.\")\n",
    "\n",
    "    # --- Load Sanskrit Data (Placeholder) ---\n",
    "    logger.info(\"Step 4: Loading Sanskrit data (placeholder)...\")\n",
    "    formatted_sanskrit_data = get_sanskrit_data_placeholder()\n",
    "    # Adjust 'image_path_relative' if real data is used\n",
    "    # for item in formatted_sanskrit_data:\n",
    "    #     item['image_path_relative'] = f\"sanskrit_images/{item['image_filename']}\" # Example\n",
    "\n",
    "    # --- Load Synthetic Data (Placeholder) ---\n",
    "    logger.info(\"Step 5: Loading synthetic data (placeholder)...\")\n",
    "    formatted_synthetic_data = get_synthetic_data_placeholder(SYNTHETIC_RAW_PATH)\n",
    "    # Adjust 'image_path_relative' if real data is used\n",
    "    # for item in formatted_synthetic_data:\n",
    "    #     item['image_path_relative'] = f\"synthetic_images/{item['image_filename']}\" # Example\n",
    "\n",
    "    # --- Combine and Save Data ---\n",
    "    logger.info(\"Step 6: Saving formatted data...\")\n",
    "    # Save Hindi data\n",
    "    save_formatted_data(formatted_hindi_data, HINDI_RAW_PATH, 'flickr8k_hindi_raw.jsonl')\n",
    "\n",
    "    # Save Sanskrit data (if any)\n",
    "    if formatted_sanskrit_data:\n",
    "        save_formatted_data(formatted_sanskrit_data, SANSKRIT_RAW_PATH, 'sanskrit_raw.jsonl')\n",
    "    else:\n",
    "        logger.info(\"No Sanskrit data to save.\")\n",
    "\n",
    "    # Save Synthetic data (if any)\n",
    "    if formatted_synthetic_data:\n",
    "        save_formatted_data(formatted_synthetic_data, SYNTHETIC_RAW_PATH, 'synthetic_raw.jsonl')\n",
    "    else:\n",
    "        logger.info(\"No synthetic data to save.\")\n",
    "\n",
    "    logger.info(\"--- Data Acquisition Script Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Indic-Clip\n"
     ]
    }
   ],
   "source": [
    "MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # Run this in terminal to export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
