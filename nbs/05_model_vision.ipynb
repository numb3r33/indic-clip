{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Encoder\n",
    "\n",
    "> Wrappers for vision models (e.g., ResNet, ViT) using `timm`, compatible with fast.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp model.vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Google Drive mounted successfully.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Mount Google Drive (Optional, but recommended for persistent storage)\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, skipping Drive mount.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indic_clip.core not found initially.\n",
      "Added /content/drive/MyDrive/Indic-Clip to sys.path\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive detected, setting PROJECT_ROOT to /content/drive/MyDrive/Indic-Clip\n",
      "Ensure your project files are located there.\n",
      "Imported indic_clip.core after path adjustment.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    print(\"Reloaded indic_clip.core\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"indic_clip.core not found initially.\")\n",
    "    # Attempt to set sys.path if running in Colab and project cloned\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        project_parent = '/content' # Assuming cloned into /content/indic-clip\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "             project_parent = '/content/drive/MyDrive/Indic-Clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive\")\n",
    "            # raise # Stop execution if core components missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Indic-Clip\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%cd /content/drive/MyDrive/Indic-Clip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.7/296.7 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.8/297.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "!pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import logging\n",
    "\n",
    "from fastai.vision.all import *\n",
    "\n",
    "try:\n",
    "    from indic_clip.core import get_logger, setup_logging\n",
    "except ModuleNotFoundError:\n",
    "    # Fallback if core not found (e.g. testing)\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "    def setup_logging(): pass\n",
    "\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision Encoder Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VisionEncoder(Module):\n",
    "    \"\"\"A wrapper for `timm` vision models to extract features.\n",
    "\n",
    "    Provides a consistent interface for different vision backbones like\n",
    "    ResNet, ViT, etc., loading pre-trained weights from `timm`.\n",
    "    Designed for integration into CLIP-like architectures.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = 'vit_base_patch16_224',\n",
    "                 pretrained: bool = True,\n",
    "                 output_dim: int | None = None):\n",
    "        \"\"\"\n",
    "        Initializes the Vision Encoder.\n",
    "\n",
    "        Args:\n",
    "            model_name (str): The name of the `timm` model to load (e.g., 'resnet50', 'vit_base_patch16_224').\n",
    "            pretrained (bool): Whether to load `timm`'s pre-trained weights (usually ImageNet).\n",
    "            output_dim (int | None): Optional dimension for a final projection layer.\n",
    "                                     If None, returns the raw features from the backbone.\n",
    "                                     If an int, adds a Linear layer to project features to this dimension.\n",
    "                                     This is typically handled by the main CLIP model's projection head,\n",
    "                                     so it might be better left as None here.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.pretrained = pretrained\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        try:\n",
    "            # Load the model from timm\n",
    "            # `num_classes=0` removes the final classification layer\n",
    "            # `global_pool='avg'` ensures avg pooling for CNNs, ViTs might handle pooling differently (e.g., CLS token)\n",
    "            self.backbone = timm.create_model(model_name, pretrained=pretrained, num_classes=0, global_pool='avg')\n",
    "            logger.info(f\"Loaded timm model: {model_name} with pretrained={pretrained}\")\n",
    "\n",
    "            # Get the feature dimension from the backbone\n",
    "            # For ViTs, this is often backbone.embed_dim\n",
    "            # For CNNs, it's often backbone.num_features\n",
    "            if hasattr(self.backbone, 'num_features'):\n",
    "                self.feature_dim = self.backbone.num_features\n",
    "            elif hasattr(self.backbone, 'embed_dim'):\n",
    "                 # Common for ViT models in timm\n",
    "                 self.feature_dim = self.backbone.embed_dim\n",
    "            else:\n",
    "                # Fallback: try forward pass with dummy data (can be slow)\n",
    "                try:\n",
    "                     with torch.no_grad():\n",
    "                         dummy_input = torch.randn(1, 3, 224, 224) # Assuming 224x224 input\n",
    "                         if '224' not in model_name:\n",
    "                            logger.warning(f\"Model name '{model_name}' doesn't specify input size, assuming 224x224 for feature dim check.\")\n",
    "                         dummy_output = self.backbone(dummy_input)\n",
    "                         self.feature_dim = dummy_output.shape[-1]\n",
    "                         logger.info(f\"Inferred feature dimension via dummy forward pass: {self.feature_dim}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Could not automatically determine feature dimension for {model_name}. Error: {e}\")\n",
    "                    self.feature_dim = None\n",
    "                    raise ValueError(f\"Could not determine feature dimension for {model_name}\") from e\n",
    "\n",
    "            logger.info(f\"Backbone feature dimension: {self.feature_dim}\")\n",
    "\n",
    "            # Optional projection layer\n",
    "            self.projection = nn.Identity()\n",
    "            if output_dim is not None and self.feature_dim is not None:\n",
    "                self.projection = nn.Linear(self.feature_dim, output_dim)\n",
    "                logger.info(f\"Added projection layer: Linear({self.feature_dim}, {output_dim})\")\n",
    "            elif output_dim is not None and self.feature_dim is None:\n",
    "                 logger.warning(f\"output_dim specified ({output_dim}) but could not determine backbone feature dimension. Projection layer skipped.\")\n",
    "\n",
    "            # Placeholder for gradient checkpointing enabling\n",
    "            self._gradient_checkpointing = False\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize VisionEncoder with model {model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass to extract image features.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor (batch_size, channels, height, width).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output features (batch_size, feature_dim or output_dim).\n",
    "        \"\"\"\n",
    "        # Handle gradient checkpointing if enabled and backbone supports it\n",
    "        if self._gradient_checkpointing and hasattr(self.backbone, 'set_grad_checkpointing'):\n",
    "             # Timm models often handle this internally, check if this is needed\n",
    "             # Or apply torch.utils.checkpoint if needed around specific blocks\n",
    "             # features = torch.utils.checkpoint.checkpoint(self.backbone, x)\n",
    "             features = self.backbone(x)\n",
    "        else:\n",
    "            features = self.backbone(x)\n",
    "\n",
    "        # Handle potential differences in output format (e.g., ViT CLS token vs avg pool)\n",
    "        # Timm's create_model with num_classes=0 and global_pool='avg' generally gives [B, D]\n",
    "        # If using a ViT model specifically, might need features = features[:, 0] if CLS token is used and global_pool='token'\n",
    "        # Check the specific timm model's behavior if needed.\n",
    "\n",
    "        # Apply optional projection\n",
    "        projected_features = self.projection(features)\n",
    "        return projected_features\n",
    "\n",
    "    def set_gradient_checkpointing(self, enable: bool = True):\n",
    "        \"\"\"Enables or disables gradient checkpointing for the backbone.\n",
    "           Note: Actual application depends on timm model support or custom checkpointing.\n",
    "        \"\"\"\n",
    "        if hasattr(self.backbone, 'set_grad_checkpointing'):\n",
    "            self.backbone.set_grad_checkpointing(enable=enable)\n",
    "            self._gradient_checkpointing = enable # Store state\n",
    "            logger.info(f\"Gradient checkpointing {'enabled' if enable else 'disabled'} via backbone method.\")\n",
    "        else:\n",
    "            # Store the desired state, forward pass logic might need adjustment\n",
    "            # if manual torch.utils.checkpoint is used\n",
    "            self._gradient_checkpointing = enable\n",
    "            logger.warning(f\"Backbone {self.model_name} might not directly support set_grad_checkpointing. Manual checkpointing may be needed in forward pass.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 08:34:02 - __main__ - INFO - Loaded timm model: resnet50 with pretrained=True\n",
      "2025-04-18 08:34:02 - __main__ - INFO - Backbone feature dimension: 2048\n",
      "2025-04-18 08:34:02 - __main__ - INFO - Added projection layer: Linear(2048, 512)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ResNet50 Example ---\n",
      "Encoder Type: <class '__main__.VisionEncoder'>\n",
      "Feature Dimension: 2048\n",
      "Output Dimension (after projection): 512\n",
      "Input shape: torch.Size([4, 3, 224, 224])\n",
      "Output shape: torch.Size([4, 512])\n",
      "Output dtype: torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 08:34:05 - __main__ - INFO - Loaded timm model: vit_base_patch16_224 with pretrained=True\n",
      "2025-04-18 08:34:05 - __main__ - INFO - Backbone feature dimension: 768\n",
      "2025-04-18 08:34:05 - __main__ - INFO - Loaded timm model: vit_base_patch16_224 with pretrained=True\n",
      "2025-04-18 08:34:05 - __main__ - INFO - Backbone feature dimension: 768\n",
      "2025-04-18 08:34:05 - __main__ - INFO - Gradient checkpointing disabled via backbone method.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ViT Example (No Projection) ---\n",
      "Encoder Type: <class '__main__.VisionEncoder'>\n",
      "Feature Dimension: 768\n",
      "Output Dimension (after projection): None\n",
      "Input shape: torch.Size([2, 3, 224, 224])\n",
      "Output shape: torch.Size([2, 768])\n",
      "Output dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "#| eval: false\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- ResNet18 Example ---\")\n",
    "    # Example with ResNet18 and a projection layer\n",
    "    try:\n",
    "        resnet_encoder = VisionEncoder(model_name='resnet18', pretrained=True, output_dim=512)\n",
    "        print(f\"Encoder Type: {type(resnet_encoder)}\")\n",
    "        print(f\"Feature Dimension: {resnet_encoder.feature_dim}\")\n",
    "        print(f\"Output Dimension (after projection): {resnet_encoder.output_dim}\")\n",
    "\n",
    "        # Create a dummy input batch\n",
    "        dummy_images = torch.randn(4, 3, 224, 224) # Batch size 4\n",
    "        print(f\"Input shape: {dummy_images.shape}\")\n",
    "\n",
    "        # Perform forward pass\n",
    "        resnet_encoder.eval() # Set to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            output_features = resnet_encoder(dummy_images)\n",
    "\n",
    "        print(f\"Output shape: {output_features.shape}\")\n",
    "        print(f\"Output dtype: {output_features.dtype}\")\n",
    "        assert output_features.shape == (4, 512)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ResNet18 example: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n--- ViT Example (No Projection) ---\")\n",
    "    # Example with ViT and no projection layer\n",
    "    try:\n",
    "        vit_encoder = VisionEncoder(model_name='vit_base_patch16_224', pretrained=True, output_dim=None)\n",
    "        print(f\"Encoder Type: {type(vit_encoder)}\")\n",
    "        print(f\"Feature Dimension: {vit_encoder.feature_dim}\")\n",
    "        print(f\"Output Dimension (after projection): {vit_encoder.output_dim}\")\n",
    "\n",
    "        # Enable gradient checkpointing (example)\n",
    "        # vit_encoder.set_gradient_checkpointing(True)\n",
    "        vit_encoder.set_gradient_checkpointing(False) # Keep it off for simple test\n",
    "\n",
    "        # Create a dummy input batch\n",
    "        dummy_images_vit = torch.randn(2, 3, 224, 224) # Batch size 2\n",
    "        print(f\"Input shape: {dummy_images_vit.shape}\")\n",
    "\n",
    "        # Perform forward pass\n",
    "        vit_encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            output_features_vit = vit_encoder(dummy_images_vit)\n",
    "\n",
    "        print(f\"Output shape: {output_features_vit.shape}\")\n",
    "        print(f\"Output dtype: {output_features_vit.dtype}\")\n",
    "        assert output_features_vit.shape == (2, 768)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ViT example: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # Run this in terminal to export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
