{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511667e1-d6ac-4184-a642-e4b39a3964b3",
   "metadata": {},
   "source": [
    "# Inference API\n",
    "\n",
    "> Provides high-level functions for loading a trained Indic-CLIP model and performing inference (feature extraction, similarity computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03210efd-480c-40e1-a01d-913f97d02216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b73e7c-930c-4162-85a1-b80e36978e79",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e33c05dd-ff09-4218-a489-81c680385697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /workspace/indic-clip to sys.path\n",
      "Imported indic_clip.core after path adjustment.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_parent = '/workspace'\n",
    "if Path('/workspace/indic-clip').exists():\n",
    "    project_parent = '/workspace/indic-clip'\n",
    "    if project_parent not in sys.path:\n",
    "        sys.path.insert(0, project_parent)\n",
    "        print(f\"Added {project_parent} to sys.path\")\n",
    "    try:\n",
    "        import indic_clip.core\n",
    "        print(\"Imported indic_clip.core after path adjustment.\")\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "        print(\"Expected: /workspace/indic-clip/indic-clip/core.py or similar in Drive\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2775b68-c639-481a-a1f9-f0c197a749a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Install requirements if needed (especially in Colab)\n",
    "# !pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9e1756-241b-449b-8f22-98c3e97578be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60a38eb-2899-458e-9617-671c0209c2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import albumentations\n",
    "import logging\n",
    "import PIL\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import List, Union, Dict, Optional\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.vision.all import *\n",
    "from torchvision.transforms import Compose\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# --- Project Imports ---\n",
    "from indic_clip.core import (\n",
    "        get_logger, setup_logging, DEFAULT_IMAGE_SIZE, DEFAULT_EMBED_DIM, PRETRAINED_TOKENIZER_NAME, TOKENIZER_PATH,\n",
    "        CHECKPOINT_PATH, ensure_dir, PROJECT_ROOT\n",
    "    )\n",
    "from indic_clip.model.clip import IndicCLIP\n",
    "from indic_clip.data.tokenization import IndicBERTTokenizer\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cc67c-e20a-4f2a-b2a9-5d71b426905d",
   "metadata": {},
   "source": [
    "## Core Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e089746-f523-4120-80ee-d839d9f27246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_indic_clip_model(\n",
    "    checkpoint_path: Union[str, Path],\n",
    "    model_config: Optional[Dict] = None, # Optional: pass model config if not saved with ckpt\n",
    "    device: Optional[Union[str, torch.device]] = None\n",
    ") -> IndicCLIP:\n",
    "    \"\"\"\n",
    "    Loads a trained IndicCLIP model from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (Union[str, Path]): Path to the .pth checkpoint file.\n",
    "        model_config (Optional[Dict]): Dictionary containing model configuration args\n",
    "                                      (embed_dim, vision_model_name, text_model_name, etc.)\n",
    "                                      needed to instantiate the model structure.\n",
    "                                      If None, attempts to infer from defaults or common patterns.\n",
    "        device (Optional[Union[str, torch.device]]): Device to load the model onto ('cuda', 'cpu', etc.).\n",
    "                                                      Defaults to CUDA if available, else CPU.\n",
    "\n",
    "    Returns:\n",
    "        IndicCLIP: The loaded model instance in evaluation mode.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the checkpoint file does not exist.\n",
    "        TypeError: If model_config is needed but not provided.\n",
    "        RuntimeError: If there's an issue loading the state dict.\n",
    "    \"\"\"\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    if not checkpoint_path.is_file():\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Loading model onto device: {device}\")\n",
    "\n",
    "    # --- Instantiate Model Structure ---\n",
    "    # Best practice: Save model config with checkpoint. If not, require it here.\n",
    "    if model_config is None:\n",
    "        # Attempt to use defaults - this might be incorrect!\n",
    "        logger.warning(\"Model config not provided. Attempting to load using default parameters. This may fail or lead to unexpected behavior if the checkpoint was saved with different settings.\")\n",
    "        model_config = {\n",
    "            'embed_dim': DEFAULT_EMBED_DIM,\n",
    "            'vision_model_name': 'vit_base_patch16_224', # Make sure this matches your training!\n",
    "            'vision_pretrained': False, # Pretrained flag doesn't matter much for loading weights\n",
    "            'text_model_name': PRETRAINED_TOKENIZER_NAME, # Make sure this matches!\n",
    "            'text_pretrained': False,\n",
    "            # Tokenizer instance needed if custom tokens were added, load separately!\n",
    "            'tokenizer': IndicBERTTokenizer.load_tokenizer(TOKENIZER_PATH) # Assume default path\n",
    "        }\n",
    "        # If tokenizer is required and fails to load, raise an error\n",
    "        if not model_config['tokenizer']:\n",
    "             raise ValueError(\"Could not load default tokenizer required for model instantiation.\")\n",
    "\n",
    "    # Ensure tokenizer is present in config for IndicCLIP constructor\n",
    "    if 'tokenizer' not in model_config or model_config['tokenizer'] is None:\n",
    "         logger.warning(\"Loading model without providing a tokenizer instance. Trying to load default.\")\n",
    "         try:\n",
    "             model_config['tokenizer'] = IndicBERTTokenizer.load_tokenizer(TOKENIZER_PATH)\n",
    "             if not model_config['tokenizer']:\n",
    "                 raise ValueError(\"Failed to load default tokenizer.\")\n",
    "         except Exception as e:\n",
    "             raise ValueError(f\"Could not load default tokenizer required for model instantiation from {TOKENIZER_PATH}. Please provide it in model_config. Error: {e}\")\n",
    "\n",
    "    try:\n",
    "        model = IndicCLIP(**model_config)\n",
    "        logger.info(f\"Instantiated IndicCLIP model structure with config: {model_config}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to instantiate model with provided config: {e}\", exc_info=True)\n",
    "        raise TypeError(f\"Could not instantiate IndicCLIP model. Check model_config. Error: {e}\")\n",
    "\n",
    "    # --- Load State Dict ---\n",
    "    try:\n",
    "        # Map location ensures model loads correctly regardless of where it was saved\n",
    "        # *** ADD weights_only=False HERE ***\n",
    "        state_dict = torch.load(checkpoint_path, map_location='cpu', weights_only=False)\n",
    "        logger.info(f\"Loaded checkpoint using weights_only=False.\")\n",
    "\n",
    "        # Handle potential keys mismatch (e.g., if saved directly or via Learner.save)\n",
    "        if 'model' in state_dict:\n",
    "            state_dict = state_dict['model']\n",
    "        elif 'state_dict' in state_dict:\n",
    "            state_dict = state_dict['state_dict']\n",
    "        # If neither 'model' nor 'state_dict' key is present, assume the whole dict is the state_dict\n",
    "        \n",
    "        # Remove potential DDP prefix 'module.'\n",
    "        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "        # Load the weights\n",
    "        model.load_state_dict(state_dict)\n",
    "        logger.info(f\"Successfully loaded model weights from {checkpoint_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading state dict from {checkpoint_path}: {e}\", exc_info=True)\n",
    "        # Add the original exception 'e' to the message for more context\n",
    "        raise RuntimeError(f\"Failed to load state dict. Ensure checkpoint is valid and config matches. Error: {e}\")\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    model.to(device) # Move model to the target device\n",
    "    logger.info(\"Model set to evaluation mode.\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a78c6e4-6c6a-4541-a761-12d6df1411d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_image_transform(img_size: int = DEFAULT_IMAGE_SIZE):\n",
    "    \"\"\"Creates a standard image transform pipeline for inference.\"\"\"\n",
    "    # Use transforms similar to validation set\n",
    "    return Compose([\n",
    "        Resize(img_size, method='squish'), # Or 'pad'\n",
    "        ToTensor()\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b45f68a8-2c78-4d94-99d6-ff6496c3f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def extract_image_features(\n",
    "    model: IndicCLIP,\n",
    "    image_input: Union[str, Path, Image.Image, torch.Tensor, List[Union[str, Path, Image.Image, torch.Tensor]]],\n",
    "    img_size: int = DEFAULT_IMAGE_SIZE,\n",
    "    batch_size: int = 32,\n",
    "    device: Optional[Union[str, torch.device]] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extracts normalized features for one or more images using the IndicCLIP model.\n",
    "    Manually handles image loading, resizing, tensor conversion, and normalization\n",
    "    to ensure device consistency.\n",
    "\n",
    "    Args:\n",
    "        model (IndicCLIP): The loaded IndicCLIP model instance.\n",
    "        image_input: A single image (path, PIL Image, or preprocessed Tensor) or a list of images.\n",
    "        img_size (int): The image size the model expects (both width and height).\n",
    "        batch_size (int): Batch size for processing multiple images.\n",
    "        device (Optional[Union[str, torch.device]]): Device to use for inference. If None, uses model's device.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the normalized image features (N, embed_dim).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    else:\n",
    "        model.to(device) # Ensure model is on the specified device\n",
    "\n",
    "    # Prepare normalization transform (will be applied on device later)\n",
    "    # Using torchvision's Normalize\n",
    "    mean_tensor = torch.tensor(imagenet_stats[0], device=device)\n",
    "    std_tensor = torch.tensor(imagenet_stats[1], device=device)\n",
    "    \n",
    "    def normalize_batch(b):\n",
    "        # b shape is (N, C, H, W)\n",
    "        b = b.to(device) # Ensure batch is on the correct device\n",
    "        mean = mean_tensor.view(1, -1, 1, 1) # Reshape for broadcasting\n",
    "        std = std_tensor.view(1, -1, 1, 1)\n",
    "        return (b - mean) / std\n",
    "\n",
    "    if not isinstance(image_input, list):\n",
    "        image_input = [image_input]\n",
    "\n",
    "    all_features = []\n",
    "    logger.info(f\"Processing {len(image_input)} images in batches of {batch_size}...\")\n",
    "    for i in range(0, len(image_input), batch_size):\n",
    "        batch_inputs = image_input[i:i+batch_size]\n",
    "        processed_batch_cpu = [] # Collect processed tensors on CPU first\n",
    "        \n",
    "        for item in batch_inputs:\n",
    "            try:\n",
    "                if isinstance(item, (str, Path)):\n",
    "                    # 1. Load PIL image\n",
    "                    img_pil = Image.open(item).convert('RGB')\n",
    "                elif isinstance(item, Image.Image):\n",
    "                    # 1. Assume PIL Image\n",
    "                    img_pil = item.convert('RGB')\n",
    "                elif isinstance(item, torch.Tensor):\n",
    "                    # If it's already a tensor, assume it's *almost* ready (e.g., NCHW, float)\n",
    "                    # We'll just move it to CPU before potential stacking and handle normalization later\n",
    "                    processed_batch_cpu.append(item.cpu())\n",
    "                    continue # Skip manual processing for tensors\n",
    "                else:\n",
    "                    logger.warning(f\"Unsupported image input type: {type(item)}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                # --- Manual Processing for PIL images ---\n",
    "                # 2. Resize PIL image\n",
    "                # Use ANTIALIAS for better quality resizing\n",
    "                img_pil = img_pil.resize((img_size, img_size), Image.Resampling.LANCZOS) \n",
    "                \n",
    "                # 3. Convert PIL image to PyTorch Tensor (on CPU)\n",
    "                # This converts HWC [0, 255] uint8 to CHW [0.0, 1.0] float32\n",
    "                img_tensor_cpu = T.functional.to_tensor(img_pil)\n",
    "                \n",
    "                processed_batch_cpu.append(img_tensor_cpu)\n",
    "                # --- End Manual Processing ---\n",
    "\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Failed to process image item: {item}. Error: {e}\", exc_info=True)\n",
    "                 # Continue to next item in batch if one fails\n",
    "\n",
    "        if not processed_batch_cpu:\n",
    "            logger.warning(f\"Batch {i//batch_size + 1} is empty after processing/errors.\")\n",
    "            continue # Skip if batch is empty\n",
    "\n",
    "        # Stack processed images into a batch tensor (on CPU)\n",
    "        try:\n",
    "            batch_tensor_cpu = torch.stack(processed_batch_cpu)\n",
    "        except RuntimeError as e:\n",
    "            logger.error(f\"Error stacking tensors in batch {i//batch_size + 1}. Check image processing consistency. Error: {e}\")\n",
    "            for idx, t in enumerate(processed_batch_cpu): logger.error(f\"  Item {idx} shape: {t.shape}, dtype: {t.dtype}\")\n",
    "            continue # Skip this batch\n",
    "\n",
    "        # Move the batch tensor to the target device\n",
    "        batch_tensor = batch_tensor_cpu.to(device)\n",
    "\n",
    "        # Normalize the batch *on the target device* using explicit normalization\n",
    "        try:\n",
    "             batch_tensor = normalize_batch(batch_tensor) # Apply normalization\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error normalizing batch {i//batch_size + 1} on device {device}. Error: {e}\")\n",
    "             continue # Skip batch if normalization fails\n",
    "\n",
    "        # --- Extract features using the model ---\n",
    "        try:\n",
    "            features = model.encode_image(batch_tensor) # Model expects input on its device\n",
    "            all_features.append(features.cpu()) # Move features back to CPU immediately to free GPU mem\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during model.encode_image for batch {i//batch_size + 1}. Error: {e}\", exc_info=True)\n",
    "            # Optionally log input tensor stats\n",
    "            logger.error(f\"Input batch tensor stats: mean={batch_tensor.mean()}, std={batch_tensor.std()}, min={batch_tensor.min()}, max={batch_tensor.max()}\")\n",
    "            continue # Skip batch if encoding fails\n",
    "\n",
    "    if not all_features:\n",
    "        logger.error(\"No image features could be extracted.\")\n",
    "        # Return an empty tensor with the correct embedding dimension\n",
    "        try:\n",
    "            # Accessing projection layer directly might be fragile if model structure changes\n",
    "            # It's better if embed_dim is stored on the model, e.g., self.embed_dim\n",
    "            embed_dim = model.visual_projection.out_features\n",
    "        except AttributeError:\n",
    "            logger.warning(\"Could not determine model embedding dimension from visual_projection, using default.\")\n",
    "            embed_dim = DEFAULT_EMBED_DIM # Fallback\n",
    "        return torch.empty((0, embed_dim))\n",
    "\n",
    "    logger.info(f\"Successfully extracted features for {len(torch.cat(all_features))} images.\")\n",
    "    return torch.cat(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9f755f4-8e32-411d-b92b-84c89639f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def extract_text_features(\n",
    "    model: IndicCLIP,\n",
    "    tokenizer: IndicBERTTokenizer,\n",
    "    text_input: Union[str, List[str]],\n",
    "    batch_size: int = 32,\n",
    "    device: Optional[Union[str, torch.device]] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extracts normalized features for one or more text strings using the IndicCLIP model.\n",
    "\n",
    "    Args:\n",
    "        model (IndicCLIP): The loaded IndicCLIP model instance.\n",
    "        tokenizer (IndicBERTTokenizer): The tokenizer instance compatible with the model.\n",
    "        text_input (Union[str, List[str]]): A single text string or a list of strings.\n",
    "        batch_size (int): Batch size for processing multiple texts.\n",
    "        device (Optional[Union[str, torch.device]]): Device to use for inference. If None, uses model's device.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the normalized text features (N, embed_dim).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    else:\n",
    "        model.to(device)\n",
    "\n",
    "    if isinstance(text_input, str):\n",
    "        text_input = [text_input]\n",
    "\n",
    "    all_features = []\n",
    "    for i in range(0, len(text_input), batch_size):\n",
    "        batch_texts = text_input[i:i+batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Tokenize the batch\n",
    "            inputs = tokenizer.tokenize(batch_texts)\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            \n",
    "            # Extract features\n",
    "            features = model.encode_text(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_features.append(features.cpu()) # Move features to CPU\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing text batch starting with '{batch_texts[0][:50]}...': {e}\")\n",
    "            continue # Skip batch on error\n",
    "\n",
    "    if not all_features:\n",
    "        logger.error(\"No text features could be extracted.\")\n",
    "        embed_dim = model.text_projection.out_features # Get embed_dim from model\n",
    "        return torch.empty((0, embed_dim))\n",
    "\n",
    "    return torch.cat(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f4d6628-d103-4832-9585-f127e170a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def compute_similarity(\n",
    "    model: IndicCLIP,\n",
    "    image_features: torch.Tensor,\n",
    "    text_features: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the similarity matrix between image and text features using the model's logit scale.\n",
    "\n",
    "    Args:\n",
    "        model (IndicCLIP): The loaded IndicCLIP model instance (used for logit_scale).\n",
    "        image_features (torch.Tensor): Normalized image features (N, embed_dim).\n",
    "        text_features (torch.Tensor): Normalized text features (M, embed_dim).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A similarity matrix of shape (N, M).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device # Use model's device\n",
    "    \n",
    "    # Ensure features are on the correct device\n",
    "    image_features = image_features.to(device)\n",
    "    text_features = text_features.to(device)\n",
    "    \n",
    "    # Get the exponentiated logit scale from the model\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    \n",
    "    # Compute similarity (dot product of normalized features, scaled by temperature)\n",
    "    # Features should already be normalized by the extraction functions\n",
    "    similarity = logit_scale * image_features @ text_features.t()\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b30d5-a10a-4218-8745-f3a55f3615e2",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4840e138-0604-42fe-b616-f468e97c400a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Inference API Example ---\n",
      "CHECKPOINT_PATH: /workspace/indic-clip/models/checkpoints\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 13:51:18 - indic_clip.data.tokenization - INFO - Successfully loaded tokenizer: /workspace/indic-clip/models/tokenizer\n",
      "2025-04-22 13:51:18 - indic_clip.data.tokenization - INFO - Custom special tokens already exist or none were specified.\n",
      "2025-04-22 13:51:18 - indic_clip.data.tokenization - INFO - Tokenizer state loaded successfully from /workspace/indic-clip/models/tokenizer\n",
      "2025-04-22 13:51:18 - __main__ - INFO - Loading model onto device: cuda\n",
      "2025-04-22 13:51:18 - indic_clip.model.vision - INFO - Loaded timm model: resnet50 with pretrained=False\n",
      "2025-04-22 13:51:18 - indic_clip.model.vision - INFO - Backbone feature dimension: 2048\n",
      "2025-04-22 13:51:19 - indic_clip.model.text - INFO - Loading text model: ai4bharat/indic-bert with pretrained=False\n",
      "2025-04-22 13:51:19 - indic_clip.model.text - INFO - Model hidden dimension: 768\n",
      "2025-04-22 13:51:19 - indic_clip.model.text - WARNING - Initializing text model ai4bharat/indic-bert from scratch (pretrained=False).\n",
      "2025-04-22 13:51:19 - indic_clip.model.text - WARNING - Tokenizer vocab size (200002) differs from model embedding size (200000). Resizing model token embeddings.\n",
      "2025-04-22 13:51:19 - indic_clip.model.text - INFO - Model embedding size resized to 200002\n",
      "2025-04-22 13:51:19 - indic_clip.model.clip - INFO - IndicCLIP initialized with vision='resnet50', text='ai4bharat/indic-bert', embed_dim=512\n",
      "2025-04-22 13:51:19 - __main__ - INFO - Instantiated IndicCLIP model structure with config: {'embed_dim': 512, 'vision_model_name': 'resnet50', 'vision_pretrained': False, 'text_model_name': 'ai4bharat/indic-bert', 'text_pretrained': False, 'tokenizer': <indic_clip.data.tokenization.IndicBERTTokenizer object at 0x758f606ad120>}\n",
      "2025-04-22 13:51:19 - __main__ - INFO - Loaded checkpoint using weights_only=False.\n",
      "2025-04-22 13:51:20 - __main__ - INFO - Successfully loaded model weights from /workspace/indic-clip/models/checkpoints/best_valid_loss.pth\n",
      "2025-04-22 13:51:20 - __main__ - INFO - Model set to evaluation mode.\n",
      "2025-04-22 13:51:20 - indic_clip.data.tokenization - INFO - Successfully loaded tokenizer: /workspace/indic-clip/models/tokenizer\n",
      "2025-04-22 13:51:20 - indic_clip.data.tokenization - INFO - Custom special tokens already exist or none were specified.\n",
      "2025-04-22 13:51:20 - indic_clip.data.tokenization - INFO - Tokenizer state loaded successfully from /workspace/indic-clip/models/tokenizer\n",
      "2025-04-22 13:51:21 - __main__ - INFO - Processing 1 images in batches of 32...\n",
      "2025-04-22 13:51:21 - __main__ - INFO - Successfully extracted features for 1 images.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n",
      "\n",
      "Extracting image features...\n",
      "Image features shape: torch.Size([1, 512])\n",
      "\n",
      "Extracting text features...\n",
      "Text features shape: torch.Size([3, 512])\n",
      "\n",
      "Computing similarity...\n",
      "Similarity matrix shape: torch.Size([1, 3])\n",
      "Similarity scores:\n",
      "[[ 3.089652    0.62385345 -0.25372863]]\n",
      "\n",
      "Best matching text for the image: 'एक बिल्ली सोफे पर सो रही है।' (Score: 3.0897)\n",
      "\n",
      "--- Inference API Example Finished ---\n"
     ]
    }
   ],
   "source": [
    "#| eval: false \n",
    "# --- Example: Load Model and Run Inference ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Running Inference API Example ---\")\n",
    "    \n",
    "    # --- Configuration ---\n",
    "    # !! IMPORTANT: Replace with the *actual* path to your trained checkpoint !!\n",
    "    # checkpoint_name = 'best_recall_interactive.pth' # From 10_training example\n",
    "    checkpoint_name = 'best_valid_loss.pth' # Default name if SaveModelCallback wasn't configured fully\n",
    "    print(f'CHECKPOINT_PATH: {CHECKPOINT_PATH}')\n",
    "    checkpoint_file = CHECKPOINT_PATH / checkpoint_name\n",
    "    \n",
    "    # !! IMPORTANT: Provide the config used during training if it wasn't saved with the checkpoint !!\n",
    "    #             (If Learner.save was used, config is often not saved)\n",
    "    # Example: Use the smaller ResNet18 config from the 10_training notebook test\n",
    "    model_configuration = {\n",
    "        'embed_dim': 512, # Example: ResNet18 from test\n",
    "        'vision_model_name': 'resnet50', # Example: ResNet18 from test\n",
    "        'vision_pretrained': False, # Doesn't matter for loading state_dict\n",
    "        'text_model_name': PRETRAINED_TOKENIZER_NAME,\n",
    "        'text_pretrained': False,\n",
    "        'tokenizer': IndicBERTTokenizer.load_tokenizer(TOKENIZER_PATH) # Load the tokenizer used for training\n",
    "    }\n",
    "    \n",
    "    # Create dummy checkpoint if it doesn't exist for demonstration\n",
    "    if not checkpoint_file.exists():\n",
    "        print(f\"Warning: Checkpoint {checkpoint_file} not found. Creating a dummy model and saving it.\")\n",
    "        ensure_dir(checkpoint_file.parent)\n",
    "        dummy_model = IndicCLIP(**model_configuration) \n",
    "        torch.save(dummy_model.state_dict(), checkpoint_file)\n",
    "        print(\"Dummy checkpoint created.\")\n",
    "\n",
    "    # --- Load Model and Tokenizer ---\n",
    "    try:\n",
    "        loaded_model = load_indic_clip_model(checkpoint_file, model_config=model_configuration)\n",
    "        loaded_tokenizer = IndicBERTTokenizer.load_tokenizer(TOKENIZER_PATH)\n",
    "        print(\"Model and tokenizer loaded successfully.\")\n",
    "\n",
    "        # --- Prepare Sample Data ---\n",
    "        # Use a real image path if available, otherwise create dummy\n",
    "        sample_image_path = PROJECT_ROOT / 'cat.jpg' # Create this image or use a real one\n",
    "        if not sample_image_path.exists():\n",
    "             print(f\"Creating dummy image at: {sample_image_path}\")\n",
    "             dummy_pil_img = Image.new('RGB', (250, 300), color='cyan')\n",
    "             dummy_pil_img.save(sample_image_path)\n",
    "\n",
    "        sample_texts = [\n",
    "            \"एक बिल्ली सोफे पर सो रही है।\", # A cat sleeping on a sofa\n",
    "            \"समुद्र तट पर सूर्यास्त।\",    # Sunset on a beach\n",
    "            \"साड़ी पहने एक महिला।\"       # A woman wearing a saree\n",
    "        ]\n",
    "\n",
    "        # --- Extract Features ---\n",
    "        print(\"\\nExtracting image features...\")\n",
    "        image_features = extract_image_features(loaded_model, sample_image_path)\n",
    "        print(f\"Image features shape: {image_features.shape}\")\n",
    "\n",
    "        print(\"\\nExtracting text features...\")\n",
    "        text_features = extract_text_features(loaded_model, loaded_tokenizer, sample_texts)\n",
    "        print(f\"Text features shape: {text_features.shape}\")\n",
    "\n",
    "        # --- Compute Similarity ---\n",
    "        print(\"\\nComputing similarity...\")\n",
    "        similarity_matrix = compute_similarity(loaded_model, image_features, text_features)\n",
    "        print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "        print(\"Similarity scores:\")\n",
    "        print(similarity_matrix.cpu().numpy())\n",
    "\n",
    "        # Find best matching text for the image\n",
    "        best_match_idx = similarity_matrix.argmax().item()\n",
    "        print(f\"\\nBest matching text for the image: '{sample_texts[best_match_idx]}' (Score: {similarity_matrix[0, best_match_idx]:.4f})\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(\"Please ensure the correct checkpoint and tokenizer paths are set.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    # Optional: Clean up dummy image\n",
    "    # if 'dummy_pil_img' in locals() and sample_image_path.exists():\n",
    "    #    os.remove(sample_image_path)\n",
    "    #    print(f\"Cleaned up dummy image: {sample_image_path}\")\n",
    "\n",
    "    print(\"\\n--- Inference API Example Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7130b1-4224-4309-b506-84423bd25392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # Run this in terminal to export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
