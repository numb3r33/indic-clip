{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511667e1-d6ac-4184-a642-e4b39a3964b3",
   "metadata": {},
   "source": [
    "# Inference API\n",
    "\n",
    "> Provides high-level functions for loading a trained Indic-CLIP model and performing inference (feature extraction, similarity computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03210efd-480c-40e1-a01d-913f97d02216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b73e7c-930c-4162-85a1-b80e36978e79",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c05dd-ff09-4218-a489-81c680385697",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Mount Google Drive (Optional, but recommended for persistent storage)\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "    # Define PROJECT_ROOT for Colab\n",
    "    PROJECT_ROOT = Path('/content/drive/MyDrive/Indic-Clip') # Adjust path if needed\n",
    "    if not PROJECT_ROOT.exists():\n",
    "        print(f\"Warning: Project directory not found at {PROJECT_ROOT}. Please ensure it exists.\")\n",
    "    else:\n",
    "        # Add project root to sys.path\n",
    "        if str(PROJECT_ROOT) not in sys.path:\n",
    "            sys.path.insert(0, str(PROJECT_ROOT))\n",
    "            print(f\"Added {PROJECT_ROOT} to sys.path\")\n",
    "        # Change current working directory\n",
    "        os.chdir(PROJECT_ROOT)\n",
    "        print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, skipping Drive mount.\")\n",
    "    # Define PROJECT_ROOT for local execution (adjust if needed)\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    if PROJECT_ROOT.name == 'nbs': PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    print(f\"Running locally. Project root assumed: {PROJECT_ROOT}\")\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "         sys.path.insert(0, str(PROJECT_ROOT))\n",
    "         print(f\"Added {PROJECT_ROOT} to sys.path\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Colab setup: {e}\")\n",
    "    PROJECT_ROOT = Path('.').resolve()\n",
    "    print(f\"Defaulting project root to current dir: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2775b68-c639-481a-a1f9-f0c197a749a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Install requirements if needed (especially in Colab)\n",
    "# !pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e1756-241b-449b-8f22-98c3e97578be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a38eb-2899-458e-9617-671c0209c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from typing import List, Union, Dict, Optional\n",
    "\n",
    "from fastcore.all import *\n",
    "from fastai.vision.all import (\n",
    "    PILImage, Resize, ToTensor, Normalize, imagenet_stats, Learner, DataLoaders, Datasets, TfmdLists\n",
    ")\n",
    "\n",
    "# --- Project Imports ---\n",
    "try:\n",
    "    from indic_clip.core import (\n",
    "        get_logger, setup_logging, DEFAULT_IMAGE_SIZE, DEFAULT_EMBED_DIM, PRETRAINED_TOKENIZER_NAME, TOKENIZER_PATH,\n",
    "        CHECKPOINT_PATH\n",
    "    )\n",
    "    from indic_clip.model.clip import IndicCLIP\n",
    "    from indic_clip.data.tokenization import IndicBERTTokenizer\n",
    "    # Import other necessary components if needed, e.g., specific DataLoaders\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Could not import project modules in 13_inference.ipynb. Using fallbacks.\")\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "    def setup_logging(): pass\n",
    "    DEFAULT_IMAGE_SIZE = 224\n",
    "    DEFAULT_EMBED_DIM = 768\n",
    "    PRETRAINED_TOKENIZER_NAME = \"ai4bharat/indic-bert\"\n",
    "    TOKENIZER_PATH = Path('./models/tokenizer')\n",
    "    CHECKPOINT_PATH = Path('./models/checkpoints')\n",
    "    # Dummy classes\n",
    "    class IndicCLIP(torch.nn.Module):\n",
    "        def __init__(self, *args, **kwargs): super().__init__()\n",
    "        def encode_image(self, x): return torch.randn(x.shape[0], kwargs.get('embed_dim', 768))\n",
    "        def encode_text(self, *args, **kwargs): return torch.randn(args[0].shape[0], kwargs.get('embed_dim', 768))\n",
    "        def eval(self): pass\n",
    "        def to(self, device): return self\n",
    "        logit_scale = torch.nn.Parameter(torch.tensor(1.0))\n",
    "\n",
    "    class IndicBERTTokenizer:\n",
    "        @classmethod\n",
    "        def load_tokenizer(cls, *args, **kwargs): return cls()\n",
    "        def tokenize(self, texts):\n",
    "            if isinstance(texts, str): texts = [texts]\n",
    "            bs = len(texts)\n",
    "            seq_len = 32\n",
    "            return {'input_ids': torch.randint(0, 1000, (bs, seq_len)),\n",
    "                    'attention_mask': torch.ones((bs, seq_len), dtype=torch.long)}\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182cc67c-e20a-4f2a-b2a9-5d71b426905d",
   "metadata": {},
   "source": [
    "## Core Inference Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e089746-f523-4120-80ee-d839d9f27246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def load_indic_clip_model(\n",
    "    checkpoint_path: Union[str, Path],\n",
    "    model_config: Optional[Dict] = None, # Optional: pass model config if not saved with ckpt\n",
    "    device: Optional[Union[str, torch.device]] = None\n",
    ") -> IndicCLIP:\n",
    "    \"\"\"\n",
    "    Loads a trained IndicCLIP model from a checkpoint file.\n",
    "\n",
    "    Args:\n",
    "        checkpoint_path (Union[str, Path]): Path to the .pth checkpoint file.\n",
    "        model_config (Optional[Dict]): Dictionary containing model configuration args \n",
    "                                      (embed_dim, vision_model_name, text_model_name, etc.) \n",
    "                                      needed to instantiate the model structure. \n",
    "                                      If None, attempts to infer from defaults or common patterns.\n",
    "        device (Optional[Union[str, torch.device]]): Device to load the model onto ('cuda', 'cpu', etc.). \n",
    "                                                      Defaults to CUDA if available, else CPU.\n",
    "\n",
    "    Returns:\n",
    "        IndicCLIP: The loaded model instance in evaluation mode.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the checkpoint file does not exist.\n",
    "        TypeError: If model_config is needed but not provided.\n",
    "        RuntimeError: If there's an issue loading the state dict.\n",
    "    \"\"\"\n",
    "    checkpoint_path = Path(checkpoint_path)\n",
    "    if not checkpoint_path.is_file():\n",
    "        raise FileNotFoundError(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Loading model onto device: {device}\")\n",
    "\n",
    "    # --- Instantiate Model Structure ---\n",
    "    # Best practice: Save model config with checkpoint. If not, require it here.\n",
    "    if model_config is None:\n",
    "        # Attempt to use defaults - this might be incorrect!\n",
    "        logger.warning(\"Model config not provided. Attempting to load using default parameters. This may fail or lead to unexpected behavior if the checkpoint was saved with different settings.\")\n",
    "        model_config = {\n",
    "            'embed_dim': DEFAULT_EMBED_DIM,\n",
    "            'vision_model_name': 'vit_base_patch16_224', # Make sure this matches your training!\n",
    "            'vision_pretrained': False, # Pretrained flag doesn't matter much for loading weights\n",
    "            'text_model_name': PRETRAINED_TOKENIZER_NAME, # Make sure this matches!\n",
    "            'text_pretrained': False,\n",
    "            # Tokenizer instance needed if custom tokens were added, load separately!\n",
    "            'tokenizer': IndicBERTTokenizer.load_tokenizer(TOKENIZER_PATH) # Assume default path\n",
    "        }\n",
    "        # If tokenizer is required and fails to load, raise an error\n",
    "        if not model_config['tokenizer']:\n",
    "             raise ValueError(\"Could not load default tokenizer required for model instantiation.\")\n",
    "    \n",
    "    # Ensure tokenizer is present in config for IndicCLIP constructor\n",
    "    if 'tokenizer' not in model_config or model_config['tokenizer'] is None:\n",
    "         logger.warning(\"Loading model without providing a tokenizer instance. Trying to load default.\")\n",
    "         try:\n",
    "             model_config['tokenizer'] = IndicBERTTokenizer.load_tokenizer(TOKENIZER_PATH)\n",
    "             if not model_config['tokenizer']:\n",
    "                 raise ValueError(\"Failed to load default tokenizer.\")\n",
    "         except Exception as e:\n",
    "             raise ValueError(f\"Could not load default tokenizer required for model instantiation from {TOKENIZER_PATH}. Please provide it in model_config. Error: {e}\")\n",
    "             \n",
    "    try:\n",
    "        model = IndicCLIP(**model_config)\n",
    "        logger.info(f\"Instantiated IndicCLIP model structure with config: {model_config}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to instantiate model with provided config: {e}\", exc_info=True)\n",
    "        raise TypeError(f\"Could not instantiate IndicCLIP model. Check model_config. Error: {e}\")\n",
    "\n",
    "    # --- Load State Dict --- \n",
    "    try:\n",
    "        # Map location ensures model loads correctly regardless of where it was saved\n",
    "        state_dict = torch.load(checkpoint_path, map_location='cpu') \n",
    "        \n",
    "        # Handle potential keys mismatch (e.g., if saved directly or via Learner.save)\n",
    "        if 'model' in state_dict:\n",
    "            state_dict = state_dict['model']\n",
    "        elif 'state_dict' in state_dict:\n",
    "            state_dict = state_dict['state_dict']\n",
    "\n",
    "        # Remove potential DDP prefix 'module.'\n",
    "        state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "        # Load the weights\n",
    "        model.load_state_dict(state_dict)\n",
    "        logger.info(f\"Successfully loaded model weights from {checkpoint_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading state dict from {checkpoint_path}: {e}\", exc_info=True)\n",
    "        raise RuntimeError(f\"Failed to load state dict. Ensure checkpoint is valid and config matches. Error: {e}\")\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    model.to(device) # Move model to the target device\n",
    "    logger.info(\"Model set to evaluation mode.\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a78c6e4-6c6a-4541-a761-12d6df1411d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _get_image_transform(img_size: int = DEFAULT_IMAGE_SIZE):\n",
    "    \"\"\"Creates a standard image transform pipeline for inference.\"\"\"\n",
    "    # Use transforms similar to validation set\n",
    "    return TfmCompose([\n",
    "        Resize(img_size, method='squish'), # Or 'pad'\n",
    "        ToTensor(),\n",
    "        Normalize.from_stats(*imagenet_stats)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45f68a8-2c78-4d94-99d6-ff6496c3f211",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def extract_image_features(\n",
    "    model: IndicCLIP,\n",
    "    image_input: Union[str, Path, PIL.Image.Image, torch.Tensor, List[Union[str, Path, PIL.Image.Image, torch.Tensor]]],\n",
    "    img_size: int = DEFAULT_IMAGE_SIZE,\n",
    "    batch_size: int = 32,\n",
    "    device: Optional[Union[str, torch.device]] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extracts normalized features for one or more images using the IndicCLIP model.\n",
    "\n",
    "    Args:\n",
    "        model (IndicCLIP): The loaded IndicCLIP model instance.\n",
    "        image_input: A single image (path, PIL Image, or preprocessed Tensor) or a list of images.\n",
    "        img_size (int): The image size the model expects.\n",
    "        batch_size (int): Batch size for processing multiple images.\n",
    "        device (Optional[Union[str, torch.device]]): Device to use for inference. If None, uses model's device.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the normalized image features (N, embed_dim).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    else:\n",
    "        model.to(device)\n",
    "\n",
    "    transform = _get_image_transform(img_size)\n",
    "    \n",
    "    if not isinstance(image_input, list):\n",
    "        image_input = [image_input]\n",
    "\n",
    "    all_features = []\n",
    "    for i in range(0, len(image_input), batch_size):\n",
    "        batch_inputs = image_input[i:i+batch_size]\n",
    "        processed_batch = []\n",
    "        for item in batch_inputs:\n",
    "            if isinstance(item, (str, Path)):\n",
    "                try:\n",
    "                    img = PILImage.create(item)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Failed to load image: {item}. Error: {e}\")\n",
    "                    continue # Skip this image\n",
    "            elif isinstance(item, Image.Image):\n",
    "                img = item\n",
    "            elif isinstance(item, torch.Tensor):\n",
    "                 # Assume tensor is already processed, just move to device\n",
    "                 # Warning: This assumes the input tensor matches model expectations!\n",
    "                 processed_batch.append(item.to(device)) \n",
    "                 continue # Skip transform for tensors\n",
    "            else:\n",
    "                logger.warning(f\"Unsupported image input type: {type(item)}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            # Apply transforms if not already a tensor\n",
    "            try:\n",
    "                processed_img = transform(img)\n",
    "                processed_batch.append(processed_img)\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Failed to transform image: {item}. Error: {e}\")\n",
    "\n",
    "        if not processed_batch:\n",
    "            continue # Skip if batch is empty after errors\n",
    "            \n",
    "        # Stack processed images into a batch tensor\n",
    "        batch_tensor = torch.stack(processed_batch).to(device)\n",
    "        \n",
    "        # Extract features\n",
    "        features = model.encode_image(batch_tensor)\n",
    "        all_features.append(features.cpu()) # Move features to CPU to accumulate\n",
    "\n",
    "    if not all_features:\n",
    "        logger.error(\"No image features could be extracted.\")\n",
    "        # Return an empty tensor with the correct embedding dimension\n",
    "        embed_dim = model.visual_projection.out_features # Get embed_dim from model\n",
    "        return torch.empty((0, embed_dim))\n",
    "        \n",
    "    return torch.cat(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f755f4-8e32-411d-b92b-84c89639f16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def extract_text_features(\n",
    "    model: IndicCLIP,\n",
    "    tokenizer: IndicBERTTokenizer,\n",
    "    text_input: Union[str, List[str]],\n",
    "    batch_size: int = 32,\n",
    "    device: Optional[Union[str, torch.device]] = None\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extracts normalized features for one or more text strings using the IndicCLIP model.\n",
    "\n",
    "    Args:\n",
    "        model (IndicCLIP): The loaded IndicCLIP model instance.\n",
    "        tokenizer (IndicBERTTokenizer): The tokenizer instance compatible with the model.\n",
    "        text_input (Union[str, List[str]]): A single text string or a list of strings.\n",
    "        batch_size (int): Batch size for processing multiple texts.\n",
    "        device (Optional[Union[str, torch.device]]): Device to use for inference. If None, uses model's device.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the normalized text features (N, embed_dim).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = next(model.parameters()).device\n",
    "    else:\n",
    "        model.to(device)\n",
    "\n",
    "    if isinstance(text_input, str):\n",
    "        text_input = [text_input]\n",
    "\n",
    "    all_features = []\n",
    "    for i in range(0, len(text_input), batch_size):\n",
    "        batch_texts = text_input[i:i+batch_size]\n",
    "        \n",
    "        try:\n",
    "            # Tokenize the batch\n",
    "            inputs = tokenizer.tokenize(batch_texts)\n",
    "            input_ids = inputs['input_ids'].to(device)\n",
    "            attention_mask = inputs['attention_mask'].to(device)\n",
    "            \n",
    "            # Extract features\n",
    "            features = model.encode_text(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            all_features.append(features.cpu()) # Move features to CPU\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing text batch starting with '{batch_texts[0][:50]}...': {e}\")\n",
    "            continue # Skip batch on error\n",
    "\n",
    "    if not all_features:\n",
    "        logger.error(\"No text features could be extracted.\")\n",
    "        embed_dim = model.text_projection.out_features # Get embed_dim from model\n",
    "        return torch.empty((0, embed_dim))\n",
    "\n",
    "    return torch.cat(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d6628-d103-4832-9585-f127e170a5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def compute_similarity(\n",
    "    model: IndicCLIP,\n",
    "    image_features: torch.Tensor,\n",
    "    text_features: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the similarity matrix between image and text features using the model's logit scale.\n",
    "\n",
    "    Args:\n",
    "        model (IndicCLIP): The loaded IndicCLIP model instance (used for logit_scale).\n",
    "        image_features (torch.Tensor): Normalized image features (N, embed_dim).\n",
    "        text_features (torch.Tensor): Normalized text features (M, embed_dim).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A similarity matrix of shape (N, M).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device # Use model's device\n",
    "    \n",
    "    # Ensure features are on the correct device\n",
    "    image_features = image_features.to(device)\n",
    "    text_features = text_features.to(device)\n",
    "    \n",
    "    # Get the exponentiated logit scale from the model\n",
    "    logit_scale = model.logit_scale.exp()\n",
    "    \n",
    "    # Compute similarity (dot product of normalized features, scaled by temperature)\n",
    "    # Features should already be normalized by the extraction functions\n",
    "    similarity = logit_scale * image_features @ text_features.t()\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6b30d5-a10a-4218-8745-f3a55f3615e2",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4840e138-0604-42fe-b616-f468e97c400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false \n",
    "# --- Example: Load Model and Run Inference ---\n",
    "if __name__ == '__main__':\n",
    "    print(\"--- Running Inference API Example ---\")\n",
    "    \n",
    "    # --- Configuration ---\n",
    "    # !! IMPORTANT: Replace with the *actual* path to your trained checkpoint !!\n",
    "    # checkpoint_name = 'best_recall_interactive.pth' # From 10_training example\n",
    "    checkpoint_name = 'epoch_0.pth' # Default name if SaveModelCallback wasn't configured fully\n",
    "    checkpoint_file = CHECKPOINT_PATH / checkpoint_name\n",
    "    \n",
    "    # !! IMPORTANT: Provide the config used during training if it wasn't saved with the checkpoint !!\n",
    "    #             (If Learner.save was used, config is often not saved)\n",
    "    # Example: Use the smaller ResNet18 config from the 10_training notebook test\n",
    "    model_configuration = {\n",
    "        'embed_dim': 512, # Example: ResNet18 from test\n",
    "        'vision_model_name': 'resnet18', # Example: ResNet18 from test\n",
    "        'vision_pretrained': False, # Doesn't matter for loading state_dict\n",
    "        'text_model_name': PRETRAINED_TOKENIZER_NAME,\n",
    "        'text_pretrained': False,\n",
    "        'tokenizer': IndicBERTTokenizer.load_tokenizer(TOKENIZER_PATH) # Load the tokenizer used for training\n",
    "    }\n",
    "    \n",
    "    # Create dummy checkpoint if it doesn't exist for demonstration\n",
    "    if not checkpoint_file.exists():\n",
    "        print(f\"Warning: Checkpoint {checkpoint_file} not found. Creating a dummy model and saving it.\")\n",
    "        ensure_dir(checkpoint_file.parent)\n",
    "        dummy_model = IndicCLIP(**model_configuration) \n",
    "        torch.save(dummy_model.state_dict(), checkpoint_file)\n",
    "        print(\"Dummy checkpoint created.\")\n",
    "\n",
    "    # --- Load Model and Tokenizer ---\n",
    "    try:\n",
    "        loaded_model = load_indic_clip_model(checkpoint_file, model_config=model_configuration)\n",
    "        loaded_tokenizer = IndicBERTTokenizer.load_tokenizer(TOKENIZER_PATH)\n",
    "        print(\"Model and tokenizer loaded successfully.\")\n",
    "\n",
    "        # --- Prepare Sample Data ---\n",
    "        # Use a real image path if available, otherwise create dummy\n",
    "        sample_image_path = PROJECT_ROOT / 'sample_image.jpg' # Create this image or use a real one\n",
    "        if not sample_image_path.exists():\n",
    "             print(f\"Creating dummy image at: {sample_image_path}\")\n",
    "             dummy_pil_img = Image.new('RGB', (250, 300), color='cyan')\n",
    "             dummy_pil_img.save(sample_image_path)\n",
    "\n",
    "        sample_texts = [\n",
    "            \"एक बिल्ली सोफे पर सो रही है।\", # A cat sleeping on a sofa\n",
    "            \"समुद्र तट पर सूर्यास्त।\",    # Sunset on a beach\n",
    "            \"साड़ी पहने एक महिला।\"       # A woman wearing a saree\n",
    "        ]\n",
    "\n",
    "        # --- Extract Features ---\n",
    "        print(\"\\nExtracting image features...\")\n",
    "        image_features = extract_image_features(loaded_model, sample_image_path)\n",
    "        print(f\"Image features shape: {image_features.shape}\")\n",
    "\n",
    "        print(\"\\nExtracting text features...\")\n",
    "        text_features = extract_text_features(loaded_model, loaded_tokenizer, sample_texts)\n",
    "        print(f\"Text features shape: {text_features.shape}\")\n",
    "\n",
    "        # --- Compute Similarity ---\n",
    "        print(\"\\nComputing similarity...\")\n",
    "        similarity_matrix = compute_similarity(loaded_model, image_features, text_features)\n",
    "        print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "        print(\"Similarity scores:\")\n",
    "        print(similarity_matrix.cpu().numpy())\n",
    "\n",
    "        # Find best matching text for the image\n",
    "        best_match_idx = similarity_matrix.argmax().item()\n",
    "        print(f\"\\nBest matching text for the image: '{sample_texts[best_match_idx]}' (Score: {similarity_matrix[0, best_match_idx]:.4f})\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        print(\"Please ensure the correct checkpoint and tokenizer paths are set.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    # Optional: Clean up dummy image\n",
    "    # if 'dummy_pil_img' in locals() and sample_image_path.exists():\n",
    "    #    os.remove(sample_image_path)\n",
    "    #    print(f\"Cleaned up dummy image: {sample_image_path}\")\n",
    "\n",
    "    print(\"\\n--- Inference API Example Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7130b1-4224-4309-b506-84423bd25392",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # Run this in terminal to export"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}