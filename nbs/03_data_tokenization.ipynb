{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "> Load and configure the pre-trained tokenizer (e.g., ai4bharat/indic-bert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Google Drive mounted successfully.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Mount Google Drive (Optional, but recommended for persistent storage)\n",
    "from pathlib import Path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, skipping Drive mount.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting Google Drive: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indic_clip.core not found initially.\n",
      "Added /content/drive/MyDrive/Indic-Clip to sys.path\n",
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive detected, setting PROJECT_ROOT to /content/drive/MyDrive/Indic-Clip\n",
      "Ensure your project files are located there.\n",
      "Imported indic_clip.core after path adjustment.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    print(\"Reloaded indic_clip.core\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"indic_clip.core not found initially.\")\n",
    "    # Attempt to set sys.path if running in Colab and project cloned\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        project_parent = '/content' # Assuming cloned into /content/indic-clip\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "             project_parent = '/content/drive/MyDrive/Indic-Clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive\")\n",
    "            # raise # Stop execution if core components missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Install necessary libraries if in Colab\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install -q nbdev transformers torch sentencepiece # Sentencepiece might be a dependency for some tokenizers\n",
    "    # Ensure indic_clip core is accessible (adjust path if needed)\n",
    "    # !pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import logging\n",
    "from transformers import AutoTokenizer\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from indic_clip.core import (\n",
    "        PRETRAINED_TOKENIZER_NAME,\n",
    "        TOKENIZER_PATH,\n",
    "        CUSTOM_SPECIAL_TOKENS,\n",
    "        CLS_TOKEN, SEP_TOKEN, PAD_TOKEN, UNK_TOKEN, # Import standard tokens for reference\n",
    "        get_logger,\n",
    "        setup_logging,\n",
    "        ensure_dir\n",
    "    )\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Error importing from indic_clip.core. Using Fallbacks.\")\n",
    "    # Fallbacks if core isn't importable (e.g., interactive testing)\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "    logger = logging.getLogger(__name__)\n",
    "    PRETRAINED_TOKENIZER_NAME = \"ai4bharat/indic-bert\"\n",
    "    TOKENIZER_PATH = Path('./models/tokenizer')\n",
    "    CUSTOM_SPECIAL_TOKENS = [\"<Sa>\", \"<Hi>\"]\n",
    "    CLS_TOKEN, SEP_TOKEN, PAD_TOKEN, UNK_TOKEN = \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[UNK]\"\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "    def setup_logging(): pass\n",
    "    def ensure_dir(path: Path): path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IndicBERTTokenizer:\n",
    "    \"\"\"Wrapper for the Hugging Face tokenizer (ai4bharat/indic-bert).\"\"\"\n",
    "    def __init__(self, tokenizer_name: str = PRETRAINED_TOKENIZER_NAME, max_length: int = 77):\n",
    "        \"\"\"\n",
    "        Initializes the wrapper by loading the pre-trained tokenizer.\n",
    "\n",
    "        Args:\n",
    "            tokenizer_name (str): The name of the Hugging Face tokenizer model.\n",
    "            max_length (int): The maximum sequence length for padding/truncation.\n",
    "        \"\"\"\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.max_length = max_length\n",
    "        try:\n",
    "            self.hf_tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "            logger.info(f\"Successfully loaded tokenizer: {tokenizer_name}\")\n",
    "            self._add_custom_special_tokens()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load tokenizer '{tokenizer_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def _add_custom_special_tokens(self):\n",
    "        \"\"\"Adds custom special tokens if they don't already exist.\"\"\"\n",
    "        if not CUSTOM_SPECIAL_TOKENS:\n",
    "            return\n",
    "\n",
    "        num_added_toks = self.hf_tokenizer.add_special_tokens({'additional_special_tokens': CUSTOM_SPECIAL_TOKENS})\n",
    "        if num_added_toks > 0:\n",
    "            logger.info(f\"Added {num_added_toks} custom special tokens: {CUSTOM_SPECIAL_TOKENS}\")\n",
    "            # Note: The text encoder's embedding layer needs to be resized after adding tokens\n",
    "            # model.resize_token_embeddings(len(tokenizer.hf_tokenizer))\n",
    "            logger.warning(\"Remember to resize model token embeddings if new tokens were added!\")\n",
    "        else:\n",
    "            logger.info(\"Custom special tokens already exist or none were specified.\")\n",
    "\n",
    "    def tokenize(self, texts: list[str] | str) -> dict:\n",
    "        \"\"\"Tokenizes a single text or a batch of texts.\n",
    "\n",
    "        Args:\n",
    "            texts (list[str] | str): The text or list of texts to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing 'input_ids' and 'attention_mask' tensors.\n",
    "        \"\"\"\n",
    "        # Handle single string input\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        # Tokenize using the Hugging Face tokenizer\n",
    "        # It handles adding CLS/SEP based on its configuration\n",
    "        # padding='max_length' ensures all sequences have the same length\n",
    "        # truncation=True cuts sequences longer than max_length\n",
    "        # return_tensors='pt' returns PyTorch tensors\n",
    "        try:\n",
    "            batch_encodings = self.hf_tokenizer(\n",
    "                texts,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.max_length,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            return batch_encodings\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during tokenization: {e}\")\n",
    "            # Return empty tensors or raise error based on desired handling\n",
    "            return {'input_ids': torch.tensor([]), 'attention_mask': torch.tensor([])}\n",
    "\n",
    "    def decode(self, token_ids: torch.Tensor | list[int]) -> list[str] | str:\n",
    "        \"\"\"Decodes token IDs back into text.\n",
    "\n",
    "        Args:\n",
    "            token_ids (torch.Tensor | list[int]): A tensor or list of token IDs.\n",
    "                                                  Can be a single sequence or a batch.\n",
    "\n",
    "        Returns:\n",
    "            list[str] | str: The decoded text(s).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Handle tensor input\n",
    "            if isinstance(token_ids, torch.Tensor):\n",
    "                 # If it's a batch, decode each sequence\n",
    "                if token_ids.ndim > 1:\n",
    "                     return self.hf_tokenizer.batch_decode(token_ids, skip_special_tokens=True)\n",
    "                else:\n",
    "                     return self.hf_tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "            # Handle list input (assume single sequence)\n",
    "            elif isinstance(token_ids, list):\n",
    "                 return self.hf_tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "            else:\n",
    "                 logger.error(f\"Unsupported input type for decode: {type(token_ids)}\")\n",
    "                 return \"\"\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during decoding: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Returns the size of the tokenizer's vocabulary.\"\"\"\n",
    "        return len(self.hf_tokenizer)\n",
    "\n",
    "    @property\n",
    "    def special_tokens(self) -> dict:\n",
    "        \"\"\"Returns a dictionary of special tokens used by the tokenizer.\"\"\"\n",
    "        return self.hf_tokenizer.special_tokens_map\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self) -> int | None:\n",
    "        \"\"\"Returns the ID used for padding.\"\"\"\n",
    "        return self.hf_tokenizer.pad_token_id\n",
    "\n",
    "    def save_tokenizer(self, save_directory: Path = TOKENIZER_PATH):\n",
    "        \"\"\"Saves the tokenizer's state (including vocab and added tokens) to a directory.\"\"\"\n",
    "        try:\n",
    "            ensure_dir(save_directory)\n",
    "            self.hf_tokenizer.save_pretrained(str(save_directory))\n",
    "            logger.info(f\"Tokenizer state saved to {save_directory}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to save tokenizer state to {save_directory}: {e}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_tokenizer(cls, load_directory: Path = TOKENIZER_PATH, max_length: int = 77):\n",
    "        \"\"\"Loads tokenizer state from a directory.\"\"\"\n",
    "        if not load_directory.exists():\n",
    "             logger.error(f\"Cannot load tokenizer. Directory not found: {load_directory}\")\n",
    "             raise FileNotFoundError(f\"Tokenizer directory not found: {load_directory}\")\n",
    "        try:\n",
    "            # Instantiate the class, which loads the tokenizer from the specified dir\n",
    "            instance = cls(tokenizer_name=str(load_directory), max_length=max_length)\n",
    "            logger.info(f\"Tokenizer state loaded successfully from {load_directory}\")\n",
    "            return instance\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load tokenizer state from {load_directory}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "if __name__ == '__main__':\n",
    "    print(f\"Loading tokenizer: {PRETRAINED_TOKENIZER_NAME}\")\n",
    "    try:\n",
    "        tokenizer = IndicBERTTokenizer(max_length=64)\n",
    "\n",
    "        print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")\n",
    "        print(f\"Special tokens map: {tokenizer.special_tokens}\")\n",
    "        print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n",
    "        print(f\"Custom Special Tokens potentially added: {CUSTOM_SPECIAL_TOKENS}\")\n",
    "\n",
    "        # Example tokenization\n",
    "        hindi_text = \"गुलाबी पोशाक में बच्चा प्रवेश के रास्ते में सीढ़ियों के सेट पर चढ़ रहा है।\"\n",
    "        sanskrit_text = \"कृष्णं वन्दे जगद्गुरुम्।\" # Example\n",
    "        texts_to_tokenize = [hindi_text, sanskrit_text, \"A short sentence.\"]\n",
    "\n",
    "        print(f\"\\nTokenizing example texts (max_length={tokenizer.max_length}):\")\n",
    "        batch_encoding = tokenizer.tokenize(texts_to_tokenize)\n",
    "\n",
    "        print(\"Input IDs:\")\n",
    "        print(batch_encoding['input_ids'])\n",
    "        print(\"\\nAttention Mask:\")\n",
    "        print(batch_encoding['attention_mask'])\n",
    "\n",
    "        # Example decoding\n",
    "        print(\"\\nDecoding the first sequence:\")\n",
    "        decoded_text = tokenizer.decode(batch_encoding['input_ids'][0])\n",
    "        print(f\"Decoded: '{decoded_text}'\")\n",
    "\n",
    "        print(\"\\nDecoding the batch:\")\n",
    "        decoded_batch = tokenizer.decode(batch_encoding['input_ids'])\n",
    "        print(decoded_batch)\n",
    "\n",
    "        # Example save/load\n",
    "        print(f\"\\nSaving tokenizer state to: {TOKENIZER_PATH}\")\n",
    "        tokenizer.save_tokenizer()\n",
    "\n",
    "        print(\"\\nLoading tokenizer state from save directory...\")\n",
    "        loaded_tokenizer = IndicBERTTokenizer.load_tokenizer(max_length=64)\n",
    "        print(f\"Loaded tokenizer vocab size: {loaded_tokenizer.vocab_size}\")\n",
    "        # Verify consistency\n",
    "        assert tokenizer.vocab_size == loaded_tokenizer.vocab_size\n",
    "        re_encoded = loaded_tokenizer.tokenize(texts_to_tokenize)\n",
    "        assert torch.equal(batch_encoding['input_ids'], re_encoded['input_ids'])\n",
    "        print(\"Save/Load test passed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in the example usage: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Indic-Clip\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%cd /content/drive/MyDrive/Indic-Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # Run this in terminal to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
