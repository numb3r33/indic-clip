{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a3c13a-70b7-416a-a258-015020b31972",
   "metadata": {},
   "source": [
    "# Initial Evaluation Run\n",
    "\n",
    "> Load the trained Indic-CLIP model and evaluate its performance, focusing on cross-modal retrieval metrics on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ed9117-6c15-4f1c-b3d8-07a4f4043b5d",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368325f-a709-46d2-9149-f7cf8d365d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Add project root to sys.path to allow importing project modules\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Determine project root based on environment (colab vs local)\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    if not Path('/content/drive').exists(): drive.mount('/content/drive')\n",
    "    if Path('/content/drive/MyDrive').exists():\n",
    "       project_parent = '/content/drive/MyDrive/Indic-Clip' # Adjust if your path differs\n",
    "       print(f\"Assuming Project Directory in Google Drive: {project_parent}\")\n",
    "    else:\n",
    "        project_parent = '/content/indic-clip' # Adjust if cloned to /content\n",
    "        print(f\"Assuming Project Directory in Colab /content: {project_parent}\")\n",
    "else:\n",
    "    # Try to find the project root assuming script is in 'nbs' or similar\n",
    "    current_path = Path.cwd()\n",
    "    if current_path.name == 'nbs':\n",
    "         project_parent = str(current_path.parent)\n",
    "    else:\n",
    "         # Assume current dir is project root if not in 'nbs'\n",
    "         project_parent = str(current_path) \n",
    "    print(f\"Assuming Project Directory (Local): {project_parent}\")\n",
    "\n",
    "project_path = Path(project_parent)\n",
    "if project_path.exists() and str(project_path) not in sys.path:\n",
    "    sys.path.insert(0, str(project_path))\n",
    "    print(f\"Added {project_path} to sys.path\")\n",
    "    # Change working directory to project root for consistency\n",
    "    try:\n",
    "        os.chdir(project_path)\n",
    "        print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not change directory to {project_path}. Error: {e}\")\n",
    "else:\n",
    "    print(f\"Project path {project_path} not found or already in sys.path.\")\n",
    "\n",
    "# Verify import after path adjustment\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    print(\"Imported indic_clip.core successfully.\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure and path are correct.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d24a08-1e27-487a-a38c-1399f8e285c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Install requirements if needed (e.g., in Colab)\n",
    "# !pip install -qr requirements.txt\n",
    "# !pip install scikit-learn # For accuracy metrics if used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a273cf-d8c9-4388-b1b2-56ac11c9e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Project specific imports\n",
    "from indic_clip.core import (\n",
    "    get_logger, setup_logging, CHECKPOINT_PATH, TOKENIZER_PATH,\n",
    "    PROCESSED_DATA_PATH, DEFAULT_IMAGE_SIZE, DEFAULT_EMBED_DIM, PRETRAINED_TOKENIZER_NAME\n",
    ")\n",
    "from indic_clip.data.creation import IndicCLIPDataBlock, get_indic_clip_items\n",
    "from indic_clip.data.tokenization import IndicBERTTokenizer\n",
    "from indic_clip.model.clip import IndicCLIP # Needed for type hints in load_indic_clip_model\n",
    "from indic_clip.inference import load_indic_clip_model #, extract_image_features, extract_text_features, compute_similarity\n",
    "from indic_clip.evaluation.metrics import calculate_retrieval_metrics #, calculate_zeroshot_accuracy\n",
    "# from indic_clip.evaluation.benchmarks import load_benchmark_data, create_zeroshot_dataloader, DEFAULT_PROMPT_TEMPLATES_HI, DEFAULT_ZS_CATEGORIES_HI # For ZS evaluation\n",
    "\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d521406-d3a4-4481-98f2-0ef89721c58a",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b06143-d241-4e2b-8239-47a294bd03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation Configuration ---\n",
    "checkpoint_name = 'best_valid_loss.pth' # <<< Name of the checkpoint file to evaluate (e.g., from training)\n",
    "model_vision_backbone = 'resnet50'      # <<< Vision backbone used during training\n",
    "model_text_backbone = PRETRAINED_TOKENIZER_NAME # <<< Text backbone used during training\n",
    "model_embed_dim = 512                     # <<< Embedding dimension used during training\n",
    "\n",
    "# Data configuration (should match training validation split)\n",
    "processed_data_path = PROCESSED_DATA_PATH / 'filtered_data.jsonl'\n",
    "tokenizer_path = TOKENIZER_PATH\n",
    "img_size = DEFAULT_IMAGE_SIZE\n",
    "max_seq_len = 128\n",
    "valid_pct = 0.25 # <<< IMPORTANT: Must match the valid_pct used during training!\n",
    "seed = 42      # <<< IMPORTANT: Must match the seed used during training!\n",
    "batch_size = 64 # Batch size for evaluation inference (adjust based on GPU memory)\n",
    "num_workers = 4\n",
    "\n",
    "# Retrieval metric config\n",
    "k_values = [1, 5, 10]\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Construct full checkpoint path\n",
    "checkpoint_file = CHECKPOINT_PATH / checkpoint_name\n",
    "logger.info(f\"Attempting to load checkpoint: {checkpoint_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21969e4-81e2-4d88-8c82-131517f9ddac",
   "metadata": {},
   "source": [
    "## Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9882d0-3569-4e41-b54e-ac88a67ff09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = None\n",
    "tokenizer = None\n",
    "try:\n",
    "    # Load tokenizer first (needed for model config)\n",
    "    tokenizer = IndicBERTTokenizer.load_tokenizer(tokenizer_path, max_length=max_seq_len)\n",
    "    logger.info(f\"Tokenizer loaded from {tokenizer_path}\")\n",
    "\n",
    "    # Prepare model configuration (required by load_indic_clip_model)\n",
    "    model_config = {\n",
    "        'embed_dim': model_embed_dim,\n",
    "        'vision_model_name': model_vision_backbone,\n",
    "        'vision_pretrained': False, # Pretrained flag doesn't affect loading state dict\n",
    "        'text_model_name': model_text_backbone,\n",
    "        'text_pretrained': False,\n",
    "        'tokenizer': tokenizer # Pass the loaded tokenizer instance\n",
    "    }\n",
    "\n",
    "    # Load the model\n",
    "    model = load_indic_clip_model(\n",
    "        checkpoint_path=checkpoint_file,\n",
    "        model_config=model_config,\n",
    "        device=device\n",
    "    )\n",
    "    logger.info(\"Model loaded successfully.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.error(f\"Error: {e}. Please ensure the checkpoint file exists and the path is correct.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An unexpected error occurred loading model/tokenizer: {e}\", exc_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e5611e-9e08-435a-a9b1-a95c9003e5de",
   "metadata": {},
   "source": [
    "## Prepare Validation DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883105b-6531-493f-826d-0f21df1a8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = None\n",
    "if model and tokenizer: # Proceed only if model and tokenizer loaded successfully\n",
    "    try:\n",
    "        logger.info(\"Loading data items for validation set...\")\n",
    "        items_df = get_indic_clip_items(data_path=processed_data_path)\n",
    "\n",
    "        if not items_df.empty:\n",
    "            logger.info(\"Instantiating DataBlock for validation...\")\n",
    "            # Use augmentations=False for validation\n",
    "            eval_dblock = IndicCLIPDataBlock(\n",
    "                tokenizer_name_or_path=model_text_backbone,\n",
    "                tokenizer_save_path=tokenizer_path,\n",
    "                max_length=max_seq_len,\n",
    "                img_size=img_size,\n",
    "                valid_pct=valid_pct, # Use the same split as training\n",
    "                seed=seed,           # Use the same seed as training\n",
    "                batch_size=batch_size,\n",
    "                num_workers=num_workers,\n",
    "                use_augmentations=False # NO augmentations for validation\n",
    "            )\n",
    "\n",
    "            logger.info(\"Creating DataLoaders to get validation set...\")\n",
    "            dls = eval_dblock.get_dataloaders(items_df, shuffle_train=False) # Shuffle=False for deterministic validation set\n",
    "            valid_dl = dls.valid # Extract the validation dataloader\n",
    "            logger.info(f\"Validation DataLoader created with {len(valid_dl.dataset)} items and {len(valid_dl)} batches.\")\n",
    "        else:\n",
    "            logger.error(\"Failed to load items dataframe. Cannot create DataLoader.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred preparing the DataLoader: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.warning(\"Model or Tokenizer not loaded. Skipping DataLoader creation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e30d9a6-4f2d-45e5-8611-35d0f3c2361a",
   "metadata": {},
   "source": [
    "## Extract Features from Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26868c09-5b38-4250-b38d-781434efc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_features = []\n",
    "all_text_features = []\n",
    "val_image_features = None\n",
    "val_text_features = None\n",
    "\n",
    "if model and valid_dl:\n",
    "    logger.info(\"Extracting features from validation set...\")\n",
    "    model.eval() # Ensure model is in eval mode\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dl, desc=\"Extracting Features\"):\n",
    "            # Batch structure from IndicCLIPDataBlock: (img_tensor, (text_ids, text_mask)), _dummy_cat_\n",
    "            img_batch, txt_tuple_batch, _ = batch\n",
    "            img_batch = img_batch.to(device)\n",
    "            txt_ids_batch = txt_tuple_batch[0].to(device)\n",
    "            txt_mask_batch = txt_tuple_batch[1].to(device)\n",
    "\n",
    "            try:\n",
    "                # Use model's specific encoding methods\n",
    "                img_feat = model.encode_image(img_batch)\n",
    "                txt_feat = model.encode_text(txt_ids_batch, txt_mask_batch)\n",
    "\n",
    "                all_image_features.append(img_feat.cpu()) # Store on CPU\n",
    "                all_text_features.append(txt_feat.cpu())\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error extracting features for a batch: {e}\", exc_info=True)\n",
    "                # Decide whether to skip batch or halt\n",
    "\n",
    "    if all_image_features and all_text_features:\n",
    "        # Concatenate all features into single tensors\n",
    "        val_image_features = torch.cat(all_image_features)\n",
    "        val_text_features = torch.cat(all_text_features)\n",
    "        logger.info(f\"Feature extraction complete. Image features shape: {val_image_features.shape}, Text features shape: {val_text_features.shape}\")\n",
    "    else:\n",
    "        logger.error(\"Feature extraction failed or resulted in empty lists.\")\n",
    "else:\n",
    "    logger.warning(\"Model or DataLoader not available. Skipping feature extraction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5355f44b-b942-4a27-8543-a0f139b2431b",
   "metadata": {},
   "source": [
    "## Calculate and Report Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb1a702-1bd4-4e4b-9645-3f6701f41979",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_results = None\n",
    "if val_image_features is not None and val_text_features is not None and model is not None:\n",
    "    logger.info(\"Calculating retrieval metrics...\")\n",
    "    try:\n",
    "        # Get the logit scale from the model\n",
    "        with torch.no_grad():\n",
    "            logit_scale = model.logit_scale.exp().cpu() # Use the trained logit scale\n",
    "\n",
    "        # Move features to the correct device for calculation\n",
    "        val_image_features = val_image_features.to(device)\n",
    "        val_text_features = val_text_features.to(device)\n",
    "\n",
    "        retrieval_results = calculate_retrieval_metrics(\n",
    "            image_features=val_image_features,\n",
    "            text_features=val_text_features,\n",
    "            logit_scale=logit_scale.to(device),\n",
    "            k_values=k_values\n",
    "        )\n",
    "\n",
    "        print(\"\\n--- Retrieval Metrics (Validation Set) ---\")\n",
    "        if retrieval_results:\n",
    "            for key, value in retrieval_results.items():\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "        else:\n",
    "            print(\"  No retrieval results calculated.\")\n",
    "        print(\"------------------------------------------\")\n",
    "        # Suggest saving to Results.md\n",
    "        print(\"\\nConsider adding these results to Results.md\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating retrieval metrics: {e}\", exc_info=True)\n",
    "\n",
    "else:\n",
    "    logger.warning(\"Features or model not available. Skipping retrieval metric calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80338673-0935-45c7-88e0-7e39280d3c59",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification (Placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e354d8-bd30-4274-9717-84f0227ccb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Zero-Shot Evaluation (Example/Placeholder) ---\n",
    "# This requires benchmark data to be prepared in the format expected by\n",
    "# load_benchmark_data and create_zeroshot_dataloader (from 12_evaluation_benchmarks.ipynb)\n",
    "\n",
    "# benchmark_name_zs = 'flickr30k_hi' # Or another benchmark name\n",
    "# df_zs = load_benchmark_data(benchmark_name_zs)\n",
    "\n",
    "# if df_zs is not None and model and tokenizer:\n",
    "#     logger.info(f\"Preparing Zero-Shot DataLoader for benchmark: {benchmark_name_zs}\")\n",
    "#     # Assuming benchmark images are relative to BENCHMARK_DATA_PATH / benchmark_name_zs\n",
    "#     zs_dl = create_zeroshot_dataloader(\n",
    "#         df=df_zs,\n",
    "#         benchmark_base_path=(BENCHMARK_DATA_PATH / benchmark_name_zs),\n",
    "#         # Ensure label_col matches the column with integer class indices in your benchmark CSV/JSONL\n",
    "#         label_col='label_idx', # <<< Adjust if necessary\n",
    "#         batch_size=batch_size,\n",
    "#         num_workers=num_workers\n",
    "#     )\n",
    "#\n",
    "#     if zs_dl:\n",
    "#         logger.info(f\"Extracting image features for Zero-Shot benchmark...\")\n",
    "#         all_zs_image_features = []\n",
    "#         all_zs_labels = []\n",
    "#         with torch.no_grad():\n",
    "#             for batch in tqdm(zs_dl, desc=f\"ZS Features ({benchmark_name_zs})\"):\n",
    "#                 img_batch, lbl_batch = batch\n",
    "#                 img_batch = img_batch.to(device)\n",
    "#                 try:\n",
    "#                     img_feat = model.encode_image(img_batch)\n",
    "#                     all_zs_image_features.append(img_feat.cpu())\n",
    "#                     all_zs_labels.append(lbl_batch.cpu()) # Labels are already on CPU from DataLoader\n",
    "#                 except Exception as e:\n",
    "#                     logger.error(f\"Error extracting ZS features for a batch: {e}\", exc_info=True)\n",
    "#\n",
    "#         if all_zs_image_features:\n",
    "#             zs_image_features = torch.cat(all_zs_image_features).to(device)\n",
    "#             zs_image_labels = torch.cat(all_zs_labels).numpy()\n",
    "#\n",
    "#             # Define class names and templates relevant to the benchmark\n",
    "#             # zs_class_names = # Load appropriate class names for the benchmark\n",
    "#             # zs_templates = DEFAULT_PROMPT_TEMPLATES_HI # Or other relevant templates\n",
    "#\n",
    "#             # logger.info(\"Calculating Zero-Shot accuracy...\")\n",
    "#             # accuracy = calculate_zeroshot_accuracy(\n",
    "#             #     image_features=zs_image_features,\n",
    "#             #     image_labels=zs_image_labels,\n",
    "#             #     class_names=zs_class_names,\n",
    "#             #     templates=zs_templates,\n",
    "#             #     model=model,\n",
    "#             #     tokenizer=tokenizer\n",
    "#             # )\n",
    "#             # print(f\"\\n--- Zero-Shot Accuracy ({benchmark_name_zs}) ---\")\n",
    "#             # print(f\"  Top-1 Accuracy: {accuracy:.4f}\")\n",
    "#             # print(\"-----------------------------------\")\n",
    "#             print(\"\\nZero-Shot evaluation code is placeholder. Uncomment and adapt with actual class names and benchmark data.\")\n",
    "#         else:\n",
    "#              logger.error(f\"Failed to extract features for Zero-Shot benchmark {benchmark_name_zs}.\")\n",
    "#     else:\n",
    "#         logger.warning(f\"Could not create Zero-Shot DataLoader for {benchmark_name_zs}.\")\n",
    "# else:\n",
    "#     logger.warning(f\"Could not load benchmark data for {benchmark_name_zs} or model/tokenizer not available. Skipping Zero-Shot evaluation.\")\n",
    "\n",
    "logger.info(\"Evaluation script finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}