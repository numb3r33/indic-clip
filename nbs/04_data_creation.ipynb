{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FixjomduM51I",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data.creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q-8lUMZkM_nU",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from pathlib import Path\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    print(\"Reloaded indic_clip.core\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"indic_clip.core not found initially.\")\n",
    "    # Attempt to set sys.path if running in Colab and project cloned\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        project_parent = '/content' # Assuming cloned into /content/indic-clip\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "             project_parent = '/content/drive/MyDrive/Indic-Clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive\")\n",
    "            # raise # Stop execution if core components missing\n",
    "    else:\n",
    "        project_parent = '/workspace'\n",
    "        if Path('/workspace/indic-clip').exists():\n",
    "             project_parent = '/workspace/indic-clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /workspace/indic-clip/indic-clip/core.py or similar in Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BP1hyfSfM0WJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# --- Standard Library Imports ---\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Pypi Library Imports ---\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from fastcore.all import *\n",
    "\n",
    "# --- fastai Imports ---\n",
    "from fastai.vision.all import *\n",
    "from fastai.data.all import *\n",
    "\n",
    "# --- Project Imports ---\n",
    "from indic_clip.core import *\n",
    "from indic_clip.data.tokenization import IndicBERTTokenizer\n",
    "\n",
    "PROCESSED_FILTERED_DATA_PATH = DATA_PATH / 'processed' / 'filtered_data.jsonl'\n",
    "MAX_SEQ_LEN = 128\n",
    "IMAGE_SIZE  = 256\n",
    "BATCH_SIZE = DEFAULT_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SiRkLn4GM0WN",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def summary(dblock:DataBlock):\n",
    "    \"Prints a summary of the DataBlock showing blocks, getters, and transforms.\"\n",
    "    print(f\"DataBlock Summary:\")\n",
    "    print(f\"  Blocks: {dblock.blocks}\")\n",
    "    print(f\"  Getters: {dblock.getters}\")\n",
    "    print(f\"  Item Transforms: {dblock.item_tfms}\")\n",
    "    print(f\"  Batch Transforms: {dblock.batch_tfms}\")\n",
    "    print(f\"  Splitter: {dblock.splitter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZVutW4EPM0WN",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def get_indic_clip_items(data_path: Path = PROCESSED_FILTERED_DATA_PATH):\n",
    "    \"\"\"\n",
    "    Loads image-caption pairs from a processed JSONL file.\n",
    "\n",
    "    Args:\n",
    "        data_path (Path): Path to the JSONL file. Each line should be a JSON object\n",
    "                          containing at least 'source', 'image_path_relative', 'caption'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing the loaded data.\n",
    "    \"\"\"\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Processed data file not found at: {data_path}\")\n",
    "\n",
    "    data = []\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "                # Basic validation\n",
    "                if 'source' in item and 'image_path_relative' in item and 'caption' in item:\n",
    "                    data.append(item)\n",
    "                else:\n",
    "                    print(f\"Skipping invalid line: {line.strip()}\") # Log or handle appropriately\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping malformed JSON line: {line.strip()}\") # Log or handle appropriately\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Loaded {len(df)} items from {data_path}\")\n",
    "    # Example: Print first 5 rows and columns for inspection\n",
    "    # print(\"Sample loaded data:\")\n",
    "    # print(df.head())\n",
    "    # print(\"Columns:\", df.columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XfucuZ_eM0WO",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ImageGetter(row):\n",
    "    \"\"\"\n",
    "    Constructs the full path to an image file based on the 'source'\n",
    "    and 'image_path_relative' fields in a DataFrame row.\n",
    "\n",
    "    Args:\n",
    "        row (pandas.Series or dict): A row from the DataFrame containing item info.\n",
    "\n",
    "    Returns:\n",
    "        Path: The absolute path to the image file.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If 'source' or 'image_path_relative' is missing in the row.\n",
    "        ValueError: If the 'source' is not recognized (e.g., not 'flickr8k').\n",
    "    \"\"\"\n",
    "    source = row['source']\n",
    "    relative_path = row['image_path_relative']\n",
    "\n",
    "    if source == 'flickr8k_hindi':\n",
    "        base_path = HINDI_RAW_PATH # Adjust if folder structure changes\n",
    "    # Add elif conditions for other sources (e.g., 'sanskrit_manuscript', 'ecommerce')\n",
    "    # elif source == 'sanskrit_manuscript':\n",
    "    #     base_path = SANSKRIT_RAW_PATH/'manuscripts' # Example path\n",
    "    else:\n",
    "        # Fallback or error for unknown source\n",
    "        # Option 1: Raise an error\n",
    "        raise ValueError(f\"Unknown data source '{source}' encountered in row: {row}\")\n",
    "        # Option 2: Try a default path (less safe)\n",
    "        # base_path = DATA_PATH/'raw'/'unknown_source'\n",
    "        # print(f\"Warning: Unknown source '{source}'. Assuming base path: {base_path}\")\n",
    "\n",
    "    full_path = base_path / relative_path\n",
    "\n",
    "    # Optional: Check if the file exists here, although DataBlock usually handles it\n",
    "    # if not full_path.exists():\n",
    "    #     print(f\"Warning: Image file not found at {full_path} for row: {row}\")\n",
    "    #     # Decide how to handle: skip item later, return None, etc.\n",
    "    #     # Returning the path anyway and letting DataBlock handle it might be cleaner.\n",
    "\n",
    "    return full_path\n",
    "\n",
    "def CaptionGetter(row):\n",
    "    \"\"\"\n",
    "    Extracts the caption from a DataFrame row.\n",
    "\n",
    "    Args:\n",
    "        row (pandas.Series or dict): A row from the DataFrame containing item info.\n",
    "\n",
    "    Returns:\n",
    "        str: The caption text.\n",
    "\n",
    "    Raises:\n",
    "        KeyError: If 'caption' is missing in the row.\n",
    "    \"\"\"\n",
    "    return row['caption']\n",
    "\n",
    "# Placeholder Getters for potential future use (Domain, Language)\n",
    "def DomainGetter(row):\n",
    "    \"\"\" Placeholder for extracting domain label \"\"\"\n",
    "    return row.get('domain', 'unknown') # Default if not present\n",
    "\n",
    "def LanguageGetter(row):\n",
    "    \"\"\" Placeholder for extracting language label \"\"\"\n",
    "    return row.get('language', 'un') # Default if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KfL9HHaDhmZI",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HFTextTransform(Transform):\n",
    "    \"Tokenize strings into a (ids,mask) tuple of Tensors.\"\n",
    "    def __init__(self, hf_tokenizer):\n",
    "        self.hf = hf_tokenizer\n",
    "\n",
    "    def encodes(self, o:str):\n",
    "        out = self.hf.tokenize(o)\n",
    "        ids  = out['input_ids'].squeeze(0)\n",
    "        mask = out['attention_mask'].squeeze(0)\n",
    "        return ids, mask     # <-- pure Tensors only\n",
    "\n",
    "    def decodes(self, o):\n",
    "        # o will be a tuple of two Tensors\n",
    "        ids, _ = o\n",
    "        return self.hf.decode(ids.tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sfSSOV4SM0WO",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class IndicCLIPDataBlock(DataBlock):\n",
    "    \"\"\"\n",
    "    A fast.ai DataBlock specifically designed for Indic-CLIP.\n",
    "    Handles loading images and tokenizing corresponding text captions using\n",
    "    a pre-trained Hugging Face tokenizer.\n",
    "\n",
    "    Assumes input items are structured dicts or pandas rows containing\n",
    "    'source', 'image_path_relative', and 'caption'.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 tokenizer_name_or_path:str = PRETRAINED_TOKENIZER_NAME,\n",
    "                 tokenizer_save_path:Path = TOKENIZER_PATH,\n",
    "                 vocab_size:int = None, # Auto-detected if None\n",
    "                 max_length:int = MAX_SEQ_LEN,\n",
    "                 img_size:int = IMAGE_SIZE,\n",
    "                 valid_pct:float = 0.1,\n",
    "                 seed:int = 42,\n",
    "                 batch_size:int = BATCH_SIZE,\n",
    "                 num_workers:int = 4,\n",
    "                 use_augmentations:bool = True,\n",
    "                 **db_kwargs): # Pass other DataBlock args\n",
    "        \"\"\"\n",
    "        Initializes the IndicCLIPDataBlock.\n",
    "\n",
    "        Args:\n",
    "            tokenizer_name_or_path (str): Name/path of the Hugging Face tokenizer.\n",
    "            tokenizer_save_path (Path): Path to save/load the tokenizer state.\n",
    "            vocab_size (int, optional): Expected vocabulary size. Auto-detected if None.\n",
    "            max_length (int): Maximum sequence length for tokenization.\n",
    "            img_size (int): Size to resize images to (square).\n",
    "            valid_pct (float): Percentage of data to use for validation.\n",
    "            seed (int): Random seed for splitting.\n",
    "            batch_size (int): DataLoader batch size.\n",
    "            num_workers (int): Number of worker processes for DataLoader.\n",
    "            use_augmentations (bool): Whether to apply image augmentations.\n",
    "            custom_tokenizer_cls: The tokenizer wrapper class to use.\n",
    "            **db_kwargs: Additional keyword arguments for the parent DataBlock.\n",
    "        \"\"\"\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = IndicBERTTokenizer.load_tokenizer(tokenizer_save_path, max_length=self.max_length)\n",
    "        if vocab_size is not None and self.tokenizer.vocab_size != vocab_size:\n",
    "             print(f\"Warning: Provided vocab_size ({vocab_size}) does not match tokenizer's vocab_size ({self.tokenizer.vocab_size}). Using tokenizer's value.\")\n",
    "        self.vocab_size = self.tokenizer.vocab_size # Store actual vocab size\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        # Define the text block using TransformBlock and the loaded tokenizer\n",
    "        # The tokenizer __call__ handles padding and truncation directly\n",
    "\n",
    "        text_block = TransformBlock(type_tfms=HFTextTransform(self.tokenizer))\n",
    "\n",
    "        # Define image augmentations\n",
    "        batch_tfms_list = []\n",
    "        if use_augmentations:\n",
    "             # Default aug_transforms includes random resizing, flips, brightness, contrast etc.\n",
    "             # Customize as needed: aug_transforms(mult=1.0, do_flip=True, flip_vert=False, max_rotate=10.0, min_zoom=1.0, max_zoom=1.1, max_lighting=0.2, max_warp=0.2, p_affine=0.75, p_lighting=0.75)\n",
    "            # batch_tfms_list.append(aug_transforms(size=self.img_size))\n",
    "            batch_tfms_list += aug_transforms(size=self.img_size)\n",
    "        # Always add normalization\n",
    "        batch_tfms_list.append(Normalize.from_stats(*imagenet_stats))\n",
    "\n",
    "\n",
    "        # Initialize the DataBlock\n",
    "        # For contrastive learning, y is usually not needed directly from the dataloader\n",
    "        # The pairing is implicit in the batch (image i corresponds to text i)\n",
    "        super().__init__(\n",
    "            blocks=(ImageBlock, text_block, CategoryBlock), # ImageBlock output: Tensor; text_block output: (Tensor, Tensor)\n",
    "            n_inp=2,                          # Tell DataBlock the first 2 blocks are inputs\n",
    "            # get_items=get_indic_clip_items,   # Function to get the source data (e.g., dataframe)\n",
    "            get_x=[ImageGetter,             # Function to get the image path from item\n",
    "                   CaptionGetter],          # Function to get the caption string from item\n",
    "            get_y=lambda r: 0, # Provide a dummy target for every item\n",
    "            splitter=RandomSplitter(valid_pct=valid_pct, seed=seed),\n",
    "            item_tfms=Resize(self.img_size),  # Transform applied to individual items\n",
    "            batch_tfms=batch_tfms_list,       # Transform applied to batches\n",
    "            **db_kwargs\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_dataloaders(self, source_items, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates DataLoaders from the provided source items.\n",
    "\n",
    "        Args:\n",
    "            source_items (list or pd.DataFrame): The list of items (dicts or rows).\n",
    "            **kwargs: Additional arguments passed to dataloaders method.\n",
    "\n",
    "        Returns:\n",
    "            DataLoaders: The fastai DataLoaders object.\n",
    "        \"\"\"\n",
    "        # Set default batch size and num_workers if not overridden\n",
    "        kwargs.setdefault('bs', self.batch_size)\n",
    "        kwargs.setdefault('num_workers', self.num_workers)\n",
    "\n",
    "        print(f\"Creating DataLoaders with bs={kwargs['bs']}, num_workers={kwargs['num_workers']}\")\n",
    "        return self.dataloaders(source_items, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JhGtj8NVtDpJ",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# adjust this path to wherever NotoSansDevanagari-Regular.ttf lives on your system\n",
    "font_path = \"NotoSansDevanagari.ttf\"\n",
    "# Add the font to the font manager\n",
    "fm.fontManager.addfont(font_path)\n",
    "\n",
    "if not fm.findfont(fm.FontProperties(fname=font_path), rebuild_if_missing=False):\n",
    "    raise FileNotFoundError(f\"Could not find Devanagari font at {font_path}\")\n",
    "\n",
    "deva_prop = fm.FontProperties(fname=font_path)\n",
    "\n",
    "def show_batch_with_captions(dls, hf_wrapper, max_n=4):\n",
    "    \"Show a batch of images + decoded captions. `hf_wrapper` is your IndicBERTTokenizer instance.\"\n",
    "    # Correct unpacking for 3 blocks:\n",
    "    img_batch_show, txt_tuple_batch_show, cat_batch_show = dls.one_batch() # Unpack all three\n",
    "\n",
    "    # Use the correct elements\n",
    "    ids, mask = txt_tuple_batch_show # Text tuple is the second element\n",
    "\n",
    "    mean, std = imagenet_stats\n",
    "\n",
    "    bs = img_batch_show.shape[0] # Image batch is the first element\n",
    "    n  = min(bs, max_n)\n",
    "    fig,axes = plt.subplots(1, n, figsize=(n*3, 3))\n",
    "    if n==1: axes = [axes]\n",
    "\n",
    "    for i,ax in enumerate(axes):\n",
    "        # 1) Denormalize & clamp (uses img_batch_show)\n",
    "        img = img_batch_show[i] * torch.tensor(std, device=img_batch_show.device).view(3,1,1) \\\n",
    "                   + torch.tensor(mean, device=img_batch_show.device).view(3,1,1)\n",
    "        img = img.clamp(0,1).permute(1,2,0).cpu().numpy()\n",
    "        ax.imshow(img); ax.axis('off')\n",
    "\n",
    "        # 2) Only keep the real tokens (mask==1) - Uses ids/mask from txt_tuple_batch_show\n",
    "        valid_ids = ids[i][mask[i].bool()].cpu().tolist()\n",
    "\n",
    "        # 3) Decode to text\n",
    "        txt = hf_wrapper.decode(valid_ids)\n",
    "\n",
    "        ax.set_title(txt, fontproperties=deva_prop, wrap=True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YB8GKaJGM0WO",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage & Testing\n",
    "\n",
    "# 1. Load the items\n",
    "try:\n",
    "    items_df = get_indic_clip_items()\n",
    "    print(f\"\\nSuccessfully loaded {len(items_df)} items.\")\n",
    "    print(\"Sample item (first row):\")\n",
    "    print(items_df.iloc[0])\n",
    "\n",
    "    # 2. Instantiate the DataBlock\n",
    "    print(\"\\nInstantiating IndicCLIPDataBlock...\")\n",
    "    indic_clip_dblock = IndicCLIPDataBlock(\n",
    "        # Override defaults if needed\n",
    "        # valid_pct=0.05,\n",
    "        batch_size=4, # Use a small batch size for testing\n",
    "        num_workers=0 if os.name == 'nt' else 2 # Avoid issues on Windows\n",
    "    )\n",
    "    indic_clip_dblock.summary()\n",
    "    print(f\"Using Tokenizer: {indic_clip_dblock.tokenizer.tokenizer_name}\")\n",
    "    print(f\"Tokenizer Vocab Size: {indic_clip_dblock.vocab_size}\")\n",
    "    print(f\"Max Sequence Length: {indic_clip_dblock.max_length}\")\n",
    "    print(f\"Image Size: {indic_clip_dblock.img_size}\")\n",
    "\n",
    "    # 3. Create DataLoaders\n",
    "    print(\"\\nCreating DataLoaders...\")\n",
    "    # Pass the DataFrame directly as source\n",
    "    dls = indic_clip_dblock.get_dataloaders(items_df)\n",
    "    print(\"DataLoaders created successfully.\")\n",
    "\n",
    "    # 4. Inspect a batch\n",
    "    print(\"\\nInspecting one batch...\")\n",
    "    batch = dls.one_batch()\n",
    "    print(f\"Batch type: {type(batch)}\") # tuple\n",
    "    print(f\"Batch length (number of elements): {len(batch)}\") # Should be 3\n",
    "\n",
    "    # --- CORRECTED UNPACKING ---\n",
    "    if len(batch) != 3:\n",
    "         raise ValueError(f\"Expected batch to have 3 elements (Image, TextTuple, Category), but got {len(batch)}\")\n",
    "    img_batch, txt_tuple_batch, cat_batch = batch\n",
    "    # --------------------------\n",
    "\n",
    "    # Reconstruct how Learner sees xb and yb for clarity if needed, but use direct vars below\n",
    "    # xb = (img_batch, txt_tuple_batch)\n",
    "    # yb = cat_batch\n",
    "\n",
    "    print(f\"\\nImage Batch Shape: {img_batch.shape}\")\n",
    "    print(f\"Image Batch dtype: {img_batch.dtype}\")\n",
    "\n",
    "    # The text element from the batch is the tuple (ids, mask)\n",
    "    print(f\"\\nText Tuple Batch Type: {type(txt_tuple_batch)}\")\n",
    "\n",
    "    if isinstance(txt_tuple_batch, tuple): # This should always be true now\n",
    "         print(f\"Text Tuple Batch is a tuple of length: {len(txt_tuple_batch)}\")\n",
    "         print(f\"Text Tuple Batch[0] (input_ids?) Shape: {txt_tuple_batch[0].shape}\")\n",
    "         print(f\"Text Tuple Batch[1] (attn_mask?) Shape: {txt_tuple_batch[1].shape}\")\n",
    "         print(f\"Text Tuple Batch[0] dtype: {txt_tuple_batch[0].dtype}\")\n",
    "    else:\n",
    "         # This case should ideally not happen with the current setup\n",
    "         print(f\"ERROR: Expected text element to be a tuple, but got {type(txt_tuple_batch)}\")\n",
    "         print(f\"Text Element Shape: {txt_tuple_batch.shape}\")\n",
    "         print(f\"Text Element dtype: {txt_tuple_batch.dtype}\")\n",
    "\n",
    "    print(f\"\\nCategory Batch Type: {type(cat_batch)}\")\n",
    "    print(f\"Category Batch Shape: {cat_batch.shape}\") # Expected: [bs]\n",
    "    print(f\"Category Batch dtype: {cat_batch.dtype}\") # Expected: int64\n",
    "    print(f\"Category Batch Values (sample): {cat_batch[:5]}\") # Expected: [0, 0, 0, 0]\n",
    "\n",
    "    # Basic Assertions\n",
    "    assert img_batch.shape[0] == indic_clip_dblock.batch_size\n",
    "    assert img_batch.shape[1] == 3\n",
    "    assert img_batch.shape[2] == indic_clip_dblock.img_size\n",
    "    assert img_batch.shape[3] == indic_clip_dblock.img_size\n",
    "    assert img_batch.dtype == torch.float32\n",
    "\n",
    "    assert isinstance(txt_tuple_batch, tuple) # Verify text element is a tuple\n",
    "    assert txt_tuple_batch[0].shape[0] == indic_clip_dblock.batch_size\n",
    "    assert txt_tuple_batch[0].shape[1] <= indic_clip_dblock.max_length\n",
    "    assert txt_tuple_batch[1].shape[0] == indic_clip_dblock.batch_size\n",
    "    assert txt_tuple_batch[1].shape[1] == txt_tuple_batch[0].shape[1]\n",
    "    assert txt_tuple_batch[0].dtype == torch.int64\n",
    "\n",
    "    assert isinstance(cat_batch, Tensor) # Verify category is a tensor\n",
    "    assert cat_batch.shape == (indic_clip_dblock.batch_size,)\n",
    "    assert cat_batch.dtype == torch.int64\n",
    "\n",
    "    print(\"\\nBatch structure seems correct.\")\n",
    "\n",
    "    # 5. Show a batch (Needs corrected show_batch_with_captions)\n",
    "    print(\"\\nShowing a batch...\")\n",
    "    # Make sure show_batch_with_captions is updated as well (see previous response)\n",
    "    show_batch_with_captions(dls, indic_clip_dblock.tokenizer, max_n=4)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\nError: {e}\")\n",
    "    print(\"Please ensure the filtered data file exists at the expected location:\")\n",
    "    print(f\"Expected location: {PROCESSED_FILTERED_DATA_PATH}\")\n",
    "    print(\"You might need to run the preprocessing notebook (02_*.ipynb) first.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn unexpected error occurred: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VsxXVMipM0WP",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev\n",
    "nbdev.nbdev_export() # Run this in terminal to export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb921d61-1baa-468c-9e20-cc8f9347294f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
