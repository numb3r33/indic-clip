{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L7u7V3L-mB75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default export removed as this is now interactive-only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fV0w71gNmB76",
   "metadata": {},
   "source": [
    "# Training Notebook (Interactive)\n",
    "\n",
    "> Orchestrates the Indic-CLIP model training process using fast.ai's Learner and custom components.\n",
    "> Designed for interactive execution within a notebook environment (e.g., Colab, Jupyter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fT6t2wimB76",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "D-C2pG41mB76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /workspace/indic-clip to sys.path\n",
      "Imported indic_clip.core after path adjustment.\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Mount Google Drive (Optional, but recommended for persistent storage)\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_parent = '/workspace'\n",
    "if Path('/workspace/indic-clip').exists():\n",
    "    project_parent = '/workspace/indic-clip'\n",
    "    if project_parent not in sys.path:\n",
    "        sys.path.insert(0, project_parent)\n",
    "        print(f\"Added {project_parent} to sys.path\")\n",
    "    try:\n",
    "        import indic_clip.core\n",
    "        print(\"Imported indic_clip.core after path adjustment.\")\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "        print(\"Expected: /workspace/indic-clip/indic-clip/core.py or similar in Drive\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "p4z1gM5cmB77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Install requirements if needed (especially in Colab)\n",
    "# !pip install -qr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "x5d17x_XmB77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Imported indic_clip.core\n"
     ]
    }
   ],
   "source": [
    "# --- Standard Library Imports ---\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# --- Pypi Library Imports ---\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "from fastcore.all import *\n",
    "\n",
    "# --- fastai Imports ---\n",
    "from fastai.vision.all import *\n",
    "from fastai.text.all import *\n",
    "from fastai.data.all import *\n",
    "from fastai.callback.wandb import *\n",
    "from fastai.callback.progress import ProgressCallback # Ensure ProgressCallback is imported\n",
    "from fastai.callback.schedule import ParamScheduler # For LR schedule logging\n",
    "from fastai.callback.fp16 import MixedPrecision # For AMP\n",
    "from fastai.callback.training import GradientClip, GradientAccumulation\n",
    "from fastai.callback.tracker import EarlyStoppingCallback, SaveModelCallback\n",
    "\n",
    "# --- Project Imports ---\n",
    "# Use try-except for robustness, especially during development/export\n",
    "try:\n",
    "    from indic_clip.core import * # Imports constants and utils\n",
    "    from indic_clip.data.creation import IndicCLIPDataBlock, get_indic_clip_items\n",
    "    from indic_clip.data.tokenization import IndicBERTTokenizer\n",
    "    from indic_clip.model.clip import IndicCLIP\n",
    "    from indic_clip.loss import ContrastiveLoss\n",
    "    from indic_clip.learner import RetrievalMetricCallback # Custom callback for validation\n",
    "    # Import the actual metric function\n",
    "    from indic_clip.evaluation.metrics import calculate_retrieval_metrics\n",
    "except ModuleNotFoundError as e:\n",
    "    print(f\"Error importing project modules: {e}\")\n",
    "    print(\"Please ensure the project is installed or sys.path is configured correctly.\")\n",
    "    # Define dummy classes/functions if imports fail, allowing script structure to be parsed\n",
    "    class IndicCLIPDataBlock: pass\n",
    "    def get_indic_clip_items(*args, **kwargs): return []\n",
    "    class IndicBERTTokenizer: pass\n",
    "    class IndicCLIP(torch.nn.Module): pass\n",
    "    class ContrastiveLoss(torch.nn.Module): pass\n",
    "    class RetrievalMetricCallback(Callback): pass\n",
    "    def calculate_retrieval_metrics(*args, **kwargs): return {'mean_recall': 0.0, 'i2t_r@1': 0.0, 't2i_r@1': 0.0}\n",
    "\n",
    "# --- Setup Logging ---\n",
    "setup_logging()\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_cell",
   "metadata": {},
   "source": [
    "## Configuration / Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "config_definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple class or use a dictionary to hold configuration\n",
    "class TrainConfig:\n",
    "    # --- Data Arguments ---\n",
    "    processed_data_path: str = str(PROCESSED_DATA_PATH / 'filtered_data.jsonl')\n",
    "    tokenizer_path: str = str(TOKENIZER_PATH)\n",
    "    img_size: int = DEFAULT_IMAGE_SIZE\n",
    "    max_seq_len: int = 128\n",
    "    valid_pct: float = 0.05\n",
    "    num_workers: int = 2 # Lower default for Colab compatibility\n",
    "    use_augmentations: bool = True\n",
    "\n",
    "    # --- Model Arguments ---\n",
    "    vision_model_name: str = 'vit_base_patch16_224' # Vision Transformer Base\n",
    "    text_model_name: str = PRETRAINED_TOKENIZER_NAME # e.g., \"ai4bharat/indic-bert\"\n",
    "    vision_pretrained: bool = True\n",
    "    text_pretrained: bool = True\n",
    "    embed_dim: int = DEFAULT_EMBED_DIM # Should match vision/text models or projection target\n",
    "\n",
    "    # --- Training Arguments ---\n",
    "    epochs: int = 5\n",
    "    batch_size: int = 32 # Adjust based on GPU memory\n",
    "    lr: float = 5e-6 # Learning rate for ViT/BERT might need tuning\n",
    "    wd: float = 0.01 # Weight decay\n",
    "    beta1: float = 0.9\n",
    "    beta2: float = 0.98 # AdamW defaults often work well\n",
    "    eps: float = 1e-6\n",
    "    warmup_steps: int = 1000 # Number of warmup steps for LR scheduler\n",
    "    use_amp: bool = True # Use Automatic Mixed Precision\n",
    "    grad_clip: float | None = None # Gradient clipping value (e.g., 1.0) or None\n",
    "    accum_freq: int = 1 # Gradient accumulation frequency\n",
    "    seed: int = 42\n",
    "\n",
    "    # --- Checkpointing & Resuming ---\n",
    "    checkpoint_dir: str = str(CHECKPOINT_PATH)\n",
    "    save_model_name: str = 'best_recall_interactive' # Model saved by SaveModelCallback\n",
    "    save_epoch_frequency: int = 1 # How often to save checkpoints regardless of metric\n",
    "    resume_from: str | None = None # Path to a .pth checkpoint to resume\n",
    "    early_stopping_patience: int = 5 # Epochs to wait for improvement before stopping\n",
    "\n",
    "    # --- WandB Arguments ---\n",
    "    wandb_project: str = 'Indic-CLIP-Interactive'\n",
    "    wandb_entity: str | None = os.getenv('WANDB_ENTITY') # Read from env var\n",
    "    wandb_run_name: str | None = None # Auto-generate run name if None\n",
    "    wandb_log_model: str = 'best' # Options: 'best', 'all', 'false'\n",
    "\n",
    "    # --- Debugging/Misc ---\n",
    "    max_steps: int | None = None # Limit training steps for debugging\n",
    "\n",
    "# Instantiate the config - MODIFY VALUES HERE FOR YOUR RUN\n",
    "config = TrainConfig()\n",
    "\n",
    "# Example modification for a quick test:\n",
    "# config.epochs = 1\n",
    "# config.batch_size = 16\n",
    "# config.max_steps = 10\n",
    "# config.wandb_entity = \"your_wandb_entity\" # IMPORTANT: SET THIS if not using env var\n",
    "# config.vision_model_name = 'resnet18' # Use smaller model for faster testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2518ff88-30ca-451c-9d1f-49316e5c1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputInspectCallback(Callback):\n",
    "    order = -5 # Run very early, before model prediction\n",
    "    def before_batch(self):\n",
    "        if self.training: # Only log during training for now\n",
    "            logger.critical(f\"InputInspectCB - learn.xb type: {type(self.learn.xb)}\")\n",
    "            if isinstance(self.learn.xb, (list, tuple)):\n",
    "                logger.critical(f\"InputInspectCB - learn.xb length: {len(self.learn.xb)}\")\n",
    "                logger.critical(f\"InputInspectCB - Elem 0 (Image?) type: {type(self.learn.xb[0])}, shape: {self.learn.xb[0].shape}\")\n",
    "                if len(self.learn.xb) > 1:\n",
    "                     logger.critical(f\"InputInspectCB - Elem 1 (TextTuple?) type: {type(self.learn.xb[1])}\")\n",
    "                     if isinstance(self.learn.xb[1], tuple):\n",
    "                         logger.critical(f\"InputInspectCB - TextTuple length: {len(self.learn.xb[1])}\")\n",
    "                         logger.critical(f\"InputInspectCB - TextTuple[0] shape: {self.learn.xb[1][0].shape}\")\n",
    "                         logger.critical(f\"InputInspectCB - TextTuple[1] shape: {self.learn.xb[1][1].shape}\")\n",
    "                     else:\n",
    "                          logger.error(\"!!! InputInspectCB - Elem 1 is NOT a tuple !!!\")\n",
    "                else:\n",
    "                     logger.error(\"!!! InputInspectCB - Only one element in learn.xb !!!\")\n",
    "            else:\n",
    "                 logger.error(f\"!!! InputInspectCB - learn.xb is NOT a tuple/list !!! Type: {type(self.learn.xb)}\")\n",
    "        # Don't CancelBatchException here, just observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84e77310-7483-4962-8725-6246884e6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDebugCallback(Callback):\n",
    "    def after_backward(self):\n",
    "        if not self.training: return # Only check during training steps\n",
    "        grad_norms = []\n",
    "        all_finite = True\n",
    "        for name, param in self.learn.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                if not torch.isfinite(param.grad).all():\n",
    "                    logger.error(f\"!!! Non-finite gradient detected in parameter: {name} !!!\")\n",
    "                    all_finite = False\n",
    "                    # Optional: Log the grad values\n",
    "                    # logger.error(f\"Grad values: {param.grad.detach().cpu().flatten()[:10]}...\") # Log first few values\n",
    "                    # break # Stop checking after first NaN/Inf grad? Or check all?\n",
    "                # grad_norms.append(param.grad.norm().item()) # Collect norms if needed\n",
    "            # else: logger.debug(f\"No grad for param: {name}\") # Can be noisy\n",
    "\n",
    "        if not all_finite:\n",
    "            # Optionally: Zero gradients to prevent optimizer step with NaNs?\n",
    "            # self.learn.opt.zero_grad()\n",
    "            # logger.warning(\"Zeroed gradients due to non-finite values.\")\n",
    "            pass # Just log for now\n",
    "\n",
    "        # Optional: Log norm distribution if desired\n",
    "        # if grad_norms:\n",
    "        #     logger.info(f\"Gradient norms - Min: {min(grad_norms):.4e}, Max: {max(grad_norms):.4e}, Mean: {np.mean(grad_norms):.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b13a807c-a566-4d15-b1a4-1e72fa35c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugRecorderStateCallback(Callback):\n",
    "    order = Recorder.order + 1 # Run after Recorder but potentially before SaveModelCallback\n",
    "\n",
    "    def after_epoch(self):\n",
    "        if hasattr(self.learn, 'recorder'):\n",
    "            rec = self.learn.recorder\n",
    "            epoch = self.learn.epoch\n",
    "            logger.info(f\"--- Debug CB (Epoch {epoch}) BEFORE SaveModel ---\")\n",
    "            logger.info(f\"Recorder metric_names: {getattr(rec, 'metric_names', 'N/A')}\")\n",
    "            logger.info(f\"Recorder log values: {getattr(rec, 'log', 'N/A')}\")\n",
    "            logger.info(f\"Recorder final_record: {getattr(rec, 'final_record', 'N/A')}\")\n",
    "            # Specifically check for valid_loss\n",
    "            val_loss = getattr(rec, 'final_record', None)\n",
    "            if val_loss is not None and len(val_loss) > 2: # Assuming train_loss, valid_loss order\n",
    "                logger.info(f\"Value for 'valid_loss' (via final_record[2]): {val_loss[2]}\")\n",
    "            else:\n",
    "                # Try getting from log dict if final_record isn't available or structured differently\n",
    "                #log_dict = dict(zip(getattr(rec,'metric_names',[]), getattr(rec, 'log', [])))\n",
    "                vals = rec.values[-1]\n",
    "                log_dict = {n: float(v) for n,v in zip(rec.metric_names, vals)}\n",
    "                logger.info(f\"Value for 'valid_loss' (via log dict): {log_dict.get('valid_loss', 'Not Found')}\")\n",
    "\n",
    "            logger.info(f\"--- End Debug CB ---\")\n",
    "        else:\n",
    "            logger.warning(\"Debug CB: Recorder not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b75589f-be54-4edb-9292-07cd39aaaca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleWandbCallback(Callback):\n",
    "    order = Recorder.order + 1 # Run after Recorder to ensure metrics are calculated and logged\n",
    "    remove_on_fetch = True\n",
    "\n",
    "    def __init__(self, log_model_policy='best'):\n",
    "        store_attr()\n",
    "        self._wandb_step = 0\n",
    "        self._wandb_epoch = 0\n",
    "        self.run = None # Initialize run attribute\n",
    "\n",
    "    def _find_recorder_cb(self):\n",
    "        \"Helper to find Recorder callback.\"\n",
    "        if not hasattr(self, 'learn') or not hasattr(self.learn, 'cbs'): return None\n",
    "        for cb in self.learn.cbs:\n",
    "            if isinstance(cb, Recorder): return cb\n",
    "        return None\n",
    "\n",
    "    def _find_save_model_cb(self):\n",
    "        \"Helper to find SaveModelCallback.\"\n",
    "        if not hasattr(self, 'learn') or not hasattr(self.learn, 'cbs'): return None\n",
    "        for cb in self.learn.cbs:\n",
    "            if isinstance(cb, SaveModelCallback): return cb\n",
    "        return None\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Initialize W&B run, log config, watch model, and add model save hook.\"\n",
    "        # Ensure wandb is initialized (it might have been initialized outside)\n",
    "        self.run = wandb.run\n",
    "        if self.run is None:\n",
    "            try:\n",
    "                # Get config from the Learner if available, otherwise log empty\n",
    "                # Use getattr for safety in case learn.train_config doesn't exist\n",
    "                cfg_dict = getattr(self.learn, 'train_config', {})\n",
    "                # If cfg_dict is a dataclass/object, convert to dict if possible\n",
    "                if not isinstance(cfg_dict, dict) and hasattr(cfg_dict, '__dict__'):\n",
    "                    cfg_dict = cfg_dict.__dict__\n",
    "                elif not isinstance(cfg_dict, dict):\n",
    "                     cfg_dict = {} # Fallback to empty dict\n",
    "\n",
    "                wandb_mode = \"online\" if cfg_dict.get('wandb_entity') else \"disabled\"\n",
    "\n",
    "                # Ensure wandb.run is None before potentially re-initializing\n",
    "                # This helps prevent issues in notebooks if cells are re-run\n",
    "                if wandb.run is not None:\n",
    "                     logger.warning(\"Existing wandb run detected. Finishing it before initializing a new one.\")\n",
    "                     wandb.finish()\n",
    "\n",
    "                self.run = wandb.init(project=cfg_dict.get('wandb_project', 'fastai-project'),\n",
    "                                      entity=cfg_dict.get('wandb_entity'),\n",
    "                                      name=cfg_dict.get('wandb_run_name'),\n",
    "                                      config=cfg_dict, # Log the config object used by main()\n",
    "                                      reinit=True, # Allow re-initialization\n",
    "                                      mode=wandb_mode)\n",
    "                logger.info(f\"WandB run initialized: {self.run.name if self.run else 'N/A'} (mode: {wandb.run.mode if self.run else 'N/A'})\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to initialize WandB: {e}. Logging disabled.\")\n",
    "                self.run = None # Ensure self.run is None if init fails\n",
    "\n",
    "        # Reset step/epoch counters\n",
    "        self._wandb_step = 0\n",
    "        self._wandb_epoch = 0\n",
    "\n",
    "        # Watch model if run is active\n",
    "        if self.run and self.run.mode != \"disabled\":\n",
    "            try:\n",
    "                wandb.watch(self.learn.model, log='all', log_freq=max(100, getattr(self.learn.dls,'train_ds',None) or 1000)//self.learn.dls.bs) # Log gradients/params less frequently\n",
    "                logger.info(\"WandB watching model.\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not watch model with WandB: {e}\")\n",
    "\n",
    "        # Add model save hook if policy requires it\n",
    "        if self.log_model_policy != 'false' and self.run and self.run.mode != \"disabled\":\n",
    "            save_model_cb = self._find_save_model_cb()\n",
    "            if save_model_cb:\n",
    "                 # Check for the hook attribute *before* trying to call it\n",
    "                 if hasattr(save_model_cb, 'add_save_hooks'):\n",
    "                     try:\n",
    "                         save_model_cb.add_save_hooks(self._wandb_log_model)\n",
    "                         logger.info(\"Added WandB model logging hook to SaveModelCallback.\")\n",
    "                     except Exception as e:\n",
    "                         logger.error(f\"Unexpected error attaching save hook: {e}\", exc_info=True)\n",
    "                 else:\n",
    "                    # Log the warning about missing hook method\n",
    "                    logger.warning(\n",
    "                      \"SaveModelCallback found, but it does not have the 'add_save_hooks' method; skipping W&B model hook.\"\n",
    "                    )\n",
    "            else:\n",
    "                logger.warning(\n",
    "                  \"SaveModelCallback not found; cannot attach W&B model logging hook.\"\n",
    "                )\n",
    "\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Log training loss and hyperparameters to W&B after each training batch.\"\n",
    "        if self.training and self.run and self.run.mode != \"disabled\":\n",
    "            self._wandb_step += 1\n",
    "            # Correctly log learning rate and other hypers\n",
    "            hypers = {}\n",
    "            if hasattr(self.learn.opt, 'hypers'):\n",
    "                hypers = {f'hp_{i}_{k}': v for i, h in enumerate(self.learn.opt.hypers) for k, v in h.items()}\n",
    "            elif hasattr(self.learn.opt, 'param_groups'): # Fallback for some optimizers\n",
    "                hypers = {f'hp_{i}_lr': pg['lr'] for i, pg in enumerate(self.learn.opt.param_groups)}\n",
    "\n",
    "\n",
    "            # Get smoothed loss from Recorder if available, else raw loss\n",
    "            smooth_loss = self.learn.smooth_loss.item() if hasattr(self.learn,'smooth_loss') and getattr(self.learn.smooth_loss, 'is_valid', True) else self.learn.loss.item()\n",
    "            raw_loss = self.learn.loss.item()\n",
    "            log_data = {\n",
    "                'epoch_frac': self._wandb_epoch + self.learn.pct_train, # Log fractional epoch\n",
    "                'train_loss': smooth_loss if not math.isnan(smooth_loss) else 0.0,\n",
    "                'raw_loss': raw_loss if not math.isnan(raw_loss) else 0.0,\n",
    "                **hypers\n",
    "            }\n",
    "            wandb.log(log_data, step=self._wandb_step)\n",
    "\n",
    "    def before_epoch(self):\n",
    "        \"Update internal epoch counter.\"\n",
    "        self._wandb_epoch = self.learn.epoch # Update epoch number\n",
    "\n",
    "    # (inside SimpleWandbCallback)\n",
    "    def after_epoch(self):\n",
    "        rec = next((cb for cb in self.learn.cbs if isinstance(cb, Recorder)), None)\n",
    "        if rec is None or not rec.values: return\n",
    "        names = rec.metric_names\n",
    "        vals  = rec.values[-1]\n",
    "        log_dict = {n: float(v) for n,v in zip(names, vals)}\n",
    "        log_dict['epoch'] = self._wandb_epoch\n",
    "        wandb.log(log_dict, step=self._wandb_step)\n",
    "        logger.info(f\"Logged to W&B: {log_dict}\")\n",
    "\n",
    "    def _wandb_log_model(self, learn, file, **kwargs):\n",
    "        \"Hook function called by SaveModelCallback to log model artifacts to W&B.\"\n",
    "        if self.run is None or self.run.mode == \"disabled\": return # Don't log if wandb is disabled\n",
    "\n",
    "        save_model_cb = self._find_save_model_cb()\n",
    "        if not save_model_cb:\n",
    "            logger.error(\"SaveModelCallback not found in _wandb_log_model hook.\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Check if this save corresponds to a new best model\n",
    "            # Check both `new_best` (from >=2.7) and `is_new_best` (older tracker)\n",
    "            is_new_best = getattr(save_model_cb, 'new_best', getattr(save_model_cb, 'is_new_best', False))\n",
    "            should_log = self.log_model_policy == 'all' or (self.log_model_policy == 'best' and is_new_best)\n",
    "\n",
    "            if should_log:\n",
    "                # Get metadata from recorder if possible\n",
    "                recorder_cb = self._find_recorder_cb()\n",
    "                metadata = {}\n",
    "                if recorder_cb and hasattr(recorder_cb, 'final_record') and hasattr(recorder_cb, 'metric_names'):\n",
    "                    # Use final_record for accurate end-of-epoch values\n",
    "                     metadata = { n:f'{v:.5f}' for n,v in zip(recorder_cb.metric_names, recorder_cb.final_record) if n not in ['epoch', 'time']}\n",
    "                elif recorder_cb and hasattr(recorder_cb, 'log') and hasattr(recorder_cb, 'metric_names'):\n",
    "                     # Fallback to log if final_record not available\n",
    "                     logger.warning(\"Using recorder.log for artifact metadata (may not be final values).\")\n",
    "                     # Attempt to map log values; indices might be incorrect (see after_epoch)\n",
    "                     # This part is less reliable.\n",
    "                     valid_loss_idx = recorder_cb.metric_names.index('valid_loss') if 'valid_loss' in recorder_cb.metric_names else -1\n",
    "                     if valid_loss_idx != -1 and len(recorder_cb.log) > 1:\n",
    "                         metadata['valid_loss'] = f'{recorder_cb.log[1]:.5f}' # Assuming log[1] is valid_loss\n",
    "\n",
    "                aliases = [f'epoch_{learn.epoch}']\n",
    "                if is_new_best: aliases.append('best')\n",
    "\n",
    "                fname = getattr(save_model_cb, 'fname', 'model') # fname is usually just the base name like 'best_model'\n",
    "                artifact_name = f'{self.run.name}_{fname}' if self.run.name else fname\n",
    "                artifact = wandb.Artifact(name=artifact_name, type='model', metadata=metadata)\n",
    "\n",
    "                # `file` passed by the hook is the full path to the saved file\n",
    "                model_path = Path(file)\n",
    "                if model_path.is_file():\n",
    "                    artifact.add_file(model_path)\n",
    "                    self.run.log_artifact(artifact, aliases=aliases)\n",
    "                    logger.info(f\"Logged model artifact '{artifact_name}' to WandB with aliases {aliases}.\")\n",
    "                else:\n",
    "                    logger.error(f\"Model checkpoint file not found at: {model_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to log model artifact to WandB: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "    def after_fit(self):\n",
    "        \"Log best metric value and finish W&B run.\"\n",
    "        if self.run and self.run.mode != \"disabled\":\n",
    "            save_model_cb = self._find_save_model_cb()\n",
    "            # Log best metric value if available\n",
    "            if save_model_cb and hasattr(save_model_cb, 'best') and save_model_cb.best is not None:\n",
    "                monitor_metric = save_model_cb.monitor\n",
    "                wandb.summary[f'best_{monitor_metric}'] = save_model_cb.best\n",
    "                logger.info(f\"Logged best {monitor_metric} to WandB summary: {save_model_cb.best:.4f}\")\n",
    "\n",
    "            # Remove hook - check if hook exists before removing\n",
    "            if self.log_model_policy != 'false' and save_model_cb and hasattr(save_model_cb, 'remove_save_hooks'):\n",
    "                 try:\n",
    "                     # Check if the hook is actually present before removing\n",
    "                     if hasattr(save_model_cb, 'save_hooks') and self._wandb_log_model in save_model_cb.save_hooks:\n",
    "                         save_model_cb.remove_save_hooks(self._wandb_log_model)\n",
    "                         logger.info(\"Removed WandB save hook.\")\n",
    "                     else:\n",
    "                         logger.info(\"WandB save hook not found or already removed.\")\n",
    "\n",
    "                 except Exception as e:\n",
    "                     logger.warning(f\"Could not remove WandB save hook: {e}\")\n",
    "\n",
    "            # Check if wandb.run is still active before finishing\n",
    "            if wandb.run is not None:\n",
    "                wandb.finish()\n",
    "                logger.info(\"WandB run finished.\")\n",
    "            else:\n",
    "                logger.info(\"WandB run already finished or was never initialized.\")\n",
    "\n",
    "        # Reset internal state\n",
    "        self._wandb_step = 0\n",
    "        self._wandb_epoch = 0\n",
    "        self.run = None # Clear the run object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7d60217-b965-43b7-88db-c5c8f9bc2544",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from indic_clip.learner import _retrieval_metric_values\n",
    "except (ImportError, NameError):\n",
    "     # Define if not importable (e.g. callback defined in notebook)\n",
    "     _retrieval_metric_values = {}\n",
    "\n",
    "def valid_mean_recall(inp=None, targ=None): return tensor(_retrieval_metric_values.get('mean_recall', 0.0))\n",
    "def valid_i2t_r_at_1(inp=None, targ=None): return tensor(_retrieval_metric_values.get('i2t_r@1', 0.0))\n",
    "def valid_t2i_r_at_1(inp=None, targ=None): return tensor(_retrieval_metric_values.get('t2i_r@1', 0.0))\n",
    "# Add R@5, R@10 etc. if needed\n",
    "def valid_i2t_r_at_5(inp=None, targ=None): return tensor(_retrieval_metric_values.get('i2t_r@5', 0.0))\n",
    "def valid_t2i_r_at_5(inp=None, targ=None): return tensor(_retrieval_metric_values.get('t2i_r@5', 0.0))\n",
    "def valid_i2t_r_at_10(inp=None, targ=None): return tensor(_retrieval_metric_values.get('i2t_r@10', 0.0))\n",
    "def valid_t2i_r_at_10(inp=None, targ=None): return tensor(_retrieval_metric_values.get('t2i_r@10', 0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "Mv3j41qmmB78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to encapsulate the training logic\n",
    "def main(config: TrainConfig):\n",
    "    \"\"\"Main function to setup and run the training process using the config object.\"\"\"\n",
    "    # ... (Basic Setup, Load Data, Instantiate Model, Loss, Optimizer remains largely the same) ...\n",
    "    set_seed(config.seed)\n",
    "    ensure_dir(Path(config.checkpoint_dir))\n",
    "    logger.info(f\"Starting training run with config: {config.__dict__}\")\n",
    "\n",
    "    # --- Load Data ---\\n\" +\n",
    "    logger.info(f\"Loading data items from: {config.processed_data_path}\")\n",
    "    items_df = get_indic_clip_items(data_path=Path(config.processed_data_path))\n",
    "    if items_df.empty:\n",
    "        logger.error(\"No data items loaded. Exiting.\")\n",
    "        if wandb.run: wandb.finish(exit_code=1)\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Instantiating DataBlock...\")\n",
    "    tokenizer = IndicBERTTokenizer.load_tokenizer(Path(config.tokenizer_path), max_length=config.max_seq_len)\n",
    "    indic_clip_dblock = IndicCLIPDataBlock(\n",
    "        tokenizer_name_or_path=config.text_model_name,\n",
    "        tokenizer_save_path=Path(config.tokenizer_path),\n",
    "        max_length=config.max_seq_len,\n",
    "        img_size=config.img_size,\n",
    "        valid_pct=config.valid_pct,\n",
    "        seed=config.seed,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=config.num_workers,\n",
    "        use_augmentations=config.use_augmentations\n",
    "    )\n",
    "    logger.info(f\"Creating DataLoaders...\")\n",
    "    dls = indic_clip_dblock.get_dataloaders(items_df)\n",
    "    logger.info(f\"DataLoaders created. Train batches: {len(dls.train)}, Valid batches: {len(dls.valid)}\")\n",
    "\n",
    "    # --- Instantiate Model ---\\n\" +\n",
    "    logger.info(\"Instantiating IndicCLIP model...\")\n",
    "    model = IndicCLIP(\n",
    "        embed_dim=config.embed_dim,\n",
    "        vision_model_name=config.vision_model_name,\n",
    "        vision_pretrained=config.vision_pretrained,\n",
    "        text_model_name=config.text_model_name,\n",
    "        text_pretrained=config.text_pretrained,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    # --- Loss Function ---\\n\" +\n",
    "    metrics = [\n",
    "            valid_mean_recall,\n",
    "            valid_i2t_r_at_1,\n",
    "            valid_t2i_r_at_1,\n",
    "            valid_i2t_r_at_5,\n",
    "            valid_t2i_r_at_5,\n",
    "            valid_i2t_r_at_10,\n",
    "            valid_t2i_r_at_10,\n",
    "            ]\n",
    "\n",
    "    # --- Optimizer ---\\n\" +\n",
    "    opt_func = partial(OptimWrapper, opt=torch.optim.AdamW,\n",
    "                       betas=(config.beta1, config.beta2), eps=config.eps)\n",
    "\n",
    "    # --- Callbacks ---\\n\" +\n",
    "    logger.info(\"Configuring callbacks...\")\n",
    "\n",
    "    # Create standard callbacks first\n",
    "    save_cb = SaveModelCallback(\n",
    "            monitor='valid_loss',\n",
    "            comp=np.less,\n",
    "            fname=config.save_model_name,\n",
    "            # every_epoch=(config.save_epoch_frequency > 0 and config.save_epoch_frequency <= config.epochs),\n",
    "            at_end=True,\n",
    "            with_opt=True\n",
    "            )\n",
    "    early_stop_cb = EarlyStoppingCallback(\n",
    "            monitor='valid_loss',\n",
    "            comp=np.less,\n",
    "            patience=config.early_stopping_patience\n",
    "            )\n",
    "    retrieval_cb = RetrievalMetricCallback(k_values=[1, 5, 10])\n",
    "\n",
    "    callbacks = [\n",
    "        # InputInspectCallback(),\n",
    "        # GradientDebugCallback(),\n",
    "        # Recorder() is added automatically by Learner if metrics are present\n",
    "        #retrieval_cb,\n",
    "        # DebugRecorderStateCallback(),\n",
    "        # save_cb,\n",
    "        # early_stop_cb,\n",
    "        # ProgressCallback()\n",
    "    ]\n",
    "\n",
    "    # Add optional callbacks\n",
    "    if config.use_amp:\n",
    "        logger.info(\"Using Automatic Mixed Precision (AMP).\")\n",
    "        callbacks.insert(0, MixedPrecision())\n",
    "    if config.accum_freq > 1:\n",
    "        logger.info(\"Using Gradient Accumulation with frequency {config.accum_freq}.\")\n",
    "        callbacks.insert(0, GradientAccumulation(n_acc=config.accum_freq))\n",
    "    if config.grad_clip is not None:\n",
    "         logger.info(\"Using Gradient Clipping with value {config.grad_clip}.\")\n",
    "         callbacks.append(GradientClip(config.grad_clip))\n",
    "\n",
    "    # Create SimpleWandbCallback and add the hook to SaveModelCallback\n",
    "    # Do this *before* creating the Learner so hooks are set\n",
    "    simple_wandb_cb = SimpleWandbCallback(log_model_policy=config.wandb_log_model)\n",
    "    # Guard against SaveModelCallback not supporting add_save_hooks\n",
    "    if config.wandb_log_model != 'false' and hasattr(save_cb, 'add_save_hooks'):\n",
    "        try:\n",
    "            save_cb.add_save_hooks(simple_wandb_cb._wandb_log_model)\n",
    "            logger.info(\"Added WandB save hook to SaveModelCallback.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error attaching save hook: {e}\", exc_info=True)\n",
    "    else:\n",
    "        logger.warning(\"SaveModelCallback has no add_save_hooks—skipping W&B model hook.\")\n",
    "\n",
    "    simple_wandb_cb.order = max(SaveModelCallback.order, DebugRecorderStateCallback.order) + 1\n",
    "    callbacks.append(simple_wandb_cb)\n",
    "\n",
    "    # --- Create Learner ---\\n\" +\n",
    "    logger.info(\"Creating fastai Learner...\")\n",
    "    loss_func = ContrastiveLoss()\n",
    "    logger.info(f\"Using loss function: {type(loss_func)}\")\n",
    "    learn = Learner(dls, model, loss_func=loss_func, opt_func=opt_func,\n",
    "                    wd=config.wd, cbs=callbacks, metrics=None)\n",
    "\n",
    "    # logger.info(f\"Learner Summary: \\n {learn.summary()}\")\n",
    "\n",
    "    # Handle resuming from checkpoint (load state into the created learner)\n",
    "    if config.resume_from:\n",
    "        resume_path = Path(config.resume_from)\n",
    "        if resume_path.is_file():\n",
    "            try:\n",
    "                logger.info(f\"Resuming training from checkpoint: {resume_path}\")\n",
    "                learn.load(resume_path.stem, with_opt=True, device=dls.device)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to load checkpoint {resume_path}: {e}. Starting from scratch.\")\n",
    "        else:\n",
    "            logger.warning(f\"Resume checkpoint not found at {resume_path}. Starting from scratch.\")\n",
    "\n",
    "    # --- Start Training ---\\n\" +\n",
    "    logger.info(f\"Starting training for {config.epochs} epochs...\")\n",
    "    if len(dls.train) == 0:\n",
    "         logger.error(\"Training dataloader is empty. Cannot calculate steps.\")\n",
    "         if wandb.run: wandb.finish(exit_code=1)\n",
    "         return\n",
    "\n",
    "    total_steps = len(dls.train) * config.epochs\n",
    "    actual_epochs = config.epochs\n",
    "\n",
    "    if config.max_steps:\n",
    "        logger.warning(f\"Limiting training to a maximum of {config.max_steps} steps.\" )\n",
    "        epochs_to_run = math.ceil(config.max_steps / len(dls.train))\n",
    "        actual_epochs = min(config.epochs, int(epochs_to_run))\n",
    "        logger.info(f\"Adjusted epochs to {actual_epochs} based on max_steps.\")\n",
    "        # Add TrainStepsCallback (if not already present)\n",
    "        # Need to recreate learner or add callback dynamically *before* fit\n",
    "        has_steps_cb = any(isinstance(cb, TrainStepsCallback) for cb in learn.cbs)\n",
    "        if not has_steps_cb:\n",
    "            learn.add_cb(TrainStepsCallback(config.max_steps))\n",
    "            logger.info(\"Added TrainStepsCallback.\")\n",
    "\n",
    "    print(f\"loss function: {learn.loss_func}\")\n",
    "    # Run training\n",
    "    learn.fit_one_cycle(\n",
    "        n_epoch=actual_epochs,\n",
    "        lr_max=config.lr,\n",
    "        pct_start=min(0.3, config.warmup_steps / total_steps) if total_steps > 0 else 0.1\n",
    "    )\n",
    "\n",
    "    logger.info(\"Training finished.\")\n",
    "    # Ensure WandB run is finished cleanly\n",
    "    if wandb.run is not None:\n",
    "         wandb.finish()\n",
    "         logger.info(\"WandB run finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kU1z414-mB79",
   "metadata": {},
   "source": [
    "## Run Training (Interactive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive_execution_cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 05:56:31 - __main__ - INFO - Starting training run with config: {'epochs': 1, 'batch_size': 32, 'wandb_entity': None, 'vision_model_name': 'resnet18', 'embed_dim': 512, 'num_workers': 16, 'valid_pct': 0.25, 'use_amp': False}\n",
      "2025-04-22 05:56:31 - __main__ - INFO - Loading data items from: /workspace/indic-clip/data/processed/filtered_data.jsonl\n",
      "2025-04-22 05:56:31 - __main__ - INFO - Instantiating DataBlock...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training interactively from notebook...\n",
      "*** Warning: WandB Entity is not set in config or environment. ***\n",
      "*** WandB logging will be disabled. Set config.wandb_entity or WANDB_ENTITY env var. ***\n",
      "Starting interactive run with config:\n",
      "  epochs: 1\n",
      "  batch_size: 32\n",
      "  wandb_entity: None\n",
      "  vision_model_name: resnet18\n",
      "  embed_dim: 512\n",
      "  num_workers: 16\n",
      "  valid_pct: 0.25\n",
      "  use_amp: False\n",
      "------------------------------\n",
      "Loaded 8006 items from /workspace/indic-clip/data/processed/filtered_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 05:56:31 - indic_clip.data.tokenization - INFO - Successfully loaded tokenizer: /workspace/indic-clip/models/tokenizer\n",
      "2025-04-22 05:56:32 - indic_clip.data.tokenization - INFO - Custom special tokens already exist or none were specified.\n",
      "2025-04-22 05:56:32 - indic_clip.data.tokenization - INFO - Tokenizer state loaded successfully from /workspace/indic-clip/models/tokenizer\n",
      "2025-04-22 05:56:32 - indic_clip.data.tokenization - INFO - Successfully loaded tokenizer: /workspace/indic-clip/models/tokenizer\n",
      "2025-04-22 05:56:32 - indic_clip.data.tokenization - INFO - Custom special tokens already exist or none were specified.\n",
      "2025-04-22 05:56:32 - indic_clip.data.tokenization - INFO - Tokenizer state loaded successfully from /workspace/indic-clip/models/tokenizer\n",
      "2025-04-22 05:56:32 - __main__ - INFO - Creating DataLoaders...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating DataLoaders with bs=32, num_workers=16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-22 05:56:33 - __main__ - INFO - DataLoaders created. Train batches: 187, Valid batches: 63\n",
      "2025-04-22 05:56:33 - __main__ - INFO - Instantiating IndicCLIP model...\n",
      "2025-04-22 05:56:33 - timm.models._builder - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "2025-04-22 05:56:33 - timm.models._hub - INFO - [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "2025-04-22 05:56:33 - indic_clip.model.vision - INFO - Loaded timm model: resnet18 with pretrained=True\n",
      "2025-04-22 05:56:33 - indic_clip.model.vision - INFO - Backbone feature dimension: 512\n",
      "2025-04-22 05:56:34 - indic_clip.model.text - INFO - Loading text model: ai4bharat/indic-bert with pretrained=True\n",
      "2025-04-22 05:56:34 - indic_clip.model.text - INFO - Model hidden dimension: 768\n",
      "2025-04-22 05:56:35 - indic_clip.model.text - WARNING - Tokenizer vocab size (200002) differs from model embedding size (200000). Resizing model token embeddings.\n",
      "2025-04-22 05:56:35 - indic_clip.model.text - INFO - Model embedding size resized to 200002\n",
      "2025-04-22 05:56:35 - indic_clip.model.clip - INFO - IndicCLIP initialized with vision='resnet18', text='ai4bharat/indic-bert', embed_dim=512\n",
      "2025-04-22 05:56:35 - __main__ - INFO - Configuring callbacks...\n",
      "2025-04-22 05:56:35 - __main__ - WARNING - SaveModelCallback has no add_save_hooks—skipping W&B model hook.\n",
      "2025-04-22 05:56:35 - __main__ - INFO - Creating fastai Learner...\n",
      "2025-04-22 05:56:35 - __main__ - INFO - Using loss function: <class 'indic_clip.loss.ContrastiveLoss'>\n",
      "2025-04-22 05:56:35 - __main__ - INFO - Starting training for 1 epochs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss function: ContrastiveLoss()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='76' class='' max='187' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      40.64% [76/187 00:35&lt;00:52 3.4950]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# --- Interactive Execution Block (for Notebooks) ---\n",
    "\n",
    "# Check if running interactively (not as a script imported by another module)\n",
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    print(\"Running training interactively from notebook...\")\n",
    "\n",
    "    # Instantiate the config object defined in the cell above\n",
    "    # --- >>>> IMPORTANT: Modify the TrainConfig class definition cell <<<< ---\n",
    "    # --- >>>>        directly to change hyperparameters for your run <<<< ---\n",
    "    config = TrainConfig()\n",
    "\n",
    "    # --- Example Overrides for a quick test ---\n",
    "    config.epochs = 1\n",
    "    config.batch_size = 32 # Reduced batch size for faster iteration/memory\n",
    "    # config.max_steps = 10 # Uncomment to run only a few steps\n",
    "    # config.wandb_entity = \"numb3r33\" # <<< SET YOUR WANDB ENTITY HERE >>>\n",
    "    config.wandb_entity = None # <<< SET YOUR WANDB ENTITY HERE >>>\n",
    "    \n",
    "    config.vision_model_name = 'resnet18' # Use a smaller vision model for testing\n",
    "    config.embed_dim = 512 # Adjust embed_dim to match ResNet18 output\n",
    "    config.num_workers = 16 # Often safer for interactive debugging\n",
    "    config.valid_pct = 0.25 # Use a bit more validation data for testing\n",
    "    config.use_amp = False # Use Automatic Mixed Precision\n",
    "\n",
    "    # ----------------------------------------\n",
    "\n",
    "    if not config.wandb_entity:\n",
    "        print(\"*** Warning: WandB Entity is not set in config or environment. ***\")\n",
    "        print(\"*** WandB logging will be disabled. Set config.wandb_entity or WANDB_ENTITY env var. ***\")\n",
    "\n",
    "    print(f\"Starting interactive run with config:\")\n",
    "    # Pretty print config\n",
    "    for key, value in config.__dict__.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    try:\n",
    "       main(config=config) # Pass the config object to main\n",
    "    except Exception as e:\n",
    "       logger.error(f\"Interactive run failed: {e}\", exc_info=True)\n",
    "       # Ensure wandb is finished even on error\n",
    "       if wandb.run is not None and wandb.run.mode != \"disabled\":\n",
    "           wandb.finish(exit_code=1)\n",
    "    \n",
    "    print(\"\\nInteractive execution cell finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yv97M0_AmB7-",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Export the notebook using nbdev (Optional - this notebook is now interactive-only)\n",
    "if __name__ == '__main__' and '__file__' not in globals(): # Only run in notebook context\n",
    "    pass # No export needed for interactive-only notebook\n",
    "    # print(\"Skipping export for interactive-only notebook.\")\n",
    "    # try:\n",
    "    #     import nbdev.export\n",
    "    #     print(\"Please run 'nbdev_export' in your terminal from the project root directory to export other notebooks.\")\n",
    "    # except ImportError:\n",
    "    #     print(\"nbdev not found. Run 'pip install nbdev' to use nbdev features.\")\n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error during nbdev check: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "003d759609ee48baa541716f84f7a413": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0198525a5d694fc3960540125ab84d38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "01d26739c0af48968277c16357e2d497": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0e065423bb4a46b0ac0dbe6fc5ab30dc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0ff4588043b74290a2279621f9e853aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5ab3b988ec8a4e48bc8be9c4e2a88b40",
       "IPY_MODEL_a4c3c2c8c37b4ecaa11b9da31fcd2a1a",
       "IPY_MODEL_c15f52a8cd2540d4b59f6126f52cd077"
      ],
      "layout": "IPY_MODEL_c4c96cccb89e4ea08d3aecc94fbbad8a"
     }
    },
    "11088093c4854f9a9be9fcc23bdcfe89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "12b60243d1934091ae9b88e7dc1e0f2b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc86aff92eef4fe886db9574da258439",
      "max": 46807446,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8953c1d83de24fe6aa8075ad4e878430",
      "value": 46807446
     }
    },
    "1664654612cc4297954b26c43da4b052": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fa2ebef446f40d2a3965a4fabfd6cef",
      "placeholder": "​",
      "style": "IPY_MODEL_7d265b4142de4ac6ab58e3e540cc9401",
      "value": "model.safetensors: 100%"
     }
    },
    "207c1864025d4a84b39f42c83943ccc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff3e68eba2024f029daf42d1e2a1b338",
      "max": 134979328,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_01d26739c0af48968277c16357e2d497",
      "value": 134979328
     }
    },
    "22248ede03d44bae8cdbcf448d169337": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ea59ade8847747bfb52b22ae8647158b",
       "IPY_MODEL_695814d81cb849b6b0d4406117aa5efd",
       "IPY_MODEL_260cc4fd23ec40f08bc91bfb283c7aa0"
      ],
      "layout": "IPY_MODEL_5adf5940139d486f975ec70e3d1f856b"
     }
    },
    "260cc4fd23ec40f08bc91bfb283c7aa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_77dfc3c1c17c4a7b97348c9b25e7cafe",
      "placeholder": "​",
      "style": "IPY_MODEL_ceeb9e9a7da04596831dce2fea752789",
      "value": " 507/507 [00:00&lt;00:00, 19.3kB/s]"
     }
    },
    "2c54ec5480704633b54a4588f2a7f8a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_003d759609ee48baa541716f84f7a413",
      "placeholder": "​",
      "style": "IPY_MODEL_0198525a5d694fc3960540125ab84d38",
      "value": " 135M/135M [00:02&lt;00:00, 61.0MB/s]"
     }
    },
    "434987cf195846b3ae92aa640ce59b33": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ab3b988ec8a4e48bc8be9c4e2a88b40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e6dddef2e3e4ea78969c2ca36aeecbb",
      "placeholder": "​",
      "style": "IPY_MODEL_ea6f76fd99bc43a3b6d39a0787b04442",
      "value": "pytorch_model.bin: 100%"
     }
    },
    "5adf5940139d486f975ec70e3d1f856b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5e987e715ad44415b4fcff05d88924f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62f36aeec0204385948875432556a16c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "695814d81cb849b6b0d4406117aa5efd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_62f36aeec0204385948875432556a16c",
      "max": 507,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bb98df18875640728e87d85322400c43",
      "value": 507
     }
    },
    "6a9cb6bd443842fca1be894281e35cce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77dfc3c1c17c4a7b97348c9b25e7cafe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d265b4142de4ac6ab58e3e540cc9401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7f837c368252429187d25b59f716d365": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7fc5553d95bf480f9ef96739c444570a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fedf54acd5e443f1aa4e35367cd73aef",
      "placeholder": "​",
      "style": "IPY_MODEL_11088093c4854f9a9be9fcc23bdcfe89",
      "value": " 46.8M/46.8M [00:00&lt;00:00, 95.7MB/s]"
     }
    },
    "8953c1d83de24fe6aa8075ad4e878430": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8fa2ebef446f40d2a3965a4fabfd6cef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8fc07c6ea45a49be87323335a81a817d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_90065cebfdbe43fa8304664d85db24ee",
       "IPY_MODEL_12b60243d1934091ae9b88e7dc1e0f2b",
       "IPY_MODEL_7fc5553d95bf480f9ef96739c444570a"
      ],
      "layout": "IPY_MODEL_d64a27d5aa1c426abf3dd02e5cdbc64f"
     }
    },
    "90065cebfdbe43fa8304664d85db24ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e065423bb4a46b0ac0dbe6fc5ab30dc",
      "placeholder": "​",
      "style": "IPY_MODEL_fa26e0ca48a74b17aa76cb2b2872b060",
      "value": "model.safetensors: 100%"
     }
    },
    "9a29e0ba85fb4bc6bf523ff8e1229a41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1664654612cc4297954b26c43da4b052",
       "IPY_MODEL_207c1864025d4a84b39f42c83943ccc9",
       "IPY_MODEL_2c54ec5480704633b54a4588f2a7f8a5"
      ],
      "layout": "IPY_MODEL_5e987e715ad44415b4fcff05d88924f5"
     }
    },
    "9e6dddef2e3e4ea78969c2ca36aeecbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0697e1bb16f4aa59d0f386430ebfe20": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4c3c2c8c37b4ecaa11b9da31fcd2a1a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0697e1bb16f4aa59d0f386430ebfe20",
      "max": 134982446,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7f837c368252429187d25b59f716d365",
      "value": 134982446
     }
    },
    "b247d02503cc43b497f5971b2994ea52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bb98df18875640728e87d85322400c43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c15f52a8cd2540d4b59f6126f52cd077": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f3f15c044f0849de95cd4458337758f9",
      "placeholder": "​",
      "style": "IPY_MODEL_434987cf195846b3ae92aa640ce59b33",
      "value": " 135M/135M [00:00&lt;00:00, 211MB/s]"
     }
    },
    "c4c96cccb89e4ea08d3aecc94fbbad8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ceeb9e9a7da04596831dce2fea752789": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d64a27d5aa1c426abf3dd02e5cdbc64f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc86aff92eef4fe886db9574da258439": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea59ade8847747bfb52b22ae8647158b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6a9cb6bd443842fca1be894281e35cce",
      "placeholder": "​",
      "style": "IPY_MODEL_b247d02503cc43b497f5971b2994ea52",
      "value": "config.json: 100%"
     }
    },
    "ea6f76fd99bc43a3b6d39a0787b04442": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3f15c044f0849de95cd4458337758f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fa26e0ca48a74b17aa76cb2b2872b060": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fedf54acd5e443f1aa4e35367cd73aef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ff3e68eba2024f029daf42d1e2a1b338": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
