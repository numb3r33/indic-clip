{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KlmF3Rkd_k51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oE5D6l3jAlUa",
   "metadata": {},
   "source": [
    "# Learner and Callbacks\n",
    "\n",
    "> Defines custom fast.ai Callbacks for Indic-CLIP training, specifically for calculating retrieval metrics during validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vP7oQx8TAm5A",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fT62U_f9Am5B",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Google Drive mounted successfully.\n",
      "Added /content/drive/MyDrive/Indic-Clip to sys.path\n",
      "Changed working directory to: /content/drive/MyDrive/Indic-Clip\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Mount Google Drive (Optional, but recommended for persistent storage)\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "    # Define PROJECT_ROOT for Colab\n",
    "    PROJECT_ROOT = Path('/content/drive/MyDrive/Indic-Clip') # Adjust path if needed\n",
    "    if not PROJECT_ROOT.exists():\n",
    "        print(f\"Warning: Project directory not found at {PROJECT_ROOT}. Please ensure it exists.\")\n",
    "    else:\n",
    "        # Add project root to sys.path\n",
    "        if str(PROJECT_ROOT) not in sys.path:\n",
    "            sys.path.insert(0, str(PROJECT_ROOT))\n",
    "            print(f\"Added {PROJECT_ROOT} to sys.path\")\n",
    "        # Change current working directory\n",
    "        os.chdir(PROJECT_ROOT)\n",
    "        print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, skipping Drive mount.\")\n",
    "    # Define PROJECT_ROOT for local execution (adjust if needed)\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    if PROJECT_ROOT.name == 'nbs': PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    print(f\"Running locally. Project root assumed: {PROJECT_ROOT}\")\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "         sys.path.insert(0, str(PROJECT_ROOT))\n",
    "         print(f\"Added {PROJECT_ROOT} to sys.path\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Colab setup: {e}\")\n",
    "    PROJECT_ROOT = Path('.').resolve()\n",
    "    print(f\"Defaulting project root to current dir: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lX92w1HnAm5C",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive detected, setting PROJECT_ROOT to /content/drive/MyDrive/Indic-Clip\n",
      "Ensure your project files are located there.\n",
      "Imported indic_clip.core\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Use try-except for robustness, especially during development/export\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    print(\"Imported indic_clip.core\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"indic_clip.core not found initially.\")\n",
    "    # Attempt to set sys.path if running in Colab and project cloned\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        project_parent = '/content' # Assuming cloned into /content/indic-clip\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "             project_parent = '/content/drive/MyDrive/Indic-Clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J5G3uH0JAm5D",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# !pip install -qr requirements.txt\n",
    "# !pip install scikit-learn # Make sure sklearn is installed for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wc-l9Y0RAm5E",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9k5lFhOAm5E",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.load import DataLoader\n",
    "from fastcore.all import *\n",
    "from fastai.torch_core import * # For tensor()\n",
    "from fastai.text.all import *\n",
    "\n",
    "try:\n",
    "    from indic_clip.core import get_logger\n",
    "    from indic_clip.evaluation.metrics import calculate_retrieval_metrics, calculate_zeroshot_accuracy # Import actual functions\n",
    "    from indic_clip.loss import ContrastiveLoss\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Could not import project modules in 09_learner.ipynb.\")\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "    # Define stubs if import fails\n",
    "    def calculate_retrieval_metrics(*args, **kwargs): return {'mean_recall': 0.0, 'i2t_r@1': 0.0, 't2i_r@1': 0.0}\n",
    "    def calculate_zeroshot_accuracy(*args, **kwargs): return 0.0\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oP76v4Q2Am5F",
   "metadata": {},
   "source": [
    "## Retrieval Metric Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pU_C3K_oAm5F",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Removed the stub function as we should import the actual one from evaluation.metrics\n",
    "# If 11_evaluation_metrics.ipynb is not yet implemented, the try-except block above will define a basic stub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "A2dTXgbSExVZ",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ContrastiveCallback(Callback):\n",
    "    \"\"\"\n",
    "    Handles fetching model outputs, calculating contrastive loss,\n",
    "    collecting features for RetrievalMetricCallback, and providing\n",
    "    compatible preds for default callbacks.\n",
    "    \"\"\"\n",
    "    order = Recorder.order - 1 # Run early after prediction\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loss_func = ContrastiveLoss() # Holds the REAL loss function\n",
    "\n",
    "    def before_validate(self):\n",
    "        self.learn.img_feats_for_retrieval = []\n",
    "        self.learn.txt_feats_for_retrieval = []\n",
    "        logger.debug(\"ContrastiveCallback: Initialized learner lists for retrieval features.\")\n",
    "\n",
    "    def after_pred(self):\n",
    "        \"\"\"\n",
    "        Intercept raw prediction tuple, calculate local logits for learn.pred,\n",
    "        store raw tuple for loss, and collect features for retrieval.\n",
    "        \"\"\"\n",
    "        raw_preds = self.learn.pred # Get the original tuple (img, txt, scale)\n",
    "        self._current_raw_preds = raw_preds # Store for after_loss\n",
    "\n",
    "        if raw_preds is None:\n",
    "            logger.warning(\"ContrastiveCallback.after_pred received None prediction.\")\n",
    "            self.learn.pred = None # Keep it None if input was None\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Unpack the raw predictions\n",
    "            img_feat, txt_feat, logit_scale = raw_preds\n",
    "\n",
    "            # --- Calculate LOCAL logits to populate learn.pred ---\n",
    "            # Use features directly from this batch, NOT gathered features here\n",
    "            local_logits = logit_scale * img_feat @ txt_feat.t()\n",
    "            self.learn.pred = local_logits # Set learn.pred to something GatherPredsCallback can handle\n",
    "\n",
    "            # --- Collect features for RetrievalMetricCallback during validation ---\n",
    "            if not self.training:\n",
    "                # Ensure lists exist on learner (defensive check)\n",
    "                if not hasattr(self.learn, 'img_feats_for_retrieval'):\n",
    "                    self.learn.img_feats_for_retrieval = []\n",
    "                if not hasattr(self.learn, 'txt_feats_for_retrieval'):\n",
    "                    self.learn.txt_feats_for_retrieval = []\n",
    "\n",
    "                self.learn.img_feats_for_retrieval.append(img_feat.cpu().detach())\n",
    "                self.learn.txt_feats_for_retrieval.append(txt_feat.cpu().detach())\n",
    "\n",
    "        except Exception as e:\n",
    "             logger.error(f\"Error processing preds in ContrastiveCallback.after_pred: {e}\", exc_info=True)\n",
    "             # Set pred to None or a dummy tensor if unpacking/logit calc fails\n",
    "             self.learn.pred = None\n",
    "             # Ensure raw preds are also cleared if they caused error\n",
    "             if hasattr(self, '_current_raw_preds'): delattr(self, '_current_raw_preds')\n",
    "\n",
    "\n",
    "    def after_loss(self):\n",
    "        \"\"\"\n",
    "        Calculate the *actual* contrastive loss using stored raw predictions\n",
    "        (potentially with gathered features) and overwrite learn.loss/loss_grad.\n",
    "        \"\"\"\n",
    "        # Check if we stored preds for this batch\n",
    "        if hasattr(self, '_current_raw_preds') and self._current_raw_preds is not None:\n",
    "            try:\n",
    "                # --- Re-calculate Loss Correctly (using potentially gathered features) ---\n",
    "                image_features, text_features, logit_scale = self._current_raw_preds\n",
    "\n",
    "                # gathered_image_features = image_features\n",
    "                # gathered_text_features = text_features\n",
    "\n",
    "                # # Calculate similarity and loss (similar to ContrastiveLoss internal logic)\n",
    "                # logits_per_image = logit_scale * gathered_image_features @ gathered_text_features.t()\n",
    "                # logits_per_text = logits_per_image.t()\n",
    "\n",
    "                # global_batch_size = gathered_image_features.size(0)\n",
    "                # device = image_features.device\n",
    "                # labels = torch.arange(global_batch_size, device=device, dtype=torch.long)\n",
    "\n",
    "                # loss_img = F.cross_entropy(logits_per_image, labels)\n",
    "                # loss_txt = F.cross_entropy(logits_per_text, labels)\n",
    "                # actual_loss = (loss_img + loss_txt) / 2\n",
    "\n",
    "                actual_loss = self.loss_func((image_features, text_features, logit_scale))\n",
    "\n",
    "                # --- End Re-calculation ---\n",
    "\n",
    "                # Check if loss calculation resulted in NaN\n",
    "                if torch.isnan(actual_loss):\n",
    "                    logger.warning(f\"NaN detected in calculated loss within ContrastiveCallback (Loss Img: {loss_img.item()}, Loss Txt: {loss_txt.item()}). Setting loss_grad to NaN.\")\n",
    "                    self.learn.loss_grad = actual_loss\n",
    "                    self.learn.loss = actual_loss.clone().detach()\n",
    "                else:\n",
    "                    # Overwrite learner's loss attributes with the valid loss\n",
    "                    self.learn.loss_grad = actual_loss # For backward pass\n",
    "                    self.learn.loss = actual_loss.clone().detach() # For recording\n",
    "                    # logger.debug(f\"ContrastiveCallback calculated loss: {self.learn.loss.item()}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Error calculating loss in ContrastiveCallback.after_loss: {e}\", exc_info=True)\n",
    "                 device = default_device()\n",
    "                 try: device = self._current_raw_preds[0].device\n",
    "                 except: pass\n",
    "                 self.learn.loss_grad = torch.tensor(float('nan'), device=device)\n",
    "                 self.learn.loss = torch.tensor(float('nan'), device=device)\n",
    "            finally:\n",
    "                 # Clean up stored preds for the batch\n",
    "                 delattr(self, '_current_raw_preds')\n",
    "        # else: logger.debug(\"No _current_raw_preds found in after_loss.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g7z3sLw3Am5G",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RetrievalMetricCallback(Callback):\n",
    "    \"\"\"\n",
    "    1) Reads features collected by ContrastiveCallback from learner in after_validate,\n",
    "    2) Computes retrieval metrics,\n",
    "    3) Stores mean_recall in self._last_metrics,\n",
    "    4) Injects it into recorder.values in after_epoch.\n",
    "    \"\"\"\n",
    "    order = Recorder.order + 1  # run right after Recorder.after_epoch\n",
    "\n",
    "    def __init__(self, k_values=[1, 5, 10]):\n",
    "        store_attr()\n",
    "        # No longer needs self.img_feats, self.txt_feats\n",
    "        self._last_metrics = {} # Store metrics calculated in after_validate\n",
    "\n",
    "    # REMOVED: before_validate (handled by ContrastiveCallback)\n",
    "    # REMOVED: after_pred (handled by ContrastiveCallback)\n",
    "\n",
    "    def _gather_features(self, features_list):\n",
    "        \"Gathers features from all GPUs if in distributed mode.\"\n",
    "        # Note: This needs testing in a distributed setup.\n",
    "        # Assumes features_list is a list of tensors on CPU already.\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            world_size = dist.get_world_size()\n",
    "            if world_size <= 1: return features_list # No need to gather\n",
    "\n",
    "            logger.info(f\"Gathering features from {world_size} GPUs.\")\n",
    "            gathered_nested = [None] * world_size\n",
    "            try:\n",
    "                # Ensure objects are pickleable (CPU tensors should be)\n",
    "                dist.all_gather_object(gathered_nested, features_list)\n",
    "                # Concatenate gathered lists\n",
    "                all_features = []\n",
    "                for rank_list in gathered_nested:\n",
    "                    all_features.extend(rank_list)\n",
    "                logger.info(f\"Gathered total {len(all_features)} feature tensors.\")\n",
    "                return all_features\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Error during all_gather_object: {e}\", exc_info=True)\n",
    "                 return [] # Return empty list on error\n",
    "        else:\n",
    "            # logger.debug(\"Not in distributed mode or world_size=1. Using local features.\")\n",
    "            return features_list\n",
    "\n",
    "    def after_validate(self):\n",
    "        \"\"\"Concatenate features stored on Learner, compute metrics, stash in self._last_metrics.\"\"\"\n",
    "        logger.info(\"RetrievalMetricCallback: Starting metric calculation in after_validate.\")\n",
    "\n",
    "        # Retrieve features from the Learner attributes\n",
    "        img_feats_list_local = getattr(self.learn, 'img_feats_for_retrieval', [])\n",
    "        txt_feats_list_local = getattr(self.learn, 'txt_feats_for_retrieval', [])\n",
    "\n",
    "        if not img_feats_list_local or not txt_feats_list_local:\n",
    "            logger.warning(\"No features found on learner (img_feats_for_retrieval / txt_feats_for_retrieval). Skipping metric calculation.\")\n",
    "            self._last_metrics = {}\n",
    "            # Clean up learner attributes even if empty\n",
    "            if hasattr(self.learn, 'img_feats_for_retrieval'): delattr(self.learn, 'img_feats_for_retrieval')\n",
    "            if hasattr(self.learn, 'txt_feats_for_retrieval'): delattr(self.learn, 'txt_feats_for_retrieval')\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Found {len(img_feats_list_local)} image feature batches and {len(txt_feats_list_local)} text feature batches on learner.\")\n",
    "\n",
    "        # --- Gather features across GPUs if in distributed mode ---\n",
    "        # Gather on all ranks, but only calculate on rank 0\n",
    "        all_img_features_gathered = self._gather_features(img_feats_list_local)\n",
    "        all_txt_features_gathered = self._gather_features(txt_feats_list_local)\n",
    "\n",
    "        # --- Proceed with calculation only on rank 0 ---\n",
    "        is_rank_zero = not (dist.is_available() and dist.is_initialized()) or dist.get_rank() == 0\n",
    "\n",
    "        if is_rank_zero:\n",
    "            if not all_img_features_gathered or not all_txt_features_gathered:\n",
    "                logger.warning(f\"Rank 0: No features available after gathering. Skipping metric calculation.\")\n",
    "                self._last_metrics = {}\n",
    "                # Still need to clean up learner attributes on other ranks if they exist\n",
    "                # (Cleanup moved outside rank_zero block)\n",
    "                return\n",
    "\n",
    "            # --- Concatenate features ---\n",
    "            try:\n",
    "                device = next(self.learn.model.parameters()).device\n",
    "                # Ensure lists contain tensors before concatenating\n",
    "                if not all(isinstance(t, torch.Tensor) for t in all_img_features_gathered):\n",
    "                    logger.error(\"Gathered image features list contains non-Tensor objects.\")\n",
    "                    self._last_metrics = {}; return\n",
    "                if not all(isinstance(t, torch.Tensor) for t in all_txt_features_gathered):\n",
    "                    logger.error(\"Gathered text features list contains non-Tensor objects.\")\n",
    "                    self._last_metrics = {}; return\n",
    "\n",
    "                img_feats = torch.cat(all_img_features_gathered).to(device)\n",
    "                txt_feats = torch.cat(all_txt_features_gathered).to(device)\n",
    "                logger.info(f\"Concatenated features. Img shape: {img_feats.shape}, Txt shape: {txt_feats.shape}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error concatenating or moving features: {e}\", exc_info=True)\n",
    "                self._last_metrics = {}\n",
    "                # Cleanup moved outside rank_zero block\n",
    "                return\n",
    "\n",
    "             # --- Get Logit Scale ---\n",
    "            if hasattr(self.learn.model, 'logit_scale') and isinstance(self.learn.model.logit_scale, nn.Parameter):\n",
    "                 # Clamp before exp just in case (already done in model fwd, but belt-and-suspenders)\n",
    "                 clamped_log_scale = self.learn.model.logit_scale.data.clamp(max=np.log(1 / 0.01)).clone()\n",
    "                 logit_scale = clamped_log_scale.exp().to(device)\n",
    "            else:\n",
    "                 logger.warning(\"Logit scale not found or not Parameter on model in RetrievalMetricCallback, using default.\")\n",
    "                 logit_scale = tensor(np.exp(np.log(1 / 0.07)), device=device) # Default CLIP init\n",
    "\n",
    "            # --- Calculate Metrics ---\n",
    "            try:\n",
    "                metrics = calculate_retrieval_metrics(\n",
    "                    image_features=img_feats,\n",
    "                    text_features=txt_feats,\n",
    "                    logit_scale=logit_scale,\n",
    "                    k_values=self.k_values\n",
    "                )\n",
    "                logger.info(f\"Calculated retrieval metrics: {metrics}\")\n",
    "                self._last_metrics = metrics # Store for after_epoch injection\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Error during metric calculation: {e}\", exc_info=True)\n",
    "                 self._last_metrics = {}\n",
    "\n",
    "        else:\n",
    "            logger.debug(f\"Rank {dist.get_rank()} skipping metric calculation.\")\n",
    "            self._last_metrics = {} # Ensure non-rank-zero doesn't hold stale metrics\n",
    "\n",
    "        # --- Clean up learner attributes on ALL ranks ---\n",
    "        # It's important to clean up on all ranks to free memory,\n",
    "        # especially after gathering which might duplicate lists.\n",
    "        if hasattr(self.learn, 'img_feats_for_retrieval'):\n",
    "            delattr(self.learn, 'img_feats_for_retrieval')\n",
    "            logger.debug(f\"Cleaned up learner.img_feats_for_retrieval on rank {dist.get_rank() if dist.is_initialized() else 0}\")\n",
    "        if hasattr(self.learn, 'txt_feats_for_retrieval'):\n",
    "            delattr(self.learn, 'txt_feats_for_retrieval')\n",
    "            logger.debug(f\"Cleaned up learner.txt_feats_for_retrieval on rank {dist.get_rank() if dist.is_initialized() else 0}\")\n",
    "\n",
    "\n",
    "    def after_epoch(self):\n",
    "        \"\"\"Inject mean_recall scalar into recorder.values.\"\"\"\n",
    "        # Only rank zero should modify the recorder state significantly\n",
    "        is_rank_zero = not (dist.is_available() and dist.is_initialized()) or dist.get_rank() == 0\n",
    "\n",
    "        if is_rank_zero and self._last_metrics: # Only inject if metrics were calculated\n",
    "            rec = getattr(self.learn, 'recorder', None)\n",
    "            if rec is None or not hasattr(rec, 'metric_names') or not hasattr(rec, 'values') or not rec.values:\n",
    "                if getattr(self.learn, 'metrics', None): # Only error if metrics were expected\n",
    "                   logger.error(\"Recorder/values missing â€“ cannot inject mean_recall.\")\n",
    "                else:\n",
    "                   logger.debug(\"No recorder/metrics - skipping mean_recall injection.\")\n",
    "                self._last_metrics = {} # Clear metrics if they can't be injected\n",
    "                return\n",
    "\n",
    "            # Ensure the list for the current epoch exists in recorder.values\n",
    "            current_epoch_idx = self.learn.epoch # epoch starts at 0\n",
    "            if len(rec.values) <= current_epoch_idx:\n",
    "                 logger.warning(f\"Recorder.values list (len {len(rec.values)}) too short for current epoch {current_epoch_idx}. Cannot inject metric.\")\n",
    "                 self._last_metrics = {}\n",
    "                 return\n",
    "\n",
    "            # Define the metric name we want to inject/update\n",
    "            metric_name = 'valid_mean_recall'\n",
    "\n",
    "            # Ensure metric name exists (should be added by Learner init if passed in metrics)\n",
    "            if metric_name not in rec.metric_names:\n",
    "                if getattr(self.learn, 'metrics', None):\n",
    "                    logger.warning(f\"Metric '{metric_name}' not in Recorder.metric_names. Appending.\")\n",
    "                    rec.metric_names.append(metric_name)\n",
    "                else: # No metrics expected, don't inject\n",
    "                    logger.debug(f\"'{metric_name}' not tracked by Recorder. Skipping injection.\")\n",
    "                    self._last_metrics = {}\n",
    "                    return\n",
    "\n",
    "            # Find the index for the metric\n",
    "            try:\n",
    "                 metric_idx = rec.metric_names.index(metric_name)\n",
    "            except ValueError:\n",
    "                 logger.error(f\"'{metric_name}' is somehow not in metric_names after check/append. Skipping injection.\")\n",
    "                 self._last_metrics = {}\n",
    "                 return\n",
    "\n",
    "            # Calculate the expected index in the *values* list for this epoch\n",
    "            # Assumes values[-1] order is [train_loss, valid_loss, metric1, metric2, ...]\n",
    "            # `Recorder.log` usually stores [smooth_loss, valid_loss, metric1_val, ...]\n",
    "            # `Recorder.final_record` stores [train_loss_final, valid_loss_final, metric1_final,...]\n",
    "            # Accessing `rec.values[-1]` is the direct list being built.\n",
    "            # Index should correspond to metric_idx in metric_names\n",
    "            target_value_idx = metric_idx -1 # metric_names includes 'epoch', values doesn't in the list? No, index should match.\n",
    "\n",
    "            # Check if the current values list is long enough\n",
    "            current_values = rec.values[current_epoch_idx] # Values for the *current* epoch\n",
    "            expected_len = len(rec.metric_names) - 1 # Minus 'epoch'\n",
    "\n",
    "            # Get the metric value to inject\n",
    "            metric_val = float(self._last_metrics.get('mean_recall', 0.0))\n",
    "\n",
    "            try:\n",
    "                if len(current_values) > target_value_idx:\n",
    "                     # Value potentially already exists (maybe placeholder if Recorder added it)\n",
    "                     logger.info(f\"Updating recorder.values[{current_epoch_idx}][{target_value_idx}] ({metric_name}) with value {metric_val:.4f}.\")\n",
    "                     current_values[target_value_idx] = metric_val\n",
    "                elif len(current_values) == target_value_idx:\n",
    "                     # Append if it's exactly the next expected position\n",
    "                     logger.info(f\"Appending {metric_val:.4f} to recorder.values[{current_epoch_idx}] for {metric_name}.\")\n",
    "                     current_values.append(metric_val)\n",
    "                else:\n",
    "                    # The list is shorter than expected, log warning and append\n",
    "                    logger.warning(f\"Recorder values list for epoch {current_epoch_idx} has unexpected length {len(current_values)} (expected at least {target_value_idx}). Appending {metric_name} value defensively.\")\n",
    "                    current_values.append(metric_val)\n",
    "\n",
    "            except IndexError:\n",
    "                 logger.error(f\"IndexError accessing recorder.values[{current_epoch_idx}][{target_value_idx}]. Appending defensively.\")\n",
    "                 current_values.append(metric_val)\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Failed to inject {metric_name}: {e}\", exc_info=True)\n",
    "\n",
    "        # Clear metrics dict after processing on rank 0 or if not rank 0\n",
    "        self._last_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h_f8-rUoAm5H",
   "metadata": {},
   "source": [
    "## Example Usage Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x0FhJ3v2Am5H",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# How to use this callback with a fastai Learner:\n",
    "\n",
    "# from fastai.learner import Learner\n",
    "# from fastai.optimizer import Adam\n",
    "# from fastai.callback.wandb import WandbCallback\n",
    "# from fastai.callback.schedule import fit_one_cycle\n",
    "# from fastai.callback.fp16 import MixedPrecision\n",
    "# from fastai.callback.progress import ProgressCallback\n",
    "# from fastai.callback.core import SaveModelCallback\n",
    "\n",
    "# # Assume:\n",
    "# # - 'dls' is your DataLoaders object from 04_data_creation\n",
    "# # - 'model' is your IndicCLIP model instance from 07_model_clip\n",
    "# # - 'loss_func' is your ContrastiveLoss instance from 08_loss\n",
    "\n",
    "# # Instantiate the callback\n",
    "# retrieval_cb = RetrievalMetricCallback(k_values=[1, 5, 10])\n",
    "\n",
    "# # Add it to your list of callbacks\n",
    "# cbs = [\n",
    "#     # Other essential callbacks\n",
    "#     MixedPrecision(), # If using AMP\n",
    "#     ProgressCallback(),\n",
    "#     # WandbCallback(log_preds=False, log_model=True), # Ensure WandB is initialized\n",
    "#     SaveModelCallback(monitor='valid_mean_recall', comp=np.greater), # Monitor metric logged by RetrievalMetricCallback\n",
    "\n",
    "#     # Your custom callback\n",
    "#     retrieval_cb\n",
    "# ]\n",
    "\n",
    "# # Create the Learner\n",
    "# # Make sure to pass the metric function (even a dummy one) to Learner for the name to be registered\n",
    "# # fastai needs a function reference in metrics list for SaveModelCallback monitor to work correctly\n",
    "# def valid_mean_recall(logits=None, targets=None): return 0.0 # Dummy metric fn\n",
    "# learn = Learner(dls, model, loss_func=loss_func, opt_func=Adam, cbs=cbs, metrics=[valid_mean_recall])\n",
    "\n",
    "# # Start training\n",
    "# # learn.fit_one_cycle(n_epoch=5, lr_max=1e-4)\n",
    "\n",
    "# # During validation epochs, RetrievalMetricCallback will compute and log:\n",
    "# # valid_i2t_r_at_1, valid_t2i_r_at_5, valid_mean_recall etc. via learn.recorder.log\n",
    "# # These will appear in WandB logs (picked up by WandbCallback's after_epoch) and can be used by SaveModelCallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fLh6Q3HhAm5H",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Run this in terminal if using nbdev\n",
    "if __name__ == '__main__' and '__file__' not in globals(): # Only run if executing as script\n",
    "    try:\n",
    "        import nbdev.export\n",
    "        nbdev.export.nb_export('09_learner.ipynb', lib_path='./indic_clip', name='learner')\n",
    "        print(\"Exported 09_learner.ipynb to indic_clip/learner.py\")\n",
    "    except ImportError:\n",
    "        print(\"nbdev not found. Run 'pip install nbdev' and then 'nbdev_export' in terminal.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting notebook: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J91m0bQ5A0k9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
