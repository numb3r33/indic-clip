{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KlmF3Rkd_k51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oE5D6l3jAlUa",
   "metadata": {},
   "source": [
    "# Learner and Callbacks\n",
    "\n",
    "> Defines custom fast.ai Callbacks for Indic-CLIP training, specifically for calculating retrieval metrics during validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vP7oQx8TAm5A",
   "metadata": {},
   "source": [
    "## Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fT62U_f9Am5B",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Google Drive mounted successfully.\n",
      "Added /content/drive/MyDrive/Indic-Clip to sys.path\n",
      "Changed working directory to: /content/drive/MyDrive/Indic-Clip\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "# Mount Google Drive (Optional, but recommended for persistent storage)\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"Google Drive mounted successfully.\")\n",
    "    # Define PROJECT_ROOT for Colab\n",
    "    PROJECT_ROOT = Path('/content/drive/MyDrive/Indic-Clip') # Adjust path if needed\n",
    "    if not PROJECT_ROOT.exists():\n",
    "        print(f\"Warning: Project directory not found at {PROJECT_ROOT}. Please ensure it exists.\")\n",
    "    else:\n",
    "        # Add project root to sys.path\n",
    "        if str(PROJECT_ROOT) not in sys.path:\n",
    "            sys.path.insert(0, str(PROJECT_ROOT))\n",
    "            print(f\"Added {PROJECT_ROOT} to sys.path\")\n",
    "        # Change current working directory\n",
    "        os.chdir(PROJECT_ROOT)\n",
    "        print(f\"Changed working directory to: {os.getcwd()}\")\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Not running in Colab, skipping Drive mount.\")\n",
    "    # Define PROJECT_ROOT for local execution (adjust if needed)\n",
    "    PROJECT_ROOT = Path.cwd()\n",
    "    if PROJECT_ROOT.name == 'nbs': PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "    print(f\"Running locally. Project root assumed: {PROJECT_ROOT}\")\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "         sys.path.insert(0, str(PROJECT_ROOT))\n",
    "         print(f\"Added {PROJECT_ROOT} to sys.path\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Colab setup: {e}\")\n",
    "    PROJECT_ROOT = Path('.').resolve()\n",
    "    print(f\"Defaulting project root to current dir: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lX92w1HnAm5C",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Google Drive detected, setting PROJECT_ROOT to /content/drive/MyDrive/Indic-Clip\n",
      "Ensure your project files are located there.\n",
      "Imported indic_clip.core\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "# Use try-except for robustness, especially during development/export\n",
    "try:\n",
    "    import indic_clip.core\n",
    "    print(\"Imported indic_clip.core\")\n",
    "except ModuleNotFoundError:\n",
    "    print(\"indic_clip.core not found initially.\")\n",
    "    # Attempt to set sys.path if running in Colab and project cloned\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        project_parent = '/content' # Assuming cloned into /content/indic-clip\n",
    "        if Path('/content/drive/MyDrive/Indic-Clip').exists():\n",
    "             project_parent = '/content/drive/MyDrive/Indic-Clip'\n",
    "        if project_parent not in sys.path:\n",
    "             sys.path.insert(0, project_parent)\n",
    "             print(f\"Added {project_parent} to sys.path\")\n",
    "        try:\n",
    "            import indic_clip.core\n",
    "            print(\"Imported indic_clip.core after path adjustment.\")\n",
    "        except ModuleNotFoundError:\n",
    "            print(\"ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.\")\n",
    "            print(\"Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J5G3uH0JAm5D",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# !pip install -qr requirements.txt\n",
    "# !pip install scikit-learn # Make sure sklearn is installed for metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wc-l9Y0RAm5E",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9k5lFhOAm5E",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n",
      "Reloaded indic_clip.core\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.learner import Learner\n",
    "from fastai.data.load import DataLoader\n",
    "from fastcore.all import *\n",
    "from fastai.torch_core import * # For tensor()\n",
    "from fastai.text.all import *\n",
    "\n",
    "try:\n",
    "    from indic_clip.core import get_logger\n",
    "    from indic_clip.evaluation.metrics import calculate_retrieval_metrics, calculate_zeroshot_accuracy # Import actual functions\n",
    "    from indic_clip.loss import ContrastiveLoss\n",
    "except ModuleNotFoundError:\n",
    "    print(\"Could not import project modules in 09_learner.ipynb.\")\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger = logging.getLogger(__name__)\n",
    "    def get_logger(name): return logging.getLogger(name)\n",
    "    # Define stubs if import fails\n",
    "    def calculate_retrieval_metrics(*args, **kwargs): return {'mean_recall': 0.0, 'i2t_r@1': 0.0, 't2i_r@1': 0.0}\n",
    "    def calculate_zeroshot_accuracy(*args, **kwargs): return 0.0\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oP76v4Q2Am5F",
   "metadata": {},
   "source": [
    "## Retrieval Metric Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pU_C3K_oAm5F",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "# Removed the stub function as we should import the actual one from evaluation.metrics\n",
    "# If 11_evaluation_metrics.ipynb is not yet implemented, the try-except block above will define a basic stub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g7z3sLw3Am5G",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "_retrieval_metric_values = {}\n",
    "\n",
    "class RetrievalMetricCallback(Callback):\n",
    "    \"\"\"\n",
    "    1) Collects features during validation in `after_pred`.\n",
    "    2) Computes retrieval metrics in `after_validate`.\n",
    "    3) Stores results in `_retrieval_metric_values` for placeholder metrics.\n",
    "    \"\"\"\n",
    "    order = Recorder.order - 1 # Run before Recorder to get preds, but after model\n",
    "\n",
    "    def __init__(self, k_values=[1, 5, 10]):\n",
    "        store_attr()\n",
    "        self.img_feats_val = [] # Use instance attributes for collection\n",
    "        self.txt_feats_val = []\n",
    "\n",
    "    def before_validate(self):\n",
    "        \"\"\"Initialize feature lists before validation starts.\"\"\"\n",
    "        self.img_feats_val = []\n",
    "        self.txt_feats_val = []\n",
    "        logger.debug(\"RetrievalMetricCallback: Initialized lists for validation features.\")\n",
    "\n",
    "    def after_pred(self):\n",
    "        \"\"\"If validating, collect features from learn.pred.\"\"\"\n",
    "        # Check context: learn.training, learn.model.training, learn.dls.valid is active?\n",
    "        if not self.training and self.learn.pred is not None:\n",
    "            # self.learn.pred is the tuple (img_feat, txt_feat, logit_scale) from model.forward\n",
    "            try:\n",
    "                img_feat, txt_feat, _ = self.learn.pred # logit_scale not needed for metrics here\n",
    "                # Collect features on CPU to free GPU memory\n",
    "                self.img_feats_val.append(img_feat.cpu())\n",
    "                self.txt_feats_val.append(txt_feat.cpu())\n",
    "            except Exception as e:\n",
    "                 logger.error(f\"Error collecting features in RetrievalMetricCallback.after_pred: {e}\", exc_info=True)\n",
    "\n",
    "    # _gather_features method remains the same as provided before\n",
    "\n",
    "    def _gather_features(self, features_list):\n",
    "        \"Gathers features from all GPUs if in distributed mode.\"\n",
    "        # Ensure tensors are detached before gathering if they aren't already\n",
    "        detached_list = [t.detach() for t in features_list]\n",
    "\n",
    "        if dist.is_available() and dist.is_initialized():\n",
    "            world_size = dist.get_world_size()\n",
    "            if world_size <= 1: return detached_list\n",
    "\n",
    "            logger.debug(f\"Gathering features from {world_size} GPUs.\")\n",
    "            gathered_nested = [None] * world_size\n",
    "            try:\n",
    "                dist.all_gather_object(gathered_nested, detached_list)\n",
    "                all_features = []\n",
    "                for rank_list in gathered_nested:\n",
    "                    all_features.extend(rank_list)\n",
    "                logger.debug(f\"Gathered total {len(all_features)} feature tensors.\")\n",
    "                return all_features\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error during all_gather_object: {e}\", exc_info=True)\n",
    "                return []\n",
    "        else:\n",
    "            return detached_list\n",
    "\n",
    "\n",
    "    def after_validate(self):\n",
    "        \"\"\"Concatenate features, compute metrics, store in _retrieval_metric_values.\"\"\"\n",
    "        global _retrieval_metric_values # Modify the shared dict\n",
    "\n",
    "        logger.debug(\"RetrievalMetricCallback: Starting metric calculation in after_validate.\")\n",
    "\n",
    "        if not self.img_feats_val or not self.txt_feats_val:\n",
    "            logger.warning(\"No features collected during validation. Skipping retrieval metric calculation.\")\n",
    "            _retrieval_metric_values = {} # Clear previous results\n",
    "            return\n",
    "\n",
    "        logger.debug(f\"Collected {len(self.img_feats_val)} img batches, {len(self.txt_feats_val)} txt batches.\")\n",
    "\n",
    "        # --- Gather ---\n",
    "        all_img_features_gathered = self._gather_features(self.img_feats_val)\n",
    "        all_txt_features_gathered = self._gather_features(self.txt_feats_val)\n",
    "\n",
    "        # --- Calculate (only on rank 0) ---\n",
    "        is_rank_zero = not (dist.is_available() and dist.is_initialized()) or dist.get_rank() == 0\n",
    "        calculated_metrics = {}\n",
    "        if is_rank_zero:\n",
    "            if not all_img_features_gathered or not all_txt_features_gathered:\n",
    "                logger.warning(f\"Rank 0: No features after gathering. Skipping.\")\n",
    "            else:\n",
    "                try:\n",
    "                    device = next(self.learn.model.parameters()).device\n",
    "                    img_feats_all = torch.cat(all_img_features_gathered).to(device)\n",
    "                    txt_feats_all = torch.cat(all_txt_features_gathered).to(device)\n",
    "                    logger.debug(f\"Rank 0 concatenated features. Img: {img_feats_all.shape}, Txt: {txt_feats_all.shape}\")\n",
    "\n",
    "                    # Get Logit Scale (ensure model is on correct device)\n",
    "                    self.learn.model.to(device)\n",
    "                    if hasattr(self.learn.model, 'logit_scale'):\n",
    "                        with torch.no_grad():\n",
    "                            logit_scale = self.learn.model.logit_scale.exp().to(device) # Get current exp value\n",
    "                    else:\n",
    "                        logit_scale = torch.tensor(np.exp(np.log(1 / 0.07)), device=device)\n",
    "\n",
    "                    # Calculate Metrics\n",
    "                    calculated_metrics = calculate_retrieval_metrics(img_feats_all, txt_feats_all, logit_scale, self.k_values)\n",
    "                    logger.info(f\"Rank 0 calculated retrieval metrics: {calculated_metrics}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error during metric calculation on rank 0: {e}\", exc_info=True)\n",
    "                    calculated_metrics = {}\n",
    "\n",
    "        # --- Update shared dict and clean up ---\n",
    "        _retrieval_metric_values = calculated_metrics # Update shared dict\n",
    "        self.img_feats_val = [] # Clean up instance lists\n",
    "        self.txt_feats_val = []\n",
    "        logger.debug(f\"Cleaned up feature lists on rank {dist.get_rank() if dist.is_initialized() else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h_f8-rUoAm5H",
   "metadata": {},
   "source": [
    "## Example Usage Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x0FhJ3v2Am5H",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "\n",
    "# How to use this callback with a fastai Learner:\n",
    "\n",
    "# from fastai.learner import Learner\n",
    "# from fastai.optimizer import Adam\n",
    "# from fastai.callback.wandb import WandbCallback\n",
    "# from fastai.callback.schedule import fit_one_cycle\n",
    "# from fastai.callback.fp16 import MixedPrecision\n",
    "# from fastai.callback.progress import ProgressCallback\n",
    "# from fastai.callback.core import SaveModelCallback\n",
    "\n",
    "# # Assume:\n",
    "# # - 'dls' is your DataLoaders object from 04_data_creation\n",
    "# # - 'model' is your IndicCLIP model instance from 07_model_clip\n",
    "# # - 'loss_func' is your ContrastiveLoss instance from 08_loss\n",
    "\n",
    "# # Instantiate the callback\n",
    "# retrieval_cb = RetrievalMetricCallback(k_values=[1, 5, 10])\n",
    "\n",
    "# # Add it to your list of callbacks\n",
    "# cbs = [\n",
    "#     # Other essential callbacks\n",
    "#     MixedPrecision(), # If using AMP\n",
    "#     ProgressCallback(),\n",
    "#     # WandbCallback(log_preds=False, log_model=True), # Ensure WandB is initialized\n",
    "#     SaveModelCallback(monitor='valid_mean_recall', comp=np.greater), # Monitor metric logged by RetrievalMetricCallback\n",
    "\n",
    "#     # Your custom callback\n",
    "#     retrieval_cb\n",
    "# ]\n",
    "\n",
    "# # Create the Learner\n",
    "# # Make sure to pass the metric function (even a dummy one) to Learner for the name to be registered\n",
    "# # fastai needs a function reference in metrics list for SaveModelCallback monitor to work correctly\n",
    "# def valid_mean_recall(logits=None, targets=None): return 0.0 # Dummy metric fn\n",
    "# learn = Learner(dls, model, loss_func=loss_func, opt_func=Adam, cbs=cbs, metrics=[valid_mean_recall])\n",
    "\n",
    "# # Start training\n",
    "# # learn.fit_one_cycle(n_epoch=5, lr_max=1e-4)\n",
    "\n",
    "# # During validation epochs, RetrievalMetricCallback will compute and log:\n",
    "# # valid_i2t_r_at_1, valid_t2i_r_at_5, valid_mean_recall etc. via learn.recorder.log\n",
    "# # These will appear in WandB logs (picked up by WandbCallback's after_epoch) and can be used by SaveModelCallback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fLh6Q3HhAm5H",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# Run this in terminal if using nbdev\n",
    "if __name__ == '__main__' and '__file__' not in globals(): # Only run if executing as script\n",
    "    try:\n",
    "        import nbdev.export\n",
    "        nbdev.export.nb_export('09_learner.ipynb', lib_path='./indic_clip', name='learner')\n",
    "        print(\"Exported 09_learner.ipynb to indic_clip/learner.py\")\n",
    "    except ImportError:\n",
    "        print(\"nbdev not found. Run 'pip install nbdev' and then 'nbdev_export' in terminal.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exporting notebook: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "J91m0bQ5A0k9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
