{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f474b98d-9e21-45e8-8f5a-b34846202881",
   "metadata": {},
   "source": [
    "# Gradio Demo Application for Indic-CLIP\n",
    "\n",
    "> Provides an interactive interface for testing the Indic-CLIP model using Gradio, deployable to Hugging Face Spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790b9475-898e-4e71-abcb-270324f09012",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "cell_style": "code",
   "execution_count": null,
   "id": "2409753b-1b32-4f8f-a77c-17420e07d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# This cell is primarily for running in Colab or similar environments.\n",
    "# Make sure the project library and its dependencies are installed.\n",
    "\n",
    "# !pip install -q gradio torch torchvision torchaudio fastai transformers timm sentencepiece Pillow imagehash scikit-learn indic-nlp-library\n",
    "\n",
    "# Mount Google Drive (Optional, adjust PROJECT_ROOT if not using Drive)\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PROJECT_ROOT = Path('/content/drive/MyDrive/Indic-Clip') # Adjust path if needed\n",
    "    # Add project root to Python path\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    # Change current directory (optional, but can help with relative paths)\n",
    "    # os.chdir(PROJECT_ROOT)\n",
    "    print(f\"Running in Colab. Project path added: {PROJECT_ROOT}\")\n",
    "else:\n",
    "    # Assume standard nbdev structure if not in Colab\n",
    "    # Find the project root assuming 'nbs' is the current directory's parent\n",
    "    if Path.cwd().name == 'nbs':\n",
    "        PROJECT_ROOT = Path.cwd().parent\n",
    "    else:\n",
    "        PROJECT_ROOT = Path.cwd() # Assume current dir is root if not in nbs\n",
    "    if str(PROJECT_ROOT) not in sys.path:\n",
    "        sys.path.insert(0, str(PROJECT_ROOT))\n",
    "    print(f\"Running locally. Project root assumed: {PROJECT_ROOT}\")\n",
    "\n",
    "# Ensure PROJECT_ROOT is correctly defined before proceeding\n",
    "if 'PROJECT_ROOT' not in locals():\n",
    "   PROJECT_ROOT = Path(\".\").resolve()\n",
    "   print(f\"Warning: PROJECT_ROOT not set by environment checks, defaulting to {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "cell_style": "code",
   "execution_count": null,
   "id": "803f2c42-d0a7-4470-b7a0-503111177d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Project specific imports\n",
    "from indic_clip.inference import (\n",
    "    load_indic_clip_model,\n",
    "    extract_image_features,\n",
    "    extract_text_features,\n",
    "    compute_similarity\n",
    ")\n",
    "from indic_clip.core import (\n",
    "    get_logger,\n",
    "    setup_logging,\n",
    "    CHECKPOINT_PATH,\n",
    "    TOKENIZER_PATH,\n",
    "    PRETRAINED_TOKENIZER_NAME, # Default text model name\n",
    "    DEFAULT_EMBED_DIM, # Default embedding dim\n",
    "    DEFAULT_IMAGE_SIZE,\n",
    "    HINDI_RAW_PATH # For locating sample images\n",
    ")\n",
    "from indic_clip.data.tokenization import IndicBERTTokenizer\n",
    "# Import IndicCLIP class directly for type hinting, even though we load via helper\n",
    "from indic_clip.model.clip import IndicCLIP\n",
    "\n",
    "# Setup logging\n",
    "setup_logging()\n",
    "logger = get_logger(\"indic_clip_app\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b58b9a3-ed0f-4e29-a630-430859d516f9",
   "metadata": {},
   "source": [
    "## Configuration and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "cell_style": "code",
   "execution_count": null,
   "id": "1ef980fd-e050-46f0-8b32-f36c0bd2b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Point to the specific checkpoint you want to use for the demo\n",
    "# This should ideally be the best performing model from your training runs.\n",
    "CHECKPOINT_FILENAME = 'best_valid_loss.pth' # Or 'best_recall.pth', etc.\n",
    "CHECKPOINT_FILE_PATH = CHECKPOINT_PATH / CHECKPOINT_FILENAME\n",
    "TOKENIZER_DIR_PATH = TOKENIZER_PATH\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = DEFAULT_IMAGE_SIZE # Use image size consistent with training\n",
    "TOP_K = 5 # Number of results to show for retrieval\n",
    "\n",
    "# --- !!! IMPORTANT: Model Configuration !!! ---\n",
    "# This configuration MUST match the parameters used to train the checkpoint being loaded.\n",
    "# If the checkpoint was saved using `learn.save` without `with_opt=False`,\n",
    "# the state dict might be nested. `load_indic_clip_model` tries to handle this,\n",
    "# but providing the correct instantiation config is crucial.\n",
    "# Using the config from 10_training.ipynb example run:\n",
    "MODEL_CONFIGURATION = {\n",
    "    'embed_dim': 512, # From training example\n",
    "    'vision_model_name': 'resnet50', # From training example\n",
    "    'vision_pretrained': False, # Pretrained flag doesn't matter for loading weights\n",
    "    'text_model_name': PRETRAINED_TOKENIZER_NAME, # Default, assumed from training\n",
    "    'text_pretrained': False,\n",
    "    # The tokenizer instance will be loaded separately below and passed during load_model\n",
    "    'tokenizer': None # Placeholder, will be loaded next\n",
    "}\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "try:\n",
    "    tokenizer = IndicBERTTokenizer.load_tokenizer(TOKENIZER_DIR_PATH)\n",
    "    MODEL_CONFIGURATION['tokenizer'] = tokenizer # Add loaded tokenizer to config\n",
    "    logger.info(f\"Tokenizer loaded successfully from {TOKENIZER_DIR_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    logger.error(f\"Tokenizer directory not found at {TOKENIZER_DIR_PATH}. Cannot start app.\")\n",
    "    tokenizer = None # Ensure tokenizer is None if loading failed\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading tokenizer: {e}\", exc_info=True)\n",
    "    tokenizer = None\n",
    "\n",
    "# --- Load Model ---\n",
    "model: IndicCLIP = None\n",
    "if tokenizer is not None:\n",
    "    try:\n",
    "        if not CHECKPOINT_FILE_PATH.exists():\n",
    "            logger.error(f\"Checkpoint file not found: {CHECKPOINT_FILE_PATH}. Cannot load model.\")\n",
    "        else:\n",
    "            model = load_indic_clip_model(\n",
    "                checkpoint_path=CHECKPOINT_FILE_PATH,\n",
    "                model_config=MODEL_CONFIGURATION,\n",
    "                device=DEVICE\n",
    "            )\n",
    "            logger.info(f\"Model loaded successfully from {CHECKPOINT_FILE_PATH} to device {DEVICE}.\")\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Error: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model: {e}\", exc_info=True)\n",
    "\n",
    "if model is None:\n",
    "    logger.critical(\"Model could not be loaded. The application cannot function.\")\n",
    "    # Optional: raise an exception or exit if model loading is critical\n",
    "    # raise RuntimeError(\"Failed to load IndicCLIP model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2eb22e-f0d8-46ba-a340-9768d5f6238f",
   "metadata": {},
   "source": [
    "## Sample Gallery Data (for Demo)"
   ]
  },
  {
   "cell_type": "code",
   "cell_style": "code",
   "execution_count": null,
   "id": "8c91185f-a3a8-439b-9922-ff9ac8080a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, we define a small, fixed gallery here.\n",
    "# A real application would load these from a file or database.\n",
    "\n",
    "# --- Text Gallery for Image-to-Text Retrieval ---\n",
    "TEXT_GALLERY = [\n",
    "    \"‡§è‡§ï ‡§≤‡§°‡§º‡§ï‡§æ ‡§´‡•Å‡§ü‡§¨‡•â‡§≤ ‡§ñ‡•á‡§≤ ‡§∞‡§π‡§æ ‡§π‡•à‡•§\",\n",
    "    \"‡§∏‡§Æ‡•Å‡§¶‡•ç‡§∞ ‡§§‡§ü ‡§™‡§∞ ‡§∏‡•Ç‡§∞‡•ç‡§Ø‡§æ‡§∏‡•ç‡§§‡•§\",\n",
    "    \"‡§è‡§ï ‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä ‡§∏‡•ã‡§´‡•á ‡§™‡§∞ ‡§∏‡•ã ‡§∞‡§π‡•Ä ‡§π‡•à‡•§\",\n",
    "    \"‡§™‡§æ‡§∞‡§Ç‡§™‡§∞‡§ø‡§ï ‡§∏‡§æ‡§°‡§º‡•Ä ‡§™‡§π‡§®‡•á ‡§è‡§ï ‡§Æ‡§π‡§ø‡§≤‡§æ‡•§\",\n",
    "    \"‡§è‡§ï ‡§Æ‡§Ç‡§¶‡§ø‡§∞ ‡§ï‡§æ ‡§™‡•ç‡§∞‡§µ‡•á‡§∂ ‡§¶‡•ç‡§µ‡§æ‡§∞‡•§\",\n",
    "    \"‡§¶‡•á‡§µ‡§§‡§æ ‡§ó‡§£‡•á‡§∂ ‡§ï‡•Ä ‡§Æ‡•Ç‡§∞‡•ç‡§§‡§ø‡•§\",\n",
    "    \"‡§è‡§ï ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§¨‡§æ‡§ú‡§º‡§æ‡§∞‡•§\",\n",
    "    \"‡§≤‡•à‡§™‡§ü‡•â‡§™ ‡§™‡§∞ ‡§ï‡§æ‡§Æ ‡§ï‡§∞‡§§‡§æ ‡§π‡•Å‡§Ü ‡§µ‡•ç‡§Ø‡§ï‡•ç‡§§‡§ø‡•§\",\n",
    "    \"‡§Æ‡•á‡§ú‡§º ‡§™‡§∞ ‡§∞‡§ñ‡•Ä ‡§ï‡§ø‡§§‡§æ‡§¨‡•ã‡§Ç ‡§ï‡§æ ‡§¢‡•á‡§∞‡•§\",\n",
    "    \"‡§è‡§ï ‡§≤‡§æ‡§≤ ‡§∞‡§Ç‡§ó ‡§ï‡•Ä ‡§∏‡•ç‡§™‡•ã‡§∞‡•ç‡§ü‡•ç‡§∏ ‡§ï‡§æ‡§∞‡•§\"\n",
    "]\n",
    "\n",
    "# --- Image Gallery for Text-to-Image Retrieval ---\n",
    "# Assumes these images exist relative to the project root or Colab environment\n",
    "# Ideally, copy these samples into a 'data/samples' directory\n",
    "SAMPLE_IMAGE_DIR = PROJECT_ROOT / 'data' / 'samples'\n",
    "IMAGE_GALLERY_FILES = [\n",
    "    SAMPLE_IMAGE_DIR / 'cat.jpg', # Needs to exist\n",
    "    SAMPLE_IMAGE_DIR / 'dog_park.jpg',\n",
    "    SAMPLE_IMAGE_DIR / 'sunset_beach.jpg',\n",
    "    SAMPLE_IMAGE_DIR / 'woman_saree.jpg',\n",
    "    SAMPLE_IMAGE_DIR / 'temple.jpg',\n",
    "    SAMPLE_IMAGE_DIR / 'ganesh.jpg',\n",
    "    SAMPLE_IMAGE_DIR / 'market.jpg',\n",
    "    SAMPLE_IMAGE_DIR / 'laptop.jpg',\n",
    "    SAMPLE_IMAGE_DIR / 'books.jpg',\n",
    "    SAMPLE_IMAGE_DIR / 'car.jpg'\n",
    "]\n",
    "\n",
    "# Ensure sample image directory exists\n",
    "SAMPLE_IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Filter out images that don't exist\n",
    "valid_image_gallery_files = [f for f in IMAGE_GALLERY_FILES if f.exists()]\n",
    "if len(valid_image_gallery_files) < len(IMAGE_GALLERY_FILES):\n",
    "    logger.warning(f\"Missing some sample images. Found {len(valid_image_gallery_files)} out of {len(IMAGE_GALLERY_FILES)} expected in {SAMPLE_IMAGE_DIR}\")\n",
    "    IMAGE_GALLERY_FILES = valid_image_gallery_files\n",
    "\n",
    "# Pre-encode gallery features (optional, but improves demo speed)\n",
    "text_gallery_features: torch.Tensor = None\n",
    "image_gallery_features: torch.Tensor = None\n",
    "\n",
    "if model is not None and tokenizer is not None:\n",
    "    try:\n",
    "        logger.info(\"Pre-encoding text gallery features...\")\n",
    "        text_gallery_features = extract_text_features(model, tokenizer, TEXT_GALLERY, device=DEVICE)\n",
    "        logger.info(f\"Encoded {len(TEXT_GALLERY)} text gallery items.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to pre-encode text gallery: {e}\", exc_info=True)\n",
    "\n",
    "    if valid_image_gallery_files:\n",
    "        try:\n",
    "            logger.info(\"Pre-encoding image gallery features...\")\n",
    "            # Pass file paths to extract_image_features\n",
    "            image_gallery_features = extract_image_features(model, valid_image_gallery_files, img_size=IMAGE_SIZE, device=DEVICE)\n",
    "            logger.info(f\"Encoded {len(valid_image_gallery_files)} image gallery items.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to pre-encode image gallery: {e}\", exc_info=True)\n",
    "else:\n",
    "    logger.warning(\"Model or tokenizer not loaded, cannot pre-encode gallery features.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18741624-f259-4435-8a39-277b92622f3e",
   "metadata": {},
   "source": [
    "## Gradio Interface Functions"
   ]
  },
  {
   "cell_type": "code",
   "cell_style": "code",
   "execution_count": null,
   "id": "f359980d-60e8-4c2c-aeab-62b71a10746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text_from_image(image_input: Image.Image) -> str:\n",
    "    \"\"\"Gradio interface function for Image-to-Text retrieval.\"\"\"\n",
    "    if model is None or tokenizer is None or text_gallery_features is None:\n",
    "        return \"Error: Model, tokenizer, or text gallery features not loaded.\"\n",
    "    if image_input is None:\n",
    "        return \"Error: Please upload an image.\"\n",
    "\n",
    "    try:\n",
    "        # 1. Encode the input image\n",
    "        # extract_image_features handles PIL image input directly\n",
    "        img_feat = extract_image_features(model, image_input, img_size=IMAGE_SIZE, device=DEVICE)\n",
    "\n",
    "        # 2. Compute similarity with pre-encoded text gallery features\n",
    "        # compute_similarity expects normalized features\n",
    "        similarity = compute_similarity(model, img_feat, text_gallery_features)\n",
    "\n",
    "        # 3. Get top K results\n",
    "        scores, indices = torch.topk(similarity.squeeze(0), k=min(TOP_K, len(TEXT_GALLERY)), dim=-1)\n",
    "\n",
    "        # 4. Format results\n",
    "        results = \"\\n\".join([f\"{scores[i].item():.4f}: {TEXT_GALLERY[indices[i].item()]}\" for i in range(len(indices))])\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in predict_text_from_image: {e}\", exc_info=True)\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "def predict_image_from_text(text_input: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Gradio interface function for Text-to-Image retrieval.\"\"\"\n",
    "    if model is None or tokenizer is None or image_gallery_features is None or not valid_image_gallery_files:\n",
    "        # Gradio expects a list for Gallery output, even on error\n",
    "        return [(\"error.png\", \"Error: Model, tokenizer, or image gallery features not loaded.\")]\n",
    "    if not text_input or not text_input.strip():\n",
    "        return [(\"error.png\", \"Error: Please enter text.\")]\n",
    "\n",
    "    try:\n",
    "        # 1. Encode the input text\n",
    "        txt_feat = extract_text_features(model, tokenizer, text_input, device=DEVICE)\n",
    "\n",
    "        # 2. Compute similarity with pre-encoded image gallery features\n",
    "        similarity = compute_similarity(model, image_gallery_features, txt_feat) # Note order for T2I\n",
    "\n",
    "        # 3. Get top K results\n",
    "        scores, indices = torch.topk(similarity.squeeze(-1), k=min(TOP_K, len(valid_image_gallery_files)), dim=0)\n",
    "\n",
    "        # 4. Format results for Gradio Gallery (List of tuples: (image_path, caption))\n",
    "        results = []\n",
    "        for i in range(len(indices)):\n",
    "            img_index = indices[i].item()\n",
    "            img_path = valid_image_gallery_files[img_index]\n",
    "            score = scores[i].item()\n",
    "            # Use filename as caption, or potentially retrieve original text if mapped\n",
    "            caption = f\"{score:.4f}: {img_path.name}\"\n",
    "            # Gradio needs string paths for local files\n",
    "            results.append((str(img_path), caption))\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in predict_image_from_text: {e}\", exc_info=True)\n",
    "        return [(\"error.png\", f\"An error occurred: {e}\")]\n",
    "\n",
    "def predict_zero_shot(image_input: Image.Image, candidate_labels_text: str) -> Dict[str, float]:\n",
    "    \"\"\"Gradio interface function for Zero-Shot Classification.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        return {\"Error\": 1.0, \"Message\": \"Model or tokenizer not loaded.\"}\n",
    "    if image_input is None:\n",
    "        return {\"Error\": 1.0, \"Message\": \"Please upload an image.\"}\n",
    "    if not candidate_labels_text or not candidate_labels_text.strip():\n",
    "        return {\"Error\": 1.0, \"Message\": \"Please enter candidate labels.\"}\n",
    "\n",
    "    try:\n",
    "        # Parse labels\n",
    "        class_names = [label.strip() for label in candidate_labels_text.split(',') if label.strip()]\n",
    "        if not class_names:\n",
    "            return {\"Error\": 1.0, \"Message\": \"Invalid label format. Enter comma-separated labels.\"}\n",
    "\n",
    "        # Use default English templates for simplicity in this demo\n",
    "        # A more advanced demo could detect language or allow template selection\n",
    "        templates = DEFAULT_PROMPT_TEMPLATES_EN\n",
    "\n",
    "        # Encode image\n",
    "        img_feat = extract_image_features(model, image_input, img_size=IMAGE_SIZE, device=DEVICE)\n",
    "\n",
    "        # Generate and encode text prompts for all classes\n",
    "        all_prompts = []\n",
    "        for template in templates:\n",
    "            for classname in class_names:\n",
    "                all_prompts.append(template.format(classname))\n",
    "\n",
    "        # Use extract_text_features which handles batching internally\n",
    "        text_embeddings = extract_text_features(model, tokenizer, all_prompts, device=DEVICE)\n",
    "\n",
    "        # Average embeddings if multiple templates were used\n",
    "        if len(templates) > 1:\n",
    "            num_classes = len(class_names)\n",
    "            text_embeddings = text_embeddings.view(len(templates), num_classes, -1).mean(dim=0)\n",
    "\n",
    "        # Normalize final text embeddings (should already be normalized by extract_text_features, but safe to redo)\n",
    "        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)\n",
    "\n",
    "        # Compute similarity (image features should also be normalized)\n",
    "        similarity = compute_similarity(model, img_feat, text_embeddings).squeeze()\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probs = F.softmax(similarity, dim=-1)\n",
    "\n",
    "        # Format results for Gradio Label output\n",
    "        results = {class_names[i]: probs[i].item() for i in range(len(class_names))}\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in predict_zero_shot: {e}\", exc_info=True)\n",
    "        return {\"Error\": 1.0, \"Message\": f\"An error occurred: {e}\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09a7edb-c068-4219-9f68-7b629e08055b",
   "metadata": {},
   "source": [
    "## Gradio Interface Definition"
   ]
  },
  {
   "cell_type": "code",
   "cell_style": "code",
   "execution_count": null,
   "id": "f90236c2-ff81-4732-97de-c936f34cc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "css = \"\"\"\n",
    ".gradio-container { font-family: 'IBM Plex Sans', sans-serif; }\n",
    ".gr-button { color: white; border-color: black; background: black; }\n",
    "input[type='range'] { accent-color: black; }\n",
    ".dark input[type='range'] { accent-color: #dfdqdq; }\n",
    ".container { max-width: 1100px; margin: auto; padding-top: 1.5rem; }\n",
    "#gallery { min-height: 22rem; margin-bottom: 15px; margin-left: auto; margin-right: auto; }\n",
    "#gallery>div>.h-full { min-height: 20rem; }\n",
    ".details:hover { text-decoration: underline; }\n",
    ".feedback { font-size: 0.8rem; margin-bottom: 5px; }\n",
    ".feedback textarea { font-size: 0.8rem; }\n",
    ".feedback button { margin: 0; }\n",
    ".gradio-container { max-width: 1140px !important; }\n",
    "\"\"\"\n",
    "\n",
    "block = gr.Blocks(css=css, theme=gr.themes.Default())\n",
    "\n",
    "with block:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        <div style=\"text-align: center; max-width: 1000px; margin: 20px auto;\">\n",
    "        <h1 style=\"font-weight: 900; font-size: 3rem;\">\n",
    "            Indic-CLIP\n",
    "        </h1>\n",
    "        <p style=\"margin-bottom: 10px; font-size: 94%\">\n",
    "            Multimodal Vision-Language Model for Indic Languages (Hindi/Sanskrit)\n",
    "         </p>\n",
    "         <p>Provide an image or text to retrieve corresponding matches, or perform zero-shot classification.</p>\n",
    "         <p>Note: This demo uses a small, fixed gallery for retrieval. Model trained on Flickr8k-Hindi (example).</p>\n",
    "       </div>\n",
    "        \"\"\"\n",
    "    )\n",
    "    with gr.Tabs():\n",
    "        with gr.TabItem(\"üñºÔ∏è Image-to-Text Retrieval\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    input_image = gr.Image(type=\"pil\", label=\"Input Image\")\n",
    "                    submit_i2t = gr.Button(\"Retrieve Text\", variant=\"primary\")\n",
    "                with gr.Column():\n",
    "                    output_text = gr.Textbox(lines=TOP_K, label=f\"Top {TOP_K} Text Matches (Score: Text)\")\n",
    "            gr.Examples(\n",
    "                examples=[os.path.join(SAMPLE_IMAGE_DIR, fn.name) for fn in IMAGE_GALLERY_FILES[:min(3, len(IMAGE_GALLERY_FILES))]],\n",
    "                inputs=input_image,\n",
    "                label=\"Sample Images\"\n",
    "            )\n",
    "\n",
    "        with gr.TabItem(\"üìù Text-to-Image Retrieval\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    input_text = gr.Textbox(label=\"Input Text (Hindi/Sanskrit)\")\n",
    "                    submit_t2i = gr.Button(\"Retrieve Images\", variant=\"primary\")\n",
    "                with gr.Column():\n",
    "                    # Gallery output expects list of (image_path, caption) tuples\n",
    "                    output_gallery = gr.Gallery(label=f\"Top {TOP_K} Image Matches (Score: Filename)\", show_label=True).style(columns=TOP_K, height=\"auto\", object_fit=\"contain\")\n",
    "            gr.Examples(\n",
    "                examples=TEXT_GALLERY[:min(3, len(TEXT_GALLERY))],\n",
    "                inputs=input_text,\n",
    "                label=\"Sample Texts\"\n",
    "            )\n",
    "\n",
    "        with gr.TabItem(\"üè∑Ô∏è Zero-Shot Classification\"):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    input_image_zs = gr.Image(type=\"pil\", label=\"Input Image\")\n",
    "                with gr.Column():\n",
    "                    candidate_labels = gr.Textbox(label=\"Candidate Labels (Comma-separated)\", placeholder=\"e.g., ‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä, ‡§ï‡•Å‡§§‡•ç‡§§‡§æ, ‡§™‡§ï‡•ç‡§∑‡•Ä, ‡§ï‡§æ‡§∞\")\n",
    "                    submit_zs = gr.Button(\"Classify Image\", variant=\"primary\")\n",
    "                    output_labels = gr.Label(num_top_classes=3, label=\"Classification Results\")\n",
    "            gr.Examples(\n",
    "                examples=[\n",
    "                    [os.path.join(SAMPLE_IMAGE_DIR, IMAGE_GALLERY_FILES[0].name), \"‡§¨‡§ø‡§≤‡•ç‡§≤‡•Ä, ‡§ï‡•Å‡§§‡•ç‡§§‡§æ, ‡§™‡§ï‡•ç‡§∑‡•Ä\"], # Cat example\n",
    "                    [os.path.join(SAMPLE_IMAGE_DIR, IMAGE_GALLERY_FILES[3].name), \"‡§∏‡§æ‡§°‡§º‡•Ä, ‡§ï‡•Å‡§∞‡•ç‡§§‡§æ, ‡§™‡•ã‡§∂‡§æ‡§ï\"], # Saree example\n",
    "                    [os.path.join(SAMPLE_IMAGE_DIR, IMAGE_GALLERY_FILES[4].name), \"‡§Æ‡§Ç‡§¶‡§ø‡§∞, ‡§Æ‡§∏‡•ç‡§ú‡§ø‡§¶, ‡§ö‡§∞‡•ç‡§ö\"], # Temple example\n",
    "                ],\n",
    "                inputs=[input_image_zs, candidate_labels],\n",
    "                outputs=output_labels,\n",
    "                label=\"Sample Images and Labels\"\n",
    "            )\n",
    "\n",
    "    # Define button click actions\n",
    "    submit_i2t.click(\n",
    "        predict_text_from_image,\n",
    "        inputs=[input_image],\n",
    "        outputs=[output_text]\n",
    "    )\n",
    "    submit_t2i.click(\n",
    "        predict_image_from_text,\n",
    "        inputs=[input_text],\n",
    "        outputs=[output_gallery]\n",
    "    )\n",
    "    submit_zs.click(\n",
    "        predict_zero_shot,\n",
    "        inputs=[input_image_zs, candidate_labels],\n",
    "        outputs=[output_labels]\n",
    "    )\n",
    "\n",
    "    # Launch the interface\n",
    "    # block.launch(debug=True) # Use debug=True for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f0ae2-211f-4826-b263-6784c971b361",
   "metadata": {},
   "source": [
    "## Launch App"
   ]
  },
  {
   "cell_type": "code",
   "cell_style": "code",
   "execution_count": null,
   "id": "e8cf0148-5e61-499b-a2e5-0615423ddc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "# The launch command should ideally be the last cell executed\n",
    "# or run in the app.py script when deploying to Spaces.\n",
    "if __name__ == '__main__' and 'google.colab' not in sys.modules:\n",
    "    print(\"Launching Gradio interface...\")\n",
    "    # Check if model loaded before launching\n",
    "    if model is not None:\n",
    "        block.launch(share=False) # share=True to create public link\n",
    "    else:\n",
    "        print(\"ERROR: Model not loaded. Cannot launch Gradio app.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}