"""Basic utilities, constants, and shared functions for the Indic-CLIP project. Adapted for potential Colab usage."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_core.ipynb.

# %% auto 0
__all__ = ['IN_COLAB', 'PROJECT_ROOT', 'DATA_PATH', 'RAW_DATA_PATH', 'HINDI_RAW_PATH', 'SANSKRIT_RAW_PATH', 'SYNTHETIC_RAW_PATH',
           'PROCESSED_DATA_PATH', 'BENCHMARK_DATA_PATH', 'MODEL_PATH', 'CHECKPOINT_PATH', 'ONNX_PATH', 'QUANTIZED_PATH',
           'PRETRAINED_TOKENIZER_NAME', 'TOKENIZER_PATH', 'PAD_TOKEN', 'UNK_TOKEN', 'CLS_TOKEN', 'SEP_TOKEN',
           'MASK_TOKEN', 'SANSKRIT_TOKEN', 'HINDI_TOKEN', 'CUSTOM_SPECIAL_TOKENS', 'DEFAULT_IMAGE_SIZE',
           'DEFAULT_BATCH_SIZE', 'DEFAULT_EMBED_DIM', 'setup_logging', 'get_logger', 'ensure_dir']

# %% ../nbs/00_core.ipynb 4
import os
import sys
from pathlib import Path
import logging

# %% ../nbs/00_core.ipynb 6
def setup_logging(level=logging.INFO):
    """Configures basic logging for the project."""
    # Remove existing handlers if any to avoid duplicate logs in interactive environments
    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S',
        # stream=sys.stdout # Ensure logs go to stdout, useful in Colab
    )

def get_logger(name):
    """Returns a logger instance."""
    return logging.getLogger(name)

def ensure_dir(path: Path):
    """Ensure a directory exists, creating it if necessary."""
    path.mkdir(parents=True, exist_ok=True)

# %% ../nbs/00_core.ipynb 8
# --- Project Root (Colab compatibility) ---
IN_COLAB = 'google.colab' in sys.modules

if IN_COLAB:
    # Assume project is cloned into /content/Indic-Clip or Drive is mounted
    # If using Drive, adjust this path accordingly e.g., Path('/content/drive/MyDrive/Indic-Clip')
    # Check if Google Drive is mounted
    from google.colab import drive
    drive.mount('/content/drive')

    if Path('/content/drive/MyDrive').exists():
       DEFAULT_ROOT = Path('/content/drive/MyDrive/Indic-Clip')
       print("Google Drive detected, setting PROJECT_ROOT to /content/drive/MyDrive/Indic-Clip")
       print("Ensure your project files are located there.")
    else:
       DEFAULT_ROOT = Path('/content/Indic-Clip')
       print("Google Drive not detected. Setting PROJECT_ROOT to /content/Indic-Clip")
       print("Ensure your project files are located there (e.g., via !git clone).")
else:
    # Default for local execution: use env var or parent of current dir
    DEFAULT_ROOT = Path(os.getenv("INDIC_CLIP_ROOT", default=Path.cwd()))
    if DEFAULT_ROOT.name == 'nbs':
        DEFAULT_ROOT = DEFAULT_ROOT.parent

PROJECT_ROOT = DEFAULT_ROOT

# --- Data Paths ---
DATA_PATH = PROJECT_ROOT / 'data'
RAW_DATA_PATH = DATA_PATH / 'raw'
HINDI_RAW_PATH = RAW_DATA_PATH / 'hindi'
SANSKRIT_RAW_PATH = RAW_DATA_PATH / 'sanskrit' # Placeholder
SYNTHETIC_RAW_PATH = RAW_DATA_PATH / 'synthetic' # Placeholder
PROCESSED_DATA_PATH = DATA_PATH / 'processed'
BENCHMARK_DATA_PATH = DATA_PATH / 'benchmarks'

# --- Model Paths ---
MODEL_PATH = PROJECT_ROOT / 'models'
CHECKPOINT_PATH = MODEL_PATH / 'checkpoints'
ONNX_PATH = MODEL_PATH / 'onnx'
QUANTIZED_PATH = MODEL_PATH / 'quantized'

# --- Tokenizer Info ---
# We are using a pre-trained tokenizer from Hugging Face
PRETRAINED_TOKENIZER_NAME = "ai4bharat/indic-bert"
TOKENIZER_PATH = MODEL_PATH / 'tokenizer' # Directory to save tokenizer config if needed

# --- Special Tokens (Check compatibility with pre-trained tokenizer) ---
# Standard tokens handled by HF tokenizer: [PAD], [UNK], [CLS], [SEP], [MASK]
PAD_TOKEN = "[PAD]" # Typically handled by HF Tokenizer config
UNK_TOKEN = "[UNK]" # Typically handled by HF Tokenizer config
CLS_TOKEN = "[CLS]" # Typically handled by HF Tokenizer config
SEP_TOKEN = "[SEP]" # Typically handled by HF Tokenizer config
MASK_TOKEN = "[MASK]" # Typically handled by HF Tokenizer config

# Custom language tokens (May need to be added to HF tokenizer vocabulary manually if used)
# IndicBERT might implicitly handle language via context or specific training.
# Check if these are strictly necessary for the chosen architecture.
SANSKRIT_TOKEN = "<Sa>"
HINDI_TOKEN = "<Hi>"

# List of *custom* special tokens potentially needing addition
CUSTOM_SPECIAL_TOKENS = [SANSKRIT_TOKEN, HINDI_TOKEN]

# --- Default Values ---
DEFAULT_IMAGE_SIZE = 224
DEFAULT_BATCH_SIZE = 64 # Adjust based on GPU memory (especially in Colab)
# DEFAULT_VOCAB_SIZE = 32000 # No longer needed, vocab size determined by pre-trained tokenizer
DEFAULT_EMBED_DIM = 768     # Default for IndicBERT base, adjust if using a different text model

# %% ../nbs/00_core.ipynb 9
# Example Usage & Directory Creation (remove or comment out in final export if not needed)
if __name__ == '__main__':
    setup_logging()
    logger = get_logger(__name__)
    logger.info(f"Running in Colab: {IN_COLAB}")
    logger.info(f"Project Root: {PROJECT_ROOT}")

    # Create essential directories
    logger.info("Ensuring essential directories exist...")
    ensure_dir(DATA_PATH)
    ensure_dir(RAW_DATA_PATH)
    ensure_dir(HINDI_RAW_PATH)
    ensure_dir(SANSKRIT_RAW_PATH)
    ensure_dir(SYNTHETIC_RAW_PATH)
    ensure_dir(PROCESSED_DATA_PATH)
    ensure_dir(BENCHMARK_DATA_PATH)
    ensure_dir(MODEL_PATH)
    ensure_dir(CHECKPOINT_PATH)
    ensure_dir(ONNX_PATH)
    ensure_dir(QUANTIZED_PATH)
    ensure_dir(TOKENIZER_PATH)
    logger.info("Directory check complete.")

    logger.info(f"Data Path: {DATA_PATH}")
    logger.info(f"Model Path: {MODEL_PATH}")
    logger.info(f"Tokenizer Name: {PRETRAINED_TOKENIZER_NAME}")
    logger.info(f"Custom Special Tokens: {CUSTOM_SPECIAL_TOKENS}")
