"""Functions to calculate cross-modal retrieval (R@k, MR) and zero-shot classification accuracy for Indic-CLIP."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/11_evaluation_metrics.ipynb.

# %% auto 0
__all__ = ['logger', 'calculate_retrieval_metrics', 'calculate_zeroshot_accuracy']

# %% ../../nbs/11_evaluation_metrics.ipynb 6
import torch
import torch.nn.functional as F
import numpy as np
import logging
from typing import List, Dict, Callable
from fastcore.all import *
from sklearn.metrics import accuracy_score

# --- Project Imports ---
try:
    from indic_clip.core import get_logger, setup_logging
    from indic_clip.model.clip import IndicCLIP # Needed for type hinting and model methods
    from indic_clip.data.tokenization import IndicBERTTokenizer # Needed for type hinting
except ModuleNotFoundError:
    print("Could not import project modules in 11_evaluation_metrics.ipynb. Ensure path is correct.")
    # Define dummy logger if core not found
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    def get_logger(name): return logging.getLogger(name)
    def setup_logging(): pass
    # Define dummy classes for type hints if needed
    class IndicCLIP(torch.nn.Module): pass
    class IndicBERTTokenizer: pass

logger = get_logger(__name__)

# %% ../../nbs/11_evaluation_metrics.ipynb 8
def calculate_retrieval_metrics(
    image_features: torch.Tensor, 
    text_features: torch.Tensor, 
    logit_scale: torch.Tensor, 
    k_values: List[int] = [1, 5, 10]
) -> Dict[str, float]:
    """
    Calculates Recall@k (R@k) and Mean Recall (MR) for image-to-text and text-to-image retrieval.

    Assumes image_features and text_features are L2 normalized and correspond to paired data
    along the batch dimension (i.e., image_i matches text_i).

    Args:
        image_features (torch.Tensor): Tensor of normalized image features (B, EmbeddingDim).
        text_features (torch.Tensor): Tensor of normalized text features (B, EmbeddingDim).
        logit_scale (torch.Tensor): The temperature scaling factor (scalar tensor, already exponentiated).
        k_values (List[int]): List of k values for which to calculate Recall@k.

    Returns:
        Dict[str, float]: A dictionary containing calculated metrics like 'i2t_r@1', 't2i_r@5', 'mean_recall'.
    """
    metrics = {}
    device = image_features.device

    # Calculate similarity matrix
    similarity = logit_scale * image_features @ text_features.t()
    num_samples = similarity.shape[0]
    
    # --- Image-to-Text Retrieval ---
    # Find the top k text indices for each image query
    i2t_indices = similarity.topk(max(k_values), dim=1)[1]
    # Create ground truth labels (image i should match text i)
    gt_labels = torch.arange(num_samples, device=device).view(-1, 1)
    
    # Check if the ground truth label is within the top k predictions
    i2t_correct = i2t_indices == gt_labels

    # Calculate R@k for image-to-text
    all_recalls = []
    for k in k_values:
        recall_k = i2t_correct[:, :k].any(dim=1).float().mean().item()
        metrics[f'i2t_r@{k}'] = recall_k
        all_recalls.append(recall_k)
        logger.debug(f"i2t R@{k}: {recall_k:.4f}")

    # --- Text-to-Image Retrieval ---
    # Find the top k image indices for each text query (using transpose)
    t2i_indices = similarity.t().topk(max(k_values), dim=1)[1]
    # Ground truth labels are the same (text i should match image i)
    
    # Check if the ground truth label is within the top k predictions
    t2i_correct = t2i_indices == gt_labels
    
    # Calculate R@k for text-to-image
    for k in k_values:
        recall_k = t2i_correct[:, :k].any(dim=1).float().mean().item()
        metrics[f't2i_r@{k}'] = recall_k
        all_recalls.append(recall_k)
        logger.debug(f"t2i R@{k}: {recall_k:.4f}")

    # --- Mean Recall (MR) ---
    metrics['mean_recall'] = np.mean(all_recalls) if all_recalls else 0.0
    logger.debug(f"Mean Recall (MR): {metrics['mean_recall']:.4f}")

    return metrics

# %% ../../nbs/11_evaluation_metrics.ipynb 10
def calculate_zeroshot_accuracy(
    image_features: torch.Tensor,
    image_labels: List[int] | np.ndarray | torch.Tensor,
    class_names: List[str],
    templates: List[str] | Callable[[str], str],
    model: IndicCLIP,
    tokenizer: IndicBERTTokenizer,
) -> float:
    """
    Calculates the Top-1 Zero-Shot Classification accuracy.

    Args:
        image_features (torch.Tensor): Tensor of normalized image features (B, EmbeddingDim).
        image_labels (List[int] | np.ndarray | torch.Tensor): Ground truth labels for the images (indices corresponding to class_names).
        class_names (List[str]): List of class names.
        templates (List[str] | Callable[[str], str]): List of prompt templates (e.g., "a photo of {}") 
                                                       or a function that takes a class name and returns a prompt.
        model (IndicCLIP): The trained IndicCLIP model instance.
        tokenizer (IndicBERTTokenizer): The tokenizer instance used by the model.

    Returns:
        float: The Top-1 zero-shot classification accuracy.
    """
    device = image_features.device
    num_classes = len(class_names)
    model.eval() # Ensure model is in evaluation mode

    logger.info(f"Starting zero-shot classification for {num_classes} classes.")

    # 1. Generate and encode text prompts for all classes
    all_prompts = []
    if callable(templates):
        for classname in class_names:
            all_prompts.append(templates(classname))
    else: # templates is a list of strings
        for template in templates:
            for classname in class_names:
                all_prompts.append(template.format(classname))
    
    logger.debug(f"Generated {len(all_prompts)} prompts.")

    with torch.no_grad(): # No need for gradients during text encoding
        # Tokenize prompts - assumes tokenizer handles batching
        tokenized_prompts = tokenizer.tokenize(all_prompts)
        # Move tensors within the tokenized output to the model's device
        tokenized_prompts = {k: v.to(device) for k, v in tokenized_prompts.items() if isinstance(v, torch.Tensor)}
        
        # Encode text prompts
        # Note: model.encode_text expects a dictionary like {'input_ids': ..., 'attention_mask': ...}
        # Ensure tokenizer output format matches this expectation.
        text_embeddings = model.encode_text(tokenized_prompts)
        
        # Normalize text embeddings
        text_embeddings = F.normalize(text_embeddings, p=2, dim=-1)

        # Average embeddings if multiple templates were used
        if isinstance(templates, list) and len(templates) > 1:
            num_templates = len(templates)
            text_embeddings = text_embeddings.view(num_templates, num_classes, -1).mean(dim=0)
            text_embeddings = F.normalize(text_embeddings, p=2, dim=-1) # Re-normalize after averaging
        
        logger.debug(f"Encoded text embeddings shape: {text_embeddings.shape}")
            
        # 2. Calculate Similarity and Predict
        # Image features should already be normalized
        # Calculate cosine similarity (dot product of normalized features)
        similarity = image_features @ text_embeddings.t() # Shape: [B, NumClasses]
        logger.debug(f"Calculated similarity shape: {similarity.shape}")

        # Get predictions (index of the class with highest similarity)
        predictions = similarity.argmax(dim=1).cpu().numpy()
        logger.debug(f"Predictions shape: {predictions.shape}")

    # 3. Calculate Accuracy
    # Ensure labels are in numpy format
    if isinstance(image_labels, torch.Tensor):
        image_labels = image_labels.cpu().numpy()
    elif isinstance(image_labels, list):
        image_labels = np.array(image_labels)
        
    logger.debug(f"Ground truth labels shape: {image_labels.shape}")

    accuracy = accuracy_score(image_labels, predictions)
    logger.info(f"Zero-shot Top-1 Accuracy: {accuracy:.4f}")

    return accuracy
