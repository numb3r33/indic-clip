"""Setup for evaluation datasets and DataLoaders. Defines Indic-specific categories and provides functions to load benchmark data."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/12_evaluation_benchmarks.ipynb.

# %% auto 0
__all__ = ['logger', 'HINDI_RELIGIOUS_FIGURES', 'HINDI_ECOMMERCE_CATEGORIES', 'SANSKRIT_RELIGIOUS_CONCEPTS',
           'DEFAULT_ZS_CATEGORIES_HI', 'DEFAULT_ZS_CATEGORIES_SA', 'DEFAULT_PROMPT_TEMPLATES_HI',
           'DEFAULT_PROMPT_TEMPLATES_SA', 'DEFAULT_PROMPT_TEMPLATES_EN', 'load_benchmark_data',
           'create_zeroshot_dataloader']

# %% ../../nbs/12_evaluation_benchmarks.ipynb 6
import torch
import logging
import json
import pandas as pd
from pathlib import Path
from typing import List, Dict, Union, Callable

from fastai.vision.all import (
    DataBlock, ImageBlock, CategoryBlock, ColReader, RandomSplitter,
    Resize, Normalize, imagenet_stats, DataLoader, Datasets, TfmdLists
)
from fastcore.all import *

# --- Project Imports ---
try:
    from indic_clip.core import (
        get_logger, setup_logging, BENCHMARK_DATA_PATH, DEFAULT_IMAGE_SIZE,
        HINDI_RAW_PATH, SANSKRIT_RAW_PATH # Needed if benchmark images are in raw dirs
    )
    from indic_clip.data.creation import ImageGetter # Reuse image getter if structure is same
except ModuleNotFoundError:
    print("Could not import project modules in 12_evaluation_benchmarks.ipynb. Using fallbacks.")
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    def get_logger(name): return logging.getLogger(name)
    def setup_logging(): pass
    BENCHMARK_DATA_PATH = Path('./data/benchmarks')
    DEFAULT_IMAGE_SIZE = 224
    HINDI_RAW_PATH = Path('./data/raw/hindi')
    SANSKRIT_RAW_PATH = Path('./data/raw/sanskrit')
    # Dummy ImageGetter if needed
    def ImageGetter(row): return BENCHMARK_DATA_PATH / 'dummy_image.jpg'

logger = get_logger(__name__)

# %% ../../nbs/12_evaluation_benchmarks.ipynb 8
# --- Example Indic Categories for Zero-Shot Classification ---
# These lists should be curated based on expected evaluation domains.
# Hindi
HINDI_RELIGIOUS_FIGURES = [
    "कृष्ण", "राम", "शिव", "गणेश", "हनुमान", "दुर्गा", "लक्ष्मी", "सरस्वती", "विष्णु", "बुद्ध", "महावीर"
]
HINDI_ECOMMERCE_CATEGORIES = [
    "साड़ी", "कुर्ता", "मोबाइल फोन", "जूते", "किताब", "घड़ी", "चश्मा", "हेडफ़ोन", "लैपटॉप", "कैमरा", "गहने"
]

# Sanskrit (Example - adjust based on available data/domain)
SANSKRIT_RELIGIOUS_CONCEPTS = [
    "देव", "देवी", "यज्ञ", "मन्त्र", "वेद", "उपनिषद्", "पुराण", "योग", "ध्यान", "धर्म", "कर्म"
]

# Combine or select based on evaluation needs
DEFAULT_ZS_CATEGORIES_HI = HINDI_RELIGIOUS_FIGURES + HINDI_ECOMMERCE_CATEGORIES
DEFAULT_ZS_CATEGORIES_SA = SANSKRIT_RELIGIOUS_CONCEPTS

# --- Example Prompt Templates --- (Use appropriate language)
DEFAULT_PROMPT_TEMPLATES_HI = [
    "यह {} की तस्वीर है।",
    "{} का एक चित्र।",
    "एक फोटो जिसमें {} दिखाया गया है।"
]
DEFAULT_PROMPT_TEMPLATES_SA = [
    "इदं {} चित्रम् अस्ति।",
    "{} एकं चित्रम्।"
]
DEFAULT_PROMPT_TEMPLATES_EN = [
    "a photo of a {}",
    "a picture of {}",
    "an image depicting {}",
    "{} shown in an image"
]

# %% ../../nbs/12_evaluation_benchmarks.ipynb 10
def load_benchmark_data(benchmark_name: str, base_path: Path = BENCHMARK_DATA_PATH) -> pd.DataFrame | None:
    """Loads image paths and labels for a specific benchmark dataset.
       Assumes a standard format (e.g., CSV/JSONL) within the benchmark directory.

    Args:
        benchmark_name (str): Name of the benchmark (e.g., 'imagenet_val', 'flickr30k_hi').
        base_path (Path): Base directory containing benchmark subdirectories.

    Returns:
        pd.DataFrame: DataFrame with columns like 'image_path', 'label' (or 'captions'), 'split'.
                      Returns None if loading fails or format is unknown.
    """
    benchmark_dir = base_path / benchmark_name
    logger.info(f"Attempting to load benchmark data from: {benchmark_dir}")

    if not benchmark_dir.is_dir():
        logger.error(f"Benchmark directory not found: {benchmark_dir}")
        return None

    # --- Example Loading Logic (Adapt based on actual benchmark formats) ---

    # Example 1: CSV format (e.g., image_path,label_id,split)
    csv_file = benchmark_dir / f"{benchmark_name}.csv"
    if csv_file.exists():
        try:
            df = pd.read_csv(csv_file)
            # Basic validation (adjust columns as needed)
            if 'image_path' not in df.columns or ('label' not in df.columns and 'caption' not in df.columns):
                logger.error(f"CSV {csv_file} missing required columns ('image_path' and 'label'/'caption'). Found: {df.columns}")
                return None
            logger.info(f"Loaded {len(df)} items from CSV: {csv_file}")
            return df
        except Exception as e:
            logger.error(f"Error loading CSV benchmark data from {csv_file}: {e}")
            return None

    # Example 2: JSONL format (e.g., {'image_path': '...', 'label': '...', 'split': '...'})
    jsonl_file = benchmark_dir / f"{benchmark_name}.jsonl"
    if jsonl_file.exists():
        data = []
        try:
            with open(jsonl_file, 'r', encoding='utf-8') as f:
                for line in f:
                    item = json.loads(line)
                    # Add validation for required keys
                    if 'image_path' in item and ('label' in item or 'caption' in item):
                        data.append(item)
                    else:
                        logger.warning(f"Skipping invalid item in {jsonl_file}: {item}")
            df = pd.DataFrame(data)
            logger.info(f"Loaded {len(df)} items from JSONL: {jsonl_file}")
            return df
        except Exception as e:
            logger.error(f"Error loading JSONL benchmark data from {jsonl_file}: {e}")
            return None

    # --- Add more loading logic for other formats --- (e.g., ImageFolder structure)

    logger.error(f"Could not find suitable data file (CSV/JSONL) in {benchmark_dir} for benchmark '{benchmark_name}'.")
    return None

# %% ../../nbs/12_evaluation_benchmarks.ipynb 12
def create_zeroshot_dataloader(
    df: pd.DataFrame,
    image_path_col: str = 'image_path',
    label_col: str = 'label',
    benchmark_base_path: Path = BENCHMARK_DATA_PATH,
    img_size: int = DEFAULT_IMAGE_SIZE,
    batch_size: int = 64,
    num_workers: int = 4
) -> DataLoader | None:
    """Creates a DataLoader for zero-shot classification evaluation.

    The DataLoader yields tuples of (image_tensor, label_index).

    Args:
        df (pd.DataFrame): DataFrame containing image paths and corresponding labels (indices).
        image_path_col (str): Name of the column containing relative image paths.
        label_col (str): Name of the column containing ground truth labels (as indices).
        benchmark_base_path (Path): The absolute base path where the benchmark images are stored.
                                      The paths in df[image_path_col] are assumed relative to this.
        img_size (int): Target image size.
        batch_size (int): DataLoader batch size.
        num_workers (int): Number of workers for the DataLoader.

    Returns:
        DataLoader: A fastai DataLoader for evaluation, or None if columns are missing.
    """
    if image_path_col not in df.columns or label_col not in df.columns:
        logger.error(f"Required columns '{image_path_col}' or '{label_col}' not found in DataFrame.")
        return None

    # Function to construct the full image path
    def get_img_path(row): return benchmark_base_path / row[image_path_col]

    # Define a simple DataBlock for loading images and labels
    # No splitting needed as we evaluate on the whole set (usually test set)
    # Use TfmdLists directly for more control over evaluation datasets

    # 1. Create item transforms (Resize)
    item_tfms = Resize(img_size)
    # 2. Create batch transforms (Normalize)
    batch_tfms = Normalize.from_stats(*imagenet_stats)

    # 3. Create Datasets
    # We need image paths and integer labels
    img_paths = df.apply(get_img_path, axis=1).tolist()
    labels = df[label_col].tolist()

    # Ensure labels are integers
    try:
        labels = [int(l) for l in labels]
    except ValueError:
        logger.error(f"Could not convert all labels in column '{label_col}' to integers.")
        return None

    # Create TfmdLists: applies item_tfms lazily
    # Input pipeline: Path -> Image -> PILImage
    # Target pipeline: int -> TensorCategory
    tls = TfmdLists(zip(img_paths, labels),
                    tfms=[[PILImage.create], [CategoryEncode()]]) # No item_tfms here yet

    # Apply item transforms to the input part (images)
    tls = tls.map_tfms([item_tfms, None], dl_type=Datasets) # None means no tfms for labels


    # 4. Create DataLoader
    # `after_item` applies transforms to individual items after they are fetched
    # `before_batch` applies transforms before collation
    # `after_batch` applies transforms to the collated batch (like normalization)
    dls = DataLoader(tls, bs=batch_size, num_workers=num_workers,
                     after_item=[ToTensor], # Convert PILImage to Tensor
                     after_batch=[IntToFloatTensor, batch_tfms], # Normalize batch
                     shuffle=False, drop_last=False) # No shuffle/drop for validation

    logger.info(f"Created Zero-Shot DataLoader with {len(tls)} items.")
    return dls

# %% ../../nbs/12_evaluation_benchmarks.ipynb 13
# Placeholder for Retrieval DataLoader - Adapt from 04_data_creation if needed
# This might be necessary if retrieval benchmarks have a different structure
# or require specific test splits.

# def create_retrieval_dataloader(...):
#     logger.info("Retrieval DataLoader creation not fully implemented yet.")
#     # Reuse IndicCLIPDataBlock logic from 04_data_creation if applicable,
#     # possibly filtering the source DataFrame for the specific benchmark and split.
#     # Ensure shuffle=False and drop_last=False for evaluation.
#     return None
