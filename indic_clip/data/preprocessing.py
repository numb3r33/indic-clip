"""Functions for cleaning and filtering raw image-text pair data. Includes image dimension/aspect ratio filtering, text length filtering, and duplicate removal using image hashing and text matching."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/02_data_preprocessing.ipynb.

# %% auto 0
__all__ = ['logger', 'MIN_IMAGE_RESOLUTION', 'MAX_IMAGE_RESOLUTION', 'MIN_ASPECT_RATIO', 'MAX_ASPECT_RATIO', 'MIN_TEXT_LENGTH',
           'MAX_TEXT_LENGTH', 'FILTERED_DATA_FILENAME', 'FILTERED_OUTPUT_PATH', 'load_raw_data', 'get_image_metadata',
           'calculate_image_hash', 'filter_data']

# %% ../../nbs/02_data_preprocessing.ipynb 6
try:
    import indic_clip.core
    print("Reloaded indic_clip.core")
except ModuleNotFoundError:
    print("indic_clip.core not found initially.")
    # Attempt to set sys.path if running in Colab and project cloned
    import sys
    if 'google.colab' in sys.modules:
        project_parent = '/content' # Assuming cloned into /content/indic-clip
        if Path('/content/drive/MyDrive/Indic-Clip').exists():
             project_parent = '/content/drive/MyDrive/Indic-Clip'
        if project_parent not in sys.path:
             sys.path.insert(0, project_parent)
             print(f"Added {project_parent} to sys.path")
        try:
            import indic_clip.core
            print("Imported indic_clip.core after path adjustment.")
        except ModuleNotFoundError:
            print("ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.")
            print("Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive")
            # raise # Stop execution if core components missing

# %% ../../nbs/02_data_preprocessing.ipynb 7
import os
import json
import logging
from pathlib import Path
from PIL import Image, UnidentifiedImageError
import imagehash
import pandas as pd # Optional, but useful for handling dataframes
from tqdm.notebook import tqdm

try:
    from indic_clip.core import (
        HINDI_RAW_PATH,
        SANSKRIT_RAW_PATH,
        SYNTHETIC_RAW_PATH,
        PROCESSED_DATA_PATH,
        get_logger,
        setup_logging,
        ensure_dir
    )
except ModuleNotFoundError:
    print("Error importing from indic_clip.core. Using Fallbacks.")
    # Fallbacks if core isn't importable (e.g., interactive testing)
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    if 'google.colab' in sys.modules:
        PROJECT_ROOT=Path('/content/Indic-Clip')
        if Path('/content/drive/MyDrive/Indic-Clip').exists():
           PROJECT_ROOT=Path('/content/drive/MyDrive/Indic-Clip')
    else:
        PROJECT_ROOT=Path('.').resolve()
        if PROJECT_ROOT.name == 'nbs': PROJECT_ROOT = PROJECT_ROOT.parent
    DATA_PATH = PROJECT_ROOT / 'data'
    RAW_DATA_PATH = DATA_PATH / 'raw'
    HINDI_RAW_PATH = RAW_DATA_PATH / 'hindi'
    SANSKRIT_RAW_PATH = RAW_DATA_PATH / 'sanskrit'
    SYNTHETIC_RAW_PATH = RAW_DATA_PATH / 'synthetic'
    PROCESSED_DATA_PATH = DATA_PATH / 'processed'
    def get_logger(name): return logging.getLogger(name)
    def setup_logging(): pass
    def ensure_dir(path: Path): path.mkdir(parents=True, exist_ok=True)

# Setup logging
setup_logging()
logger = get_logger(__name__)

# %% ../../nbs/02_data_preprocessing.ipynb 9
# --- Filtering Thresholds ---
MIN_IMAGE_RESOLUTION = 224 # Minimum width and height
MAX_IMAGE_RESOLUTION = 4096 # Optional: Maximum width/height
MIN_ASPECT_RATIO = 1/3   # Allow images like 1:3
MAX_ASPECT_RATIO = 3     # Allow images like 3:1
MIN_TEXT_LENGTH = 5      # Minimum number of characters in caption
MAX_TEXT_LENGTH = 256    # Maximum number of characters in caption

# --- Output ---
FILTERED_DATA_FILENAME = "filtered_data.jsonl"
FILTERED_OUTPUT_PATH = PROCESSED_DATA_PATH

# %% ../../nbs/02_data_preprocessing.ipynb 11
def load_raw_data(jsonl_path: Path) -> list:
    """Loads raw data from a JSONL file.

    Args:
        jsonl_path: Path to the input JSONL file (e.g., flickr8k_hindi_raw.jsonl).

    Returns:
        A list of dictionaries, where each dictionary represents a row.
        Returns an empty list if the file is not found or is empty.
    """
    data = []
    if not jsonl_path.exists():
        logger.error(f"Raw data file not found: {jsonl_path}")
        return data

    try:
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data.append(json.loads(line.strip()))
                except json.JSONDecodeError:
                    logger.warning(f"Skipping invalid JSON line in {jsonl_path}: {line.strip()}")
        logger.info(f"Loaded {len(data)} items from {jsonl_path}")
        return data
    except Exception as e:
        logger.error(f"Error loading raw data from {jsonl_path}: {e}")
        return [] # Return empty list on error

# %% ../../nbs/02_data_preprocessing.ipynb 12
def get_image_metadata(image_path: Path) -> dict | None:
    """Gets metadata (width, height, aspect ratio) for an image.

    Args:
        image_path: Path to the image file.

    Returns:
        A dictionary {'width': int, 'height': int, 'aspect_ratio': float} or None if error.
    """
    try:
        with Image.open(image_path) as img:
            width, height = img.size
            if width == 0 or height == 0:
                logger.warning(f"Image has zero dimension: {image_path} (Size: {width}x{height})")
                return None
            aspect_ratio = width / height
            return {"width": width, "height": height, "aspect_ratio": aspect_ratio}
    except FileNotFoundError:
        logger.warning(f"Image file not found: {image_path}")
        return None
    except UnidentifiedImageError:
        logger.warning(f"Could not identify image file (possibly corrupt): {image_path}")
        return None
    except Exception as e:
        logger.error(f"Error processing image {image_path}: {e}")
        return None

# %% ../../nbs/02_data_preprocessing.ipynb 13
def calculate_image_hash(image_path: Path) -> str | None:
    """Calculates the perceptual hash (phash) of an image.

    Args:
        image_path: Path to the image file.

    Returns:
        The phash string or None if hashing fails.
    """
    try:
        with Image.open(image_path) as img:
            img_hash = imagehash.phash(img)
            return str(img_hash)
    except FileNotFoundError:
        # Already logged in get_image_metadata typically, but can log again if needed
        # logger.warning(f"Image file not found for hashing: {image_path}")
        return None
    except UnidentifiedImageError:
        # logger.warning(f"Could not identify image file for hashing: {image_path}")
        return None
    except Exception as e:
        logger.error(f"Error calculating hash for image {image_path}: {e}")
        return None

# %% ../../nbs/02_data_preprocessing.ipynb 15
def filter_data(raw_data: list, base_image_path: Path) -> list:
    """Applies filtering rules to the raw data.

    Filters include:
    - Image resolution (min/max)
    - Image aspect ratio (min/max)
    - Text length (min/max)
    - Duplicate removal (based on image phash OR exact text match)

    Args:
        raw_data: List of dictionaries loaded from raw JSONL.
                  Expected keys: 'image_path_relative', 'caption'.
        base_image_path: The base directory where images corresponding to
                         'image_path_relative' are stored (e.g., HINDI_RAW_PATH).

    Returns:
        A list of filtered dictionaries.
    """
    filtered_list = []
    seen_image_hashes = set()
    seen_captions = set()
    skipped_counts = {
        "invalid_entry": 0,
        "image_error": 0,
        "resolution": 0,
        "aspect_ratio": 0,
        "text_length": 0,
        "duplicate_image": 0,
        "duplicate_caption": 0,
    }

    logger.info(f"Starting filtering process for {len(raw_data)} raw items...")

    for item in tqdm(raw_data, desc="Filtering Data"):
        if not isinstance(item, dict) or 'image_path_relative' not in item or 'caption' not in item:
            skipped_counts["invalid_entry"] += 1
            # logger.warning(f"Skipping invalid entry: {item}")
            continue

        relative_img_path = item['image_path_relative']
        caption = item['caption']
        full_image_path = base_image_path / relative_img_path

        # 1. Filter by Image Metadata
        metadata = get_image_metadata(full_image_path)
        if metadata is None:
            skipped_counts["image_error"] += 1
            continue # Skip if image can't be opened or has errors

        if not (MIN_IMAGE_RESOLUTION <= metadata['width'] <= MAX_IMAGE_RESOLUTION and
                MIN_IMAGE_RESOLUTION <= metadata['height'] <= MAX_IMAGE_RESOLUTION):
            skipped_counts["resolution"] += 1
            # logger.debug(f"Skipping {relative_img_path}: Resolution ({metadata['width']}x{metadata['height']}) out of bounds.")
            continue

        if not (MIN_ASPECT_RATIO <= metadata['aspect_ratio'] <= MAX_ASPECT_RATIO):
            skipped_counts["aspect_ratio"] += 1
            # logger.debug(f"Skipping {relative_img_path}: Aspect ratio ({metadata['aspect_ratio']:.2f}) out of bounds.")
            continue

        # 2. Filter by Text Length
        # Ensure caption is a string before checking length
        if not isinstance(caption, str) or not (MIN_TEXT_LENGTH <= len(caption) <= MAX_TEXT_LENGTH):
            skipped_counts["text_length"] += 1
            # logger.debug(f"Skipping {relative_img_path}: Caption length ({len(caption) if isinstance(caption, str) else 'N/A'}) out of bounds.")
            continue

        # 3. Filter by Duplicates
        img_hash = calculate_image_hash(full_image_path)

        # Use normalized caption for duplicate checking if needed later
        # For now, use exact match on the raw caption
        normalized_caption = caption.strip() # Basic normalization

        is_duplicate = False
        if img_hash is not None and img_hash in seen_image_hashes:
            skipped_counts["duplicate_image"] += 1
            is_duplicate = True
            # logger.debug(f"Skipping {relative_img_path}: Duplicate image hash ({img_hash}).")

        if normalized_caption in seen_captions:
             # Only count as caption duplicate if not already counted as image duplicate
            if not is_duplicate:
                 skipped_counts["duplicate_caption"] += 1
                 is_duplicate = True
            # logger.debug(f"Skipping {relative_img_path}: Duplicate caption.")

        if is_duplicate:
            continue

        # If all filters passed, add to list and update seen sets
        filtered_list.append(item)
        if img_hash is not None:
            seen_image_hashes.add(img_hash)
        seen_captions.add(normalized_caption)

    logger.info(f"Finished filtering. Kept {len(filtered_list)} items.")
    logger.info(f"Skipped counts: {skipped_counts}")
    return filtered_list
