"""Functions and tools for acquiring image-text pair data, primarily using existing datasets from Kaggle. Includes Colab setup."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/01_data_collection.ipynb.

# %% auto 0
__all__ = ['logger', 'save_formatted_data', 'download_kaggle_dataset', 'unzip_file', 'load_hindi_captions',
           'get_sanskrit_data_placeholder', 'get_synthetic_data_placeholder']

# %% ../../nbs/01_data_collection.ipynb 7
import os
import time
import json
import logging
from pathlib import Path
import zipfile
import pandas as pd
from tqdm.notebook import tqdm
import sys

# Try importing core components
try:
    from indic_clip.core import (PROJECT_ROOT, HINDI_RAW_PATH, SANSKRIT_RAW_PATH,
                               SYNTHETIC_RAW_PATH, get_logger, setup_logging, ensure_dir)
except ModuleNotFoundError as e:
    print(f"Error importing from indic_clip.core: {e}")
    print("Please ensure the indic_clip library is installed (pip install -e .) or the path is correct.")
    # Define fallbacks if running interactively without full setup
    if 'google.colab' in sys.modules:
        PROJECT_ROOT=Path('/content/Indic-Clip')
        if Path('/content/drive/MyDrive/Indic-Clip').exists():
           PROJECT_ROOT=Path('/content/drive/MyDrive/Indic-Clip')
    else:
        PROJECT_ROOT=Path('.').resolve()
    print(f"Using fallback PROJECT_ROOT: {PROJECT_ROOT}")
    DATA_PATH = PROJECT_ROOT / 'data'
    RAW_DATA_PATH = DATA_PATH / 'raw'
    HINDI_RAW_PATH = RAW_DATA_PATH / 'hindi'
    SANSKRIT_RAW_PATH = RAW_DATA_PATH / 'sanskrit'
    SYNTHETIC_RAW_PATH = RAW_DATA_PATH / 'synthetic'
    # Define simple logging if setup fails
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logger = logging.getLogger(__name__)
    def ensure_dir(path: Path): path.mkdir(parents=True, exist_ok=True)
    def setup_logging(): pass # No-op
    def get_logger(name): return logging.getLogger(name)

try:
    import kaggle
    print("Kaggle library imported.")
except OSError as e:
    print("Kaggle API Error: Ensure kaggle.json is uploaded/configured correctly in Colab or locally.")
    # raise e # Don't raise here, let download attempt fail later
except ImportError:
     print("ERROR: Kaggle library not installed. Run !pip install kaggle")

# Setup logging for this module
setup_logging()
logger = get_logger(__name__)

# %% ../../nbs/01_data_collection.ipynb 9
def save_formatted_data(data: list, output_path: Path, filename: str):
    """Saves a list of data (dicts) to a JSONL file.

    Args:
        data: A list of dictionaries, where each dict represents an image-text pair
              (e.g., {'image_filename': 'name.jpg', 'caption': 'text', 'source': 'datasource'}).
        output_path: The directory Path object where the file should be saved.
        filename: The name of the output file (e.g., 'flickr8k_hindi_raw.jsonl').
    """
    if not data:
        logger.warning(f"No data provided to save for {filename}. Skipping.")
        return

    ensure_dir(output_path)
    filepath = output_path / filename

    try:
        with open(filepath, 'w', encoding='utf-8') as f: # Overwrite mode for consistency on rerun
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        logger.info(f"Successfully wrote {len(data)} items to {filepath}")
    except IOError as e:
        logger.error(f"Error saving data to {filepath}: {e}")
    except Exception as e:
        logger.error(f"An unexpected error occurred while saving data to {filepath}: {e}")

# %% ../../nbs/01_data_collection.ipynb 10
def download_kaggle_dataset(dataset_slug: str, download_path: Path):
    """Downloads a dataset from Kaggle using the official API.

    Args:
        dataset_slug: The Kaggle dataset slug (e.g., 'user/dataset-name').
        download_path: The Path object representing the directory to download files into.
    """
    logger.info(f"Attempting to download dataset '{dataset_slug}' to '{download_path}'...")
    ensure_dir(download_path)
    try:
        kaggle.api.authenticate() # Reads credentials from ~/.kaggle/kaggle.json or env vars
        kaggle.api.dataset_download_files(dataset_slug, path=download_path, unzip=False, quiet=False)
        logger.info(f"Dataset '{dataset_slug}' downloaded successfully to '{download_path}'.")
        return True
    except NameError:
         logger.error("Kaggle library not imported correctly. Cannot download.")
         return False
    except Exception as e:
        logger.error(f"Failed to download dataset '{dataset_slug}': {e}")
        logger.error("Please ensure the Kaggle API is configured correctly (kaggle.json or env vars) and you accepted the dataset's terms on the Kaggle website if required.")
        # Consider raising the exception if download is critical
        # raise e
        return False

# %% ../../nbs/01_data_collection.ipynb 11
def unzip_file(zip_path: Path, extract_to: Path):
    """Unzips a file to a specified directory.

    Args:
        zip_path: The Path object of the zip file.
        extract_to: The Path object of the directory to extract files into.
    """
    if not zip_path.exists():
        logger.error(f"Zip file not found at {zip_path}. Cannot unzip.")
        return False

    logger.info(f"Unzipping '{zip_path.name}' to '{extract_to}'...")
    ensure_dir(extract_to)
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            for member in tqdm(zip_ref.infolist(), desc=f'Extracting {zip_path.name}'):
                try:
                    # Ensure extraction path is safe (within extract_to)
                    target_path = os.path.join(extract_to, member.filename)
                    if not os.path.abspath(target_path).startswith(os.path.abspath(extract_to)):
                         logger.warning(f"Skipping potentially unsafe path in zip: {member.filename}")
                         continue
                    zip_ref.extract(member, extract_to)
                except zipfile.error as e:
                    logger.error(f"Error extracting {member.filename} from {zip_path.name}: {e}")
                except Exception as e:
                     logger.error(f"Unexpected error extracting {member.filename}: {e}")
        logger.info(f"Successfully unzipped '{zip_path.name}'.")
        # Optional: Remove the zip file after successful extraction
        # os.remove(zip_path)
        # logger.info(f"Removed zip file: '{zip_path.name}'")
        return True
    except zipfile.BadZipFile:
        logger.error(f"Error: '{zip_path.name}' is not a valid zip file or is corrupted.")
        return False
    except Exception as e:
        logger.error(f"An unexpected error occurred during unzipping '{zip_path.name}': {e}")
        return False

# %% ../../nbs/01_data_collection.ipynb 12
def load_hindi_captions(csv_path: Path) -> pd.DataFrame | None:
    """Loads Hindi captions from the specified CSV file.

    Args:
        csv_path: Path object to the captions CSV file.

    Returns:
        A pandas DataFrame containing the captions, or None if loading fails.
    """
    if not csv_path.exists():
        logger.error(f"Caption file not found: {csv_path}")
        return None

    logger.info(f"Loading captions from {csv_path}...")
    try:
        # The provided CSV seems to have a header based on sample
        df = pd.read_csv(csv_path, header=0)

        # Basic validation
        required_columns = ['image', 'caption']
        # Clean column names (strip whitespace etc.)
        df.columns = df.columns.str.strip()
        if not all(col in df.columns for col in required_columns):
            logger.error(f"CSV file {csv_path} missing required columns. Expected: {required_columns}, Found: {df.columns.tolist()}")
            return None

        logger.info(f"Successfully loaded {len(df)} captions from {csv_path}.")
        return df
    except pd.errors.EmptyDataError:
        logger.error(f"Error: Caption file {csv_path} is empty.")
        return None
    except Exception as e:
        logger.error(f"Error loading captions from {csv_path}: {e}")
        return None

# %% ../../nbs/01_data_collection.ipynb 14
def get_sanskrit_data_placeholder() -> list:
    """Placeholder function representing the Sanskrit data acquisition process.

    In a real scenario, this function would interact with APIs, databases,
    or parsed files from digitized manuscripts or other sources.

    Returns:
        A list of dictionaries (or an empty list), each containing
        'image_filename', 'caption' (Sanskrit text), and 'source'.
    """
    logger.warning("Using placeholder function for Sanskrit data. No actual Sanskrit data loaded.")
    # TODO: Replace this with actual logic to load Sanskrit data
    # This might involve:
    # - Reading pre-processed files created manually or via collaboration
    # - Connecting to specific digital library APIs
    # - Processing OCR results linked to manuscript images
    sanskrit_data = [
        # {
        #     'image_filename': 'manuscript_page_1_illustration_1.jpg',
        #     'caption': 'ॐ असतो मा सद्गमय । तमसो मा ज्योतिर्गमय । मृत्योर्मा अमृतं गमय ॥',
        #     'source': 'example_manuscript_archive'
        # },
    ]
    if sanskrit_data:
       logger.info(f"Loaded {len(sanskrit_data)} placeholder Sanskrit items.")
    return sanskrit_data

# %% ../../nbs/01_data_collection.ipynb 16
def get_synthetic_data_placeholder(data_path: Path) -> list:
    """Placeholder function representing the synthetic data integration process.

    In a real scenario, this would read data generated by the IndicTTI project,
    assuming a specific format (e.g., a directory of images and a metadata file).

    Args:
        data_path: Path to the directory or file containing synthetic data.

    Returns:
        A list of dictionaries (or an empty list), each containing
        'image_filename', 'caption' (could be Hindi or Sanskrit), and 'source'.
    """
    logger.warning("Using placeholder function for Synthetic data. No actual data loaded.")
    # TODO: Replace with actual logic to load synthetic data from IndicTTI
    # Example: Assume a metadata JSONL file exists at data_path
    metadata_file = data_path / 'metadata.jsonl'
    synthetic_data = []
    if metadata_file.exists():
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                for line in f:
                    item = json.loads(line)
                    # Assume item has 'image_filename' and 'caption' keys
                    if 'image_filename' in item and 'caption' in item:
                         item['source'] = 'indic_tti_synthetic'
                         synthetic_data.append(item)
                    else:
                         logger.warning(f"Skipping synthetic item due to missing keys: {item}")
            logger.info(f"Loaded {len(synthetic_data)} items from synthetic source: {metadata_file}")
        except Exception as e:
            logger.error(f"Error loading synthetic data from {metadata_file}: {e}")
    else:
        logger.warning(f"Synthetic data metadata file not found at {metadata_file}")

    return synthetic_data
