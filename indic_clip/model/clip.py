"""Defines the main IndicCLIP model architecture, combining the vision and text encoders."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/07_model_clip.ipynb.

# %% auto 0
__all__ = ['logger', 'IndicCLIP']

# %% ../../nbs/07_model_clip.ipynb 4
try:
    import indic_clip.core
    print("Reloaded indic_clip.core")
except ModuleNotFoundError:
    print("indic_clip.core not found initially.")
    # Attempt to set sys.path if running in Colab and project cloned
    import sys
    if 'google.colab' in sys.modules:
        project_parent = '/content' # Assuming cloned into /content/indic-clip
        if Path('/content/drive/MyDrive/Indic-Clip').exists():
             project_parent = '/content/drive/MyDrive/Indic-Clip'
        if project_parent not in sys.path:
             sys.path.insert(0, project_parent)
             print(f"Added {project_parent} to sys.path")
        try:
            import indic_clip.core
            print("Imported indic_clip.core after path adjustment.")
        except ModuleNotFoundError:
            print("ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.")
            print("Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive")
            # raise # Stop execution if core components missing

# %% ../../nbs/07_model_clip.ipynb 8
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import logging

from fastai.vision.all import *

try:
    # Import necessary components from our project
    from indic_clip.core import get_logger, setup_logging, DEFAULT_EMBED_DIM, PRETRAINED_TOKENIZER_NAME
    from indic_clip.model.vision import VisionEncoder
    from indic_clip.model.text import TextEncoder
    from indic_clip.data.tokenization import IndicBERTTokenizer # Needed if passing tokenizer
except ModuleNotFoundError:
    print('MODULE NOT FOUND')
    # Fallback if core not found (e.g. testing)
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    def get_logger(name): return logging.getLogger(name)
    def setup_logging(): pass
    DEFAULT_EMBED_DIM = 768
    PRETRAINED_TOKENIZER_NAME = "ai4bharat/indic-bert"
    # Dummy classes if modules aren't available
    class VisionEncoder(nn.Module):
        def __init__(self, model_name='dummy_vision', pretrained=True, output_dim=None):
            super().__init__()
            self.feature_dim = 768
            self.backbone = nn.Identity()
            self.projection = nn.Linear(768, output_dim) if output_dim else nn.Identity()
        def forward(self, x): return self.projection(torch.randn(x.shape[0], self.feature_dim))
        def set_gradient_checkpointing(self, enable): pass

    class TextEncoder(nn.Module):
        def __init__(self, model_name='dummy_text', pretrained=True, output_dim=None, tokenizer=None):
            super().__init__()
            self.feature_dim = 768
            self.backbone = nn.Identity()
            self.projection = nn.Linear(768, output_dim) if output_dim else nn.Identity()
        def forward(self, input_ids, attention_mask): return self.projection(torch.randn(input_ids.shape[0], self.feature_dim))
        def set_gradient_checkpointing(self, enable): pass

    class IndicBERTTokenizer:
         def __init__(self, *args, **kwargs): self.vocab_size = 30000
         def tokenize(self, texts):
            if isinstance(texts, str): texts=[texts]
            ids = torch.randint(0, self.vocab_size, (len(texts), 10))
            mask = torch.ones_like(ids)
            return {'input_ids': ids, 'attention_mask': mask}
         @classmethod
         def load_tokenizer(cls, *args, **kwargs):
             return cls()


setup_logging()
logger = get_logger(__name__)

# %% ../../nbs/07_model_clip.ipynb 10
class IndicCLIP(Module):
    """The main IndicCLIP model, combining Vision and Text Encoders.

    This module integrates image and text processing pipelines and projects
    their features into a shared embedding space for contrastive learning.
    """
    def __init__(self,
                 embed_dim: int = DEFAULT_EMBED_DIM,
                 vision_model_name: str = 'vit_base_patch16_224',
                 vision_pretrained: bool = True,
                 text_model_name: str = PRETRAINED_TOKENIZER_NAME, # Use tokenizer's base model
                 text_pretrained: bool = True,
                 tokenizer: IndicBERTTokenizer = None):
        """
        Initializes the IndicCLIP model.

        Args:
            embed_dim (int): The dimension of the shared embedding space.
            vision_model_name (str): Name of the timm vision model.
            vision_pretrained (bool): Whether to load pretrained weights for the vision model.
            text_model_name (str): Name or path of the Hugging Face text model.
            text_pretrained (bool): Whether to load pretrained weights for the text model.
            tokenizer (IndicBERTTokenizer): The tokenizer instance, needed for text encoder setup (embedding resize).
        """
        if tokenizer is None:
             logger.warning("No tokenizer provided to IndicCLIP. Text encoder might not resize embeddings correctly.")
             # Attempt to load a default one - this might fail if path isn't set up
             # from indic_clip.core import TOKENIZER_PATH
             # tokenizer = IndicBERTTokenizer.load_tokenizer(TOKENIZER_PATH)


        self.vision_encoder = VisionEncoder(
            model_name=vision_model_name,
            pretrained=vision_pretrained,
            output_dim=None # Projection handled below
        )

        self.text_encoder = TextEncoder(
            model_name=text_model_name,
            pretrained=text_pretrained,
            output_dim=None, # Projection handled below
            tokenizer=tokenizer # Pass tokenizer for potential embedding resize
        )

        # --- Projection Heads ---
        # Project features from vision/text backbones to the shared embed_dim
        if self.vision_encoder.feature_dim is None or self.text_encoder.feature_dim is None:
             raise ValueError("Could not determine feature dimensions for vision or text encoders.")

        self.visual_projection = nn.Linear(self.vision_encoder.feature_dim, embed_dim, bias=False)
        self.text_projection = nn.Linear(self.text_encoder.feature_dim, embed_dim, bias=False)

        # Initialize projection layers (optional, but common)
        # Often initialized to match CLIP's initialization if transferring
        # Default PyTorch init is Kaiming Uniform for Linear layers

        # --- Logit Scale ---
        # Learnable parameter for scaling similarity scores
        # Initialized according to OpenAI CLIP paper (log(1/0.07))
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

        logger.info(f"IndicCLIP initialized with vision='{vision_model_name}', text='{text_model_name}', embed_dim={embed_dim}")

    def encode_image(self, image: torch.Tensor) -> torch.Tensor:
        """Encodes an image into the shared embedding space.

        Args:
            image (torch.Tensor): Input image tensor (B, C, H, W).

        Returns:
            torch.Tensor: Image features projected into the embedding space (B, embed_dim), L2-normalized.
        """
        image_features = self.vision_encoder(image)
        projected_features = self.visual_projection(image_features)
        # Normalize features
        normalized_features = F.normalize(projected_features, p=2, dim=-1)
        return normalized_features

    def encode_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:
        """Encodes text into the shared embedding space.

        Args:
            input_ids (torch.Tensor): Input token IDs (B, SeqLen).
            attention_mask (torch.Tensor): Attention mask (B, SeqLen).

        Returns:
            torch.Tensor: Text features projected into the embedding space (B, embed_dim), L2-normalized.
        """
        text_features = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)
        projected_features = self.text_projection(text_features)
        # Normalize features
        normalized_features = F.normalize(projected_features, p=2, dim=-1)
        return normalized_features

    def forward(self, image: torch.Tensor,
                text_input: tuple) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Forward pass for training. Encodes both image and text.
        Accepts image tensor and text tensor tuple separately from Learner unpacking xb.

        Args:
            image (torch.Tensor): Input image tensor (B, C, H, W).
            text_input (tuple): A tuple containing:
                - input_ids (torch.Tensor): Input token IDs (B, SeqLen).
                - attention_mask (torch.Tensor): Attention mask (B, SeqLen).

        Returns:
            tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
                - image_features: Normalized image features (B, embed_dim).
                - text_features: Normalized text features (B, embed_dim).
                - logit_scale: The learned logit scaling factor (scalar tensor, exponentiated).
        """
        input_ids, attention_mask = text_input # Unpack the text tuple received as the second argument

        image_features = self.encode_image(image)
        # Pass the unpacked tensors to encode_text
        text_features = self.encode_text(input_ids, attention_mask)

        # Clamp the logit scale parameter before exponentiating
        # Clamp log(1/T) to avoid T becoming too small (e.g., T > 0.01 -> log(1/T) < log(100))
        self.logit_scale.data.clamp_(max=np.log(1 / 0.01)) # Max log value ~4.605

        # Return the exponentiated clamped value
        logit_scale_exp = self.logit_scale.exp()

        # logit_scale.exp() is typically applied in the loss function
        # Return the raw parameter here, but exponentiate for clarity in return type
        return image_features, text_features, logit_scale_exp

    def set_gradient_checkpointing(self, enable: bool = True):
        """Enable or disable gradient checkpointing for both encoders."""
        self.vision_encoder.set_gradient_checkpointing(enable)
        self.text_encoder.set_gradient_checkpointing(enable)
        logger.info(f"IndicCLIP gradient checkpointing {'enabled' if enable else 'disabled'} for both encoders.")
