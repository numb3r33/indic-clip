"""Orchestrates the Indic-CLIP model training process using fast.ai's Learner and custom components."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/10_training.ipynb.

# %% auto 0
__all__ = ['logger', 'parse_args', 'main']

# %% ../../nbs/10_training.ipynb 5
# --- Standard Library Imports ---
import os
import argparse
import warnings
from pathlib import Path
import math

# --- Pypi Library Imports ---
import torch
import wandb
import numpy as np
from fastcore.all import *

# --- fastai Imports ---
from fastai.vision.all import *
from fastai.text.all import *
from fastai.data.all import *

# --- Project Imports ---
# Use try-except for robustness, especially during development/export
try:
    from indic_clip.core import * # Imports constants and utils
    from indic_clip.data.creation import IndicCLIPDataBlock, get_indic_clip_items
    from indic_clip.data.tokenization import IndicBERTTokenizer
    from indic_clip.model.clip import IndicCLIP
    from indic_clip.loss import ContrastiveLoss
    from indic_clip.learner import RetrievalMetricCallback # Custom callback for validation
except ModuleNotFoundError as e:
    print(f"Error importing project modules: {e}")
    print("Please ensure the project is installed or sys.path is configured correctly.")
    # Define dummy classes/functions if imports fail, allowing script structure to be parsed
    class IndicCLIPDataBlock: pass
    def get_indic_clip_items(*args, **kwargs): return []
    class IndicBERTTokenizer: pass
    class IndicCLIP(torch.nn.Module): pass
    class ContrastiveLoss(torch.nn.Module): pass
    class RetrievalMetricCallback(Callback): pass

# --- Setup Logging ---
setup_logging()
logger = get_logger(__name__)

# %% ../../nbs/10_training.ipynb 6
def parse_args():
    """Parses command-line arguments for the training script."""
    parser = argparse.ArgumentParser(description='Train Indic-CLIP Model')

    # --- Data Arguments ---
    parser.add_argument('--processed_data_path', type=str, default=str(PROCESSED_DATA_PATH / 'filtered_data.jsonl'),
                        help='Path to the processed JSONL data file.')
    parser.add_argument('--tokenizer_path', type=str, default=str(TOKENIZER_PATH),
                        help='Path to the saved tokenizer directory.')
    parser.add_argument('--img_size', type=int, default=DEFAULT_IMAGE_SIZE,
                        help='Image size for resizing.')
    parser.add_argument('--max_seq_len', type=int, default=128,
                        help='Maximum sequence length for text.')
    parser.add_argument('--valid_pct', type=float, default=0.05, # Smaller default for large datasets
                        help='Percentage of data to use for validation.')
    parser.add_argument('--num_workers', type=int, default=4,
                        help='Number of dataloader workers.')
    parser.add_argument('--use_augmentations', action='store_true', default=True,
                        help='Apply image augmentations during training.')

    # --- Model Arguments ---
    parser.add_argument('--vision_model_name', type=str, default='vit_base_patch16_224',
                        help='Name of the timm vision model.')
    parser.add_argument('--text_model_name', type=str, default=PRETRAINED_TOKENIZER_NAME,
                        help='Name/path of the Hugging Face text model.')
    parser.add_argument('--vision_pretrained', type=bool, default=True,
                        help='Use timm pretrained weights for vision model.')
    parser.add_argument('--text_pretrained', type=bool, default=True,
                        help='Use HF pretrained weights for text model.')
    parser.add_argument('--embed_dim', type=int, default=DEFAULT_EMBED_DIM,
                        help='Dimension of the shared embedding space.')

    # --- Training Arguments ---
    parser.add_argument('--epochs', type=int, default=5,
                        help='Number of training epochs.')
    parser.add_argument('--batch_size', type=int, default=DEFAULT_BATCH_SIZE,
                        help='Training batch size per GPU.')
    parser.add_argument('--lr', type=float, default=1e-4,
                        help='Maximum learning rate.')
    parser.add_argument('--wd', type=float, default=0.01,
                        help='Weight decay.')
    parser.add_argument('--beta1', type=float, default=0.9,
                        help='AdamW beta1.')
    parser.add_argument('--beta2', type=float, default=0.98,
                        help='AdamW beta2.')
    parser.add_argument('--eps', type=float, default=1e-6,
                        help='AdamW epsilon.')
    parser.add_argument('--warmup_steps', type=int, default=1000, # Number of steps for LR warmup
                        help='Number of warmup steps for the learning rate scheduler.')
    parser.add_argument('--use_amp', action='store_true', default=True,
                        help='Use Automatic Mixed Precision (AMP).')
    parser.add_argument('--grad_clip', type=float, default=None, # Default: no clipping
                        help='Value for gradient clipping (if desired).')
    parser.add_argument('--accum_freq', type=int, default=1,
                        help='Gradient accumulation frequency.')
    parser.add_argument('--seed', type=int, default=42,
                        help='Random seed for reproducibility.')

    # --- Checkpointing & Resuming ---
    parser.add_argument('--checkpoint_dir', type=str, default=str(CHECKPOINT_PATH),
                        help='Directory to save model checkpoints.')
    parser.add_argument('--save_model_name', type=str, default='best_recall',
                        help='Filename for the best saved model (without extension).')
    parser.add_argument('--save_epoch_frequency', type=int, default=1,
                        help='Save checkpoint every N epochs.')
    parser.add_argument('--resume_from', type=str, default=None,
                        help='Path to a checkpoint file to resume training from (e.g., models/checkpoints/best_recall.pth).')
    parser.add_argument('--early_stopping_patience', type=int, default=5,
                        help='Patience for early stopping based on validation metric.')

    # --- WandB Arguments ---
    parser.add_argument('--wandb_project', type=str, default='Indic-CLIP',
                        help='WandB project name.')
    parser.add_argument('--wandb_entity', type=str, default=None, # Set your WandB entity here or via env var
                        help='WandB entity (username or team).')
    parser.add_argument('--wandb_run_name', type=str, default=None,
                        help='Custom name for the WandB run (default: auto-generated).')
    parser.add_argument('--wandb_log_model', type=str, default='best', choices=['best', 'all', 'false'],
                        help='Log model checkpoints to WandB ("best", "all", or "false").')

    # --- Debugging/Misc ---
    parser.add_argument('--max_steps', type=int, default=None,
                        help='Maximum number of training steps for debugging.')

    # Workaround for notebooks: manually provide args or use default
    # In a script, 'parser.parse_args()' works directly.
    # When running interactively (like Jupyter), provide arguments as a list:
    # args = parser.parse_args([]) # Use defaults
    # args = parser.parse_args(['--batch_size', '32', '--epochs', '1']) # Example override
    # For script execution via `nbdev_export` or `python 10_training.ipynb ...`:
    if L(sys.argv).map(str).filter(lambda x: x.startswith('-')):
        args = parser.parse_args()
    else:
        # Default args for interactive use (e.g., notebook execution without CLI args)
        print("No command line args detected, using default values for interactive session.")
        args = parser.parse_args([])

    return args

# %% ../../nbs/10_training.ipynb 7
def main():
    """Main function to setup and run the training process."""
    args = parse_args()

    # --- Basic Setup ---
    set_seed(args.seed)
    ensure_dir(Path(args.checkpoint_dir))
    logger.info(f"Starting training run with args: {args}")

    if not args.wandb_entity:
         warnings.warn("WandB entity not provided (--wandb_entity). Logs will go to default entity or fail if not logged in.")
         # You might want to exit here if entity is strictly required
         # exit(1)

    # --- WandB Initialization ---
    try:
        wandb.init(
            project=args.wandb_project,
            entity=args.wandb_entity,
            name=args.wandb_run_name,
            config=vars(args) # Log all hyperparameters
        )
        log_model_policy = args.wandb_log_model
    except Exception as e:
        logger.error(f"Failed to initialize WandB: {e}. Proceeding without WandB logging.")
        wandb.init(mode="disabled") # Disable wandb if init fails
        log_model_policy = "false"

    # --- Load Data ---
    logger.info(f"Loading data items from: {args.processed_data_path}")
    items_df = get_indic_clip_items(data_path=Path(args.processed_data_path))
    if items_df.empty:
        logger.error("No data items loaded. Exiting.")
        wandb.finish(exit_code=1)
        return

    logger.info(f"Instantiating DataBlock...")
    # Load tokenizer needed for DataBlock and Model
    tokenizer = IndicBERTTokenizer.load_tokenizer(Path(args.tokenizer_path), max_length=args.max_seq_len)

    indic_clip_dblock = IndicCLIPDataBlock(
        tokenizer_name_or_path=args.text_model_name, # Pass model name used by tokenizer
        tokenizer_save_path=Path(args.tokenizer_path),
        max_length=args.max_seq_len,
        img_size=args.img_size,
        valid_pct=args.valid_pct,
        seed=args.seed,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        use_augmentations=args.use_augmentations
    )
    logger.info(f"Creating DataLoaders...")
    dls = indic_clip_dblock.get_dataloaders(items_df)
    logger.info(f"DataLoaders created. Train batches: {len(dls.train)}, Valid batches: {len(dls.valid)}")

    # --- Instantiate Model ---
    logger.info("Instantiating IndicCLIP model...")
    model = IndicCLIP(
        embed_dim=args.embed_dim,
        vision_model_name=args.vision_model_name,
        vision_pretrained=args.vision_pretrained,
        text_model_name=args.text_model_name,
        text_pretrained=args.text_pretrained,
        tokenizer=tokenizer # Pass the loaded tokenizer instance
    )

    # --- Loss Function ---
    loss_func = ContrastiveLoss()

    # --- Optimizer ---
    # Use OptimWrapper for flexible optimizer configuration with fastai
    opt_func = partial(OptimWrapper, opt=torch.optim.AdamW,
                       betas=(args.beta1, args.beta2), eps=args.eps, wd=args.wd)

    # --- Callbacks ---
    logger.info("Configuring callbacks...")
    callbacks = [
        # Custom callback for validation metrics
        RetrievalMetricCallback(k_values=[1, 5, 10]),
        # Built-in callbacks
        SaveModelCallback(
            monitor='valid_mean_recall', # Monitor the metric logged by RetrievalMetricCallback
            comp=np.greater,             # Maximize recall
            fname=args.save_model_name,  # Base filename for the best model
            every_epoch= (args.save_epoch_frequency > 0),
            every_n_epochs=args.save_epoch_frequency if args.save_epoch_frequency > 0 else None,
            at_end=True,
            with_opt=True,
            path=args.checkpoint_dir     # Save checkpoints here
            ),
        EarlyStoppingCallback(
            monitor='valid_mean_recall',
            comp=np.greater,
            patience=args.early_stopping_patience
            ),
         WandbCallback(
            log_preds=False, # Predictions can be large
            log_model=(log_model_policy != 'false'), # Log model based on policy
            log_model_checkpoints=(log_model_policy == 'all'), # Only log all if specified
            model_checkpoint_path=args.checkpoint_dir, # Tell wandb where checkpoints are saved
            model_checkpoint_name=args.save_model_name, # Match SaveModelCallback name
            dataset_name='IndicData', # Optional dataset name
            n_imgs_to_log=0 # Don't log images by default
            ),
         ProgressCallback() # Show progress bars
    ]

    if args.use_amp:
        logger.info("Using Automatic Mixed Precision (AMP).")
        callbacks.insert(0, MixedPrecision())

    if args.accum_freq > 1:
        logger.info(f"Using Gradient Accumulation with frequency {args.accum_freq}.")
        callbacks.insert(0, GradientAccumulation(n_acc=args.accum_freq))

    if args.grad_clip is not None:
         logger.info(f"Using Gradient Clipping with value {args.grad_clip}.")
         callbacks.append(GradientClip(args.grad_clip))

    # --- Create Learner ---
    logger.info("Creating fastai Learner...")
    learn = Learner(dls, model, loss_func=loss_func, opt_func=opt_func, cbs=callbacks)

    # Handle resuming from checkpoint
    if args.resume_from:
        resume_path = Path(args.resume_from)
        if resume_path.is_file():
            try:
                logger.info(f"Resuming training from checkpoint: {resume_path}")
                learn.load(resume_path.stem, with_opt=True, device=dls.device)
            except Exception as e:
                logger.error(f"Failed to load checkpoint {resume_path}: {e}. Starting from scratch.")
        else:
            logger.warning(f"Resume checkpoint not found at {resume_path}. Starting from scratch.")

    # --- Start Training ---
    logger.info(f"Starting training for {args.epochs} epochs...")
    # Use fit_one_cycle for standard learning rate schedule with warmup/cooldown
    # Calculate total steps for potential max_steps usage
    total_steps = len(dls.train) * args.epochs
    steps_to_run = args.max_steps if args.max_steps is not None else total_steps
    epochs_to_run = args.epochs if args.max_steps is None else (steps_to_run / len(dls.train))

    if args.max_steps:
        logger.warning(f"Limiting training to a maximum of {args.max_steps} steps (approx {epochs_to_run:.2f} epochs)." )
        # Need a custom training loop or callback to stop after max_steps
        # For simplicity here, we'll just limit epochs, which isn't exact for steps.
        # A MaxStepsCallback could be implemented.
        epochs_to_run = math.ceil(epochs_to_run) # Run at least enough epochs
        args.epochs = int(epochs_to_run)
        logger.info(f"Adjusted epochs to {args.epochs} based on max_steps.")


    # Simple fit_one_cycle - adjust warmup/cooldown if needed via Scheduler specific callbacks
    learn.fit_one_cycle(
        n_epoch=args.epochs,
        lr_max=args.lr,
        # pct_start can simulate warmup, e.g., warmup_steps / total_steps
        pct_start=min(0.3, args.warmup_steps / total_steps) if total_steps > 0 else 0.1
        # wd=args.wd is handled by opt_func (OptimWrapper)
    )

    logger.info("Training finished.")
    wandb.finish()
    logger.info("WandB run finished.")
