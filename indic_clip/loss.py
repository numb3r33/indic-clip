"""Implements the InfoNCE loss function for CLIP training, handling distributed data parallel (DDP) correctly."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/08_loss.ipynb.

# %% auto 0
__all__ = ['logger', 'AllGather', 'ContrastiveLoss']

# %% ../nbs/08_loss.ipynb 4
try:
    import indic_clip.core
    print("Reloaded indic_clip.core")
except ModuleNotFoundError:
    print("indic_clip.core not found initially.")
    # Attempt to set sys.path if running in Colab and project cloned
    import sys
    if 'google.colab' in sys.modules:
        project_parent = '/content' # Assuming cloned into /content/indic-clip
        if Path('/content/drive/MyDrive/Indic-Clip').exists():
             project_parent = '/content/drive/MyDrive/Indic-Clip'
        if project_parent not in sys.path:
             sys.path.insert(0, project_parent)
             print(f"Added {project_parent} to sys.path")
        try:
            import indic_clip.core
            print("Imported indic_clip.core after path adjustment.")
        except ModuleNotFoundError:
            print("ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.")
            print("Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive")
            # raise # Stop execution if core components missing

# %% ../nbs/08_loss.ipynb 8
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import logging
from typing import Optional

from fastai.vision.all import *

try:
    from indic_clip.core import get_logger, setup_logging
except ModuleNotFoundError:
    # Fallback if core not found
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    def get_logger(name): return logging.getLogger(name)
    def setup_logging(): pass

setup_logging()
logger = get_logger(__name__)

# %% ../nbs/08_loss.ipynb 10
class AllGather(torch.autograd.Function):
    """Custom autograd function to gather tensors from all processes, supporting gradients."""

    @staticmethod
    def forward(ctx, tensor: torch.Tensor) -> torch.Tensor:
        """Performs the all_gather operation and prepares context for backward pass."""
        # Check if distributed environment is initialized
        if not dist.is_available() or not dist.is_initialized():
            # If not distributed, just return the input tensor
            return tensor

        # Ensure tensor is contiguous before gathering
        tensor = tensor.contiguous()
        world_size = dist.get_world_size()
        # Create a list to hold tensors from all ranks
        output = [torch.empty_like(tensor) for _ in range(world_size)]
        # Perform the all_gather operation
        dist.all_gather(output, tensor)
        # Concatenate the gathered tensors along the batch dimension (dim=0)
        gathered_tensor = torch.cat(output, dim=0)

        # Save world_size for backward pass (optional, could re-fetch)
        # ctx.world_size = world_size
        return gathered_tensor

    @staticmethod
    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:
        """Performs the reduce_scatter operation for the backward pass."""
        # Check if distributed environment is initialized
        if not dist.is_available() or not dist.is_initialized():
            # If not distributed, just return the gradient
            return grad_output

        # Ensure grad_output is contiguous
        grad_output = grad_output.contiguous()
        world_size = dist.get_world_size()

        # Check if the gradient tensor size is divisible by world_size
        if grad_output.shape[0] % world_size != 0:
            raise RuntimeError("Gradient output size must be divisible by world size for all_gather backward pass.")

        # Calculate the chunk size for each process
        chunk_size = grad_output.shape[0] // world_size

        # Prepare the input tensor for reduce_scatter (this will hold the gradient for the current rank)
        grad_input = torch.empty(chunk_size, *grad_output.shape[1:], dtype=grad_output.dtype, device=grad_output.device)

        # Perform reduce_scatter: sums gradients corresponding to each rank's input
        # The list comprehension splits the gathered gradient tensor back into chunks
        dist.reduce_scatter(grad_input, list(grad_output.chunk(world_size, dim=0)), op=dist.ReduceOp.SUM)

        # grad_input now contains the correct gradient sum for the input tensor on this rank
        return grad_input

# %% ../nbs/08_loss.ipynb 12
class ContrastiveLoss(Module):
    """Calculates the contrastive loss (InfoNCE) between image and text features.

    Handles distributed training by gathering features across GPUs before calculating loss.
    Assumes input features (image_features, text_features) are already L2 normalized.
    """
    def __init__(self, *args, axis:int = -1, **kwargs):
        """
        Args:
            args: Arguments passed to the parent BaseLoss.
            axis (int): The axis to perform the reduction over (passed to BaseLoss).
            kwargs: Keyword arguments passed to the parent BaseLoss.
        """
        self.all_gather = AllGather.apply # Use the custom autograd function

    def forward(self, preds: tuple[torch.Tensor, torch.Tensor, torch.Tensor]) -> torch.Tensor:

        """
        Calculates the contrastive loss.

        Args:
            preds (tuple): A tuple containing:
                - image_features (torch.Tensor): Normalized image features (B, D).
                - text_features (torch.Tensor): Normalized text features (B, D).
                - logit_scale (torch.Tensor): The learnable logit scaling factor (scalar tensor).

        Returns:
            torch.Tensor: The calculated contrastive loss (scalar tensor).
        """
        # logger.info(">>> ContrastiveLoss.forward ENTERED")

        image_features, text_features, logit_scale = preds

        if torch.isnan(image_features).any() or torch.isinf(image_features).any():
            logger.error("!!! NaN/Inf DETECTED IN INPUT image_features !!!")
        if torch.isnan(text_features).any() or torch.isinf(text_features).any():
            logger.error("!!! NaN/Inf DETECTED IN INPUT text_features !!!")
        if torch.isnan(logit_scale).any() or torch.isinf(logit_scale).any():
            logger.error(f"!!! NaN/Inf DETECTED IN INPUT logit_scale: {logit_scale.item()} !!!")

        # logger.info(f"Input shapes: Img={image_features.shape}, Txt={text_features.shape}, Scale={logit_scale.shape}")
        # logger.info(f"Input norms (mean): Img={image_features.norm(dim=-1).mean().item():.4f}, Txt={text_features.norm(dim=-1).mean().item():.4f}")
        # logger.info(f"Logit Scale value: {logit_scale.item():.4f}")

        # --- Gather Features in Distributed Setting ---
        if dist.is_available() and dist.is_initialized():
            gathered_image_features = self.all_gather(image_features)
            gathered_text_features = self.all_gather(text_features)
            world_size = dist.get_world_size()
        else:
            gathered_image_features = image_features
            gathered_text_features = text_features
            world_size = 1

        # --- Calculate Similarity Scores ---
        # Note: logit_scale is applied *before* softmax in cross_entropy
        # We use the raw logit_scale parameter and apply exp() inside the loss calculation if needed,
        # or directly multiply as CLIP does.
        # The forward pass of IndicCLIP already returns exp(logit_scale).
        # Here, we assume logit_scale passed in is already exponentiated.

        # Cosine similarity as dot product of normalized features
        # logits_per_image: How well each image matches each text [Global B, Global B]
        logits_per_image = logit_scale * gathered_image_features @ gathered_text_features.t()
        # logits_per_text: How well each text matches each image [Global B, Global B]
        logits_per_text = logits_per_image.t() # More efficient than recalculating

        # --- Calculate Loss ---
        # Create ground truth labels. The diagonal elements (i,i) correspond to matching pairs.
        local_batch_size = image_features.size(0)
        global_batch_size = gathered_image_features.size(0)

        # Ensure calculation happens on the correct device
        device = image_features.device
        labels = torch.arange(global_batch_size, device=device, dtype=torch.long)

        if torch.isnan(logits_per_image).any() or torch.isinf(logits_per_image).any():
          logger.warning("NaN/Inf detected in logits_per_image!")
          # Optionally print min/max/mean
          logger.warning(f"Logit Scale (exp): {logit_scale.item()}")
          logger.warning(f"Image Features Norm: {gathered_image_features.norm(dim=-1).mean().item()}")
          logger.warning(f"Text Features Norm: {gathered_text_features.norm(dim=-1).mean().item()}")


        # Calculate cross-entropy loss for both directions
        loss_img = F.cross_entropy(logits_per_image, labels)
        loss_txt = F.cross_entropy(logits_per_text, labels)

        # Average the two losses
        total_loss = (loss_img + loss_txt) / 2

        # logger.info(f"loss_img: {loss_img}, loss_txt: {loss_txt}, total loss: {total_loss}")

        return total_loss
