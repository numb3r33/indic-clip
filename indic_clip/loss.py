"""Implements the InfoNCE loss function for CLIP training, handling distributed data parallel (DDP) correctly."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/08_loss.ipynb.

# %% auto 0
__all__ = ['logger', 'AllGather', 'ContrastiveLoss']

# %% ../nbs/08_loss.ipynb 4
try:
    import indic_clip.core
    print("Reloaded indic_clip.core")
except ModuleNotFoundError:
    print("indic_clip.core not found initially.")
    # Attempt to set sys.path if running in Colab and project cloned
    import sys
    if 'google.colab' in sys.modules:
        project_parent = '/content' # Assuming cloned into /content/indic-clip
        if Path('/content/drive/MyDrive/Indic-Clip').exists():
             project_parent = '/content/drive/MyDrive/Indic-Clip'
        if project_parent not in sys.path:
             sys.path.insert(0, project_parent)
             print(f"Added {project_parent} to sys.path")
        try:
            import indic_clip.core
            print("Imported indic_clip.core after path adjustment.")
        except ModuleNotFoundError:
            print("ERROR: Still cannot find indic_clip.core. Ensure project structure is correct.")
            print("Expected: /content/Indic-Clip/indic_clip/core.py or similar in Drive")
            # raise # Stop execution if core components missing

# %% ../nbs/08_loss.ipynb 8
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.distributed as dist
import logging
from typing import Optional

from fastai.vision.all import *

try:
    from indic_clip.core import get_logger, setup_logging
except ModuleNotFoundError:
    # Fallback if core not found
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    def get_logger(name): return logging.getLogger(name)
    def setup_logging(): pass

setup_logging()
logger = get_logger(__name__)

# %% ../nbs/08_loss.ipynb 10
class AllGather(torch.autograd.Function):
    """Custom autograd function to gather tensors from all processes, supporting gradients."""

    @staticmethod
    def forward(ctx, tensor: torch.Tensor) -> torch.Tensor:
        """Performs the all_gather operation and prepares context for backward pass."""
        # Check if distributed environment is initialized
        if not dist.is_available() or not dist.is_initialized():
            # If not distributed, just return the input tensor
            return tensor

        # Ensure tensor is contiguous before gathering
        tensor = tensor.contiguous()
        world_size = dist.get_world_size()
        # Create a list to hold tensors from all ranks
        output = [torch.empty_like(tensor) for _ in range(world_size)]
        # Perform the all_gather operation
        dist.all_gather(output, tensor)
        # Concatenate the gathered tensors along the batch dimension (dim=0)
        gathered_tensor = torch.cat(output, dim=0)

        # Save world_size for backward pass (optional, could re-fetch)
        # ctx.world_size = world_size
        return gathered_tensor

    @staticmethod
    def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:
        """Performs the reduce_scatter operation for the backward pass."""
        # Check if distributed environment is initialized
        if not dist.is_available() or not dist.is_initialized():
            # If not distributed, just return the gradient
            return grad_output

        logger.warning("!!! AllGather DEBUG: Returning SLICE of grad_output in backward pass !!!")
        # Ensure grad_output is contiguous
        grad_output = grad_output.contiguous()
        world_size = dist.get_world_size()

        # Check if the gradient tensor size is divisible by world_size
        if grad_output.shape[0] % world_size != 0:
            raise RuntimeError("Gradient output size must be divisible by world size for all_gather backward pass.")

        # Calculate the chunk size for each process
        chunk_size = grad_output.shape[0] // world_size

        # Prepare the input tensor for reduce_scatter (this will hold the gradient for the current rank)
        grad_input = torch.empty(chunk_size, *grad_output.shape[1:], dtype=grad_output.dtype, device=grad_output.device)

        # Perform reduce_scatter: sums gradients corresponding to each rank's input
        # The list comprehension splits the gathered gradient tensor back into chunks
        dist.reduce_scatter(grad_input, list(grad_output.chunk(world_size, dim=0)), op=dist.ReduceOp.SUM)

        # grad_input now contains the correct gradient sum for the input tensor on this rank
        return grad_input

# %% ../nbs/08_loss.ipynb 12
class ContrastiveLoss(Module): # Inherit from fastai's Module or nn.Module
    """Calculates the contrastive loss (InfoNCE) between image and text features.
    Handles distributed training by gathering features across GPUs before calculating loss.
    Designed to be used directly as Learner's loss_func.
    """
    def __init__(self):
        self.all_gather = AllGather.apply

    # *** IMPORTANT: Match fastai loss signature ***
    def forward(self, preds: tuple[torch.Tensor, torch.Tensor, torch.Tensor], *target) -> torch.Tensor:
        # preds is the output from the model: (image_features, text_features, logit_scale)
        # target is the dummy category batch, which we ignore here.
        # logger.critical("LOSS FORWARD CALLED") # Use critical for visibility

        image_features, text_features, logit_scale = preds
        device = image_features.device # Get the device from one of the inputs

        # --- Check features JUST BEFORE gathering/similarity calculation ---
        if torch.isnan(image_features).any():
            logger.error("!!! NaN DETECTED IN image_features JUST BEFORE ContrastiveLoss similarity calc !!!")
            return torch.tensor(torch.nan, device=device, requires_grad=True)
        if torch.isnan(text_features).any():
            logger.error("!!! NaN DETECTED IN text_features JUST BEFORE ContrastiveLoss similarity calc !!!")
            return torch.tensor(torch.nan, device=device, requires_grad=True)
        if torch.isnan(logit_scale):
            logger.error(f"!!! NaN DETECTED IN logit_scale JUST BEFORE ContrastiveLoss similarity calc: {logit_scale.item()} !!!")
            return torch.tensor(torch.nan, device=device, requires_grad=True)

        # --- Check norms AGAIN here (should be ~1.0) ---
        img_norms = image_features.norm(p=2, dim=-1)
        txt_norms = text_features.norm(p=2, dim=-1)
        if img_norms.min() < 0.9 or img_norms.max() > 1.1:
            logger.warning(f"Image feature norms deviate significantly from 1.0: min={img_norms.min().item()}, max={img_norms.max().item()}")
        if txt_norms.min() < 0.9 or txt_norms.max() > 1.1:
            logger.warning(f"Text feature norms deviate significantly from 1.0: min={txt_norms.min().item()}, max={txt_norms.max().item()}")

        # --- Gather Features Across GPUs (if applicable) ---
        if dist.is_available() and dist.is_initialized():
            logger.debug("Gathering features across GPUs.")
            gathered_image_features = self.all_gather(image_features)
            gathered_text_features = self.all_gather(text_features)
            # Logit scale is usually not gathered, assuming it's the same across devices
            # or handled differently (e.g., only rank 0 updates it).
        else:
            logger.debug("Not a distributed setup. Using local features.")
            gathered_image_features = image_features
            gathered_text_features = text_features
        # ---------------------------------------------------

        # --- Calculate Similarity ---
        # Note: logit_scale is used directly (already exponentiated by the model)
        logits_per_image = logit_scale * gathered_image_features @ gathered_text_features.t()
        # --- Check Logits Immediately ---
        if torch.isnan(logits_per_image).any() or torch.isinf(logits_per_image).any():
            logger.error("!!! NaN/Inf detected in logits_per_image AFTER similarity calculation !!!")
            logger.error(f"Logit Scale was: {logit_scale.item():.4f}")
            logger.error(f"Gathered Img Feat Stats: mean={gathered_image_features.mean().item()}, std={gathered_image_features.std().item()}, min={gathered_image_features.min().item()}, max={gathered_image_features.max().item()}")
            logger.error(f"Gathered Txt Feat Stats: mean={gathered_text_features.mean().item()}, std={gathered_text_features.std().item()}, min={gathered_text_features.min().item()}, max={gathered_text_features.max().item()}")
            return torch.tensor(torch.nan, device=device, requires_grad=True) # Return NaN

        logits_per_text = logits_per_image.t() # Efficient transpose

        # --- Create Labels ---
        # Ground truth is the identity matrix diagonal (image i matches text i)
        # The size should match the batch size *after* gathering
        gathered_batch_size = gathered_image_features.shape[0]
        labels = torch.arange(gathered_batch_size, device=device, dtype=torch.long)
        # ---------------------

        # --- Calculate Loss ---
        loss_img = F.cross_entropy(logits_per_image, labels)
        loss_txt = F.cross_entropy(logits_per_text, labels)

        # --- Check component losses ---
        if torch.isnan(loss_img): logger.error(f"!!! loss_img is NaN !!! Input logits max: {logits_per_image.max().item()}")
        if torch.isnan(loss_txt): logger.error(f"!!! loss_txt is NaN !!! Input logits max: {logits_per_text.max().item()}")

        total_loss = (loss_img + loss_txt) / 2

        # --- Final NaN check ---
        if torch.isnan(total_loss):
            logger.error(f"!!! total_loss is NaN !!! loss_img={loss_img.item()}, loss_txt={loss_txt.item()}")
            return torch.tensor(torch.nan, device=device, requires_grad=True) # Return NaN if components were NaN

        # logger.info(f"Returning total_loss: value={total_loss.item()}, shape={total_loss.shape}, dtype={total_loss.dtype}, device={total_loss.device}, requires_grad={total_loss.requires_grad}")
        return total_loss
